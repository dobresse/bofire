{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>BoFire is a framework to define and solve black-box optimization problems.  These problems can arise in a number of closely related fields including experimental design, multiobjective optimization and active learning.</p> <p>BoFire problem specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.</p>"},{"location":"#experimental-design","title":"Experimental design","text":"<p>In the context of experimental design BoFire allows to define a design space</p> \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] <p>where the design parameters may take values depending on their type and domain, e.g.</p> <ul> <li>continuous: \\(x_1 \\in [0, 1]\\)</li> <li>discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\)</li> <li>categorical: \\(x_3 \\in \\{A, B, C\\}\\)</li> </ul> <p>and a set of equations define additional experimental constraints, e.g.</p> <ul> <li>linear equality: \\(\\sum x_i = 1\\)</li> <li>linear inequality: \\(2 x_1 \\leq x_2\\)</li> <li>non-linear inequality: \\(\\sum x_i^2 \\leq 1\\)</li> <li>n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.</li> </ul>"},{"location":"#multiobjective-optimization","title":"Multiobjective optimization","text":"<p>In the context of multiobjective optimization BoFire allows to define a vector-valued optimization problem</p> \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] <p>where</p> <ul> <li>\\(x \\in \\mathbb{X}\\) is again the experimental design space</li> <li>\\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and</li> <li>\\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized.</li> </ul> <p>Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.</p>"},{"location":"#bayesian-optimization","title":"Bayesian optimization","text":"<p>In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\). An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation</p> \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] <p>and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.</p>"},{"location":"install/","title":"Installation","text":"<p>For the latest stable release install <pre><code>pip install bofire\n</code></pre> To live at head you can use <pre><code>pip install git+https://github.com/experimental-design/bofire.git\n</code></pre></p>"},{"location":"ref-constraints/","title":"Domain","text":""},{"location":"ref-constraints/#bofire.domain.constraints.Constraint","title":"<code> Constraint            (PydanticBaseModel)         </code>  <code>pydantic-model</code>","text":"<p>Abstract base class to define constraints on the optimization space.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>class Constraint(PydanticBaseModel):\n\"\"\"Abstract base class to define constraints on the optimization space.\"\"\"\n\n    type: str\n\n    @abstractmethod\n    def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe to check constraint fulfillment.\n\n        Returns:\n            bool: True if fulfilled else False\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Numerically evaluates the constraint.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n        Returns:\n            pd.Series: Distance to reach constraint fulfillment.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def from_dict(dict_: dict):\n        return parse_obj_as(AnyConstraint, dict_)\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.Constraint.__call__","title":"<code>__call__(self, experiments)</code>  <code>special</code>","text":"<p>Numerically evaluates the constraint.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe to evaluate the constraint on.</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>Distance to reach constraint fulfillment.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>@abstractmethod\ndef __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Numerically evaluates the constraint.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n    Returns:\n        pd.Series: Distance to reach constraint fulfillment.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.Constraint.is_fulfilled","title":"<code>is_fulfilled(self, experiments)</code>","text":"<p>Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe to check constraint fulfillment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if fulfilled else False</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>@abstractmethod\ndef is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe to check constraint fulfillment.\n\n    Returns:\n        bool: True if fulfilled else False\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints","title":"<code> Constraints            (PydanticBaseModel)         </code>  <code>pydantic-model</code>","text":"Source code in <code>bofire/domain/constraints.py</code> <pre><code>class Constraints(PydanticBaseModel):\n\n    constraints: Sequence[AnyConstraint] = Field(default_factory=lambda: [])\n\n    def __iter__(self):\n        return iter(self.constraints)\n\n    def __len__(self):\n        return len(self.constraints)\n\n    def __getitem__(self, i):\n        return self.constraints[i]\n\n    def __add__(\n        self, other: Union[Sequence[AnyConstraint], \"Constraints\"]\n    ) -&gt; \"Constraints\":\n        if isinstance(other, collections.abc.Sequence):\n            other_constraints = other\n        else:\n            other_constraints = other.constraints\n        constraints = list(chain(self.constraints, other_constraints))\n        return Constraints(constraints=constraints)\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Numerically evaluate all constraints\n\n        Args:\n            experiments (pd.DataFrame): data to evaluate the constraint on\n\n        Returns:\n            pd.DataFrame: Constraint evaluation for each of the constraints\n        \"\"\"\n        return pd.concat([c(experiments) for c in self.constraints], axis=1)\n\n    def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe\n\n        Args:\n            df_data (pd.DataFrame): Dataframe with data, the constraint validity should be tested on\n\n        Returns:\n            Boolean: True if all constraints are fulfilled for all rows, false if not\n        \"\"\"\n        if len(self.constraints) == 0:\n            return pd.Series([True] * len(experiments), index=experiments.index)\n        return pd.concat(\n            [c.is_fulfilled(experiments) for c in self.constraints], axis=1\n        ).all(axis=1)\n\n    def get(\n        self,\n        includes: Union[Type, List[Type]] = Constraint,\n        excludes: Union[Type, List[Type]] = None,\n        exact: bool = False,\n    ) -&gt; \"Constraints\":\n\"\"\"get constraints of the domain\n\n        Args:\n            includes (Union[Constraint, List[Constraint]], optional): Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.\n            excludes (Union[Type, List[Type]], optional): Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            List[Constraint]: List of constraints in the domain fitting to the passed requirements.\n        \"\"\"\n        return Constraints(\n            constraints=filter_by_class(\n                self.constraints,\n                includes=includes,\n                excludes=excludes,\n                exact=exact,\n            )\n        )\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.__call__","title":"<code>__call__(self, experiments)</code>  <code>special</code>","text":"<p>Numerically evaluate all constraints</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>data to evaluate the constraint on</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Constraint evaluation for each of the constraints</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Numerically evaluate all constraints\n\n    Args:\n        experiments (pd.DataFrame): data to evaluate the constraint on\n\n    Returns:\n        pd.DataFrame: Constraint evaluation for each of the constraints\n    \"\"\"\n    return pd.concat([c(experiments) for c in self.constraints], axis=1)\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.get","title":"<code>get(self, includes=&lt;class 'bofire.domain.constraints.Constraint'&gt;, excludes=None, exact=False)</code>","text":"<p>get constraints of the domain</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[Constraint, List[Constraint]]</code> <p>Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.</p> <code>&lt;class 'bofire.domain.constraints.Constraint'&gt;</code> <code>excludes</code> <code>Union[Type, List[Type]]</code> <p>Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Constraint]</code> <p>List of constraints in the domain fitting to the passed requirements.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def get(\n    self,\n    includes: Union[Type, List[Type]] = Constraint,\n    excludes: Union[Type, List[Type]] = None,\n    exact: bool = False,\n) -&gt; \"Constraints\":\n\"\"\"get constraints of the domain\n\n    Args:\n        includes (Union[Constraint, List[Constraint]], optional): Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.\n        excludes (Union[Type, List[Type]], optional): Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        List[Constraint]: List of constraints in the domain fitting to the passed requirements.\n    \"\"\"\n    return Constraints(\n        constraints=filter_by_class(\n            self.constraints,\n            includes=includes,\n            excludes=excludes,\n            exact=exact,\n        )\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.is_fulfilled","title":"<code>is_fulfilled(self, experiments)</code>","text":"<p>Check if all constraints are fulfilled on all rows of the provided dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df_data</code> <code>pd.DataFrame</code> <p>Dataframe with data, the constraint validity should be tested on</p> required <p>Returns:</p> Type Description <code>Boolean</code> <p>True if all constraints are fulfilled for all rows, false if not</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe\n\n    Args:\n        df_data (pd.DataFrame): Dataframe with data, the constraint validity should be tested on\n\n    Returns:\n        Boolean: True if all constraints are fulfilled for all rows, false if not\n    \"\"\"\n    if len(self.constraints) == 0:\n        return pd.Series([True] * len(experiments), index=experiments.index)\n    return pd.concat(\n        [c.is_fulfilled(experiments) for c in self.constraints], axis=1\n    ).all(axis=1)\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint","title":"<code> LinearConstraint            (Constraint)         </code>  <code>pydantic-model</code>","text":"<p>Abstract base class for linear equality and inequality constraints.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>list</code> <p>list of feature keys (str) on which the constraint works on.</p> <code>coefficients</code> <code>list</code> <p>list of coefficients (float) of the constraint.</p> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>class LinearConstraint(Constraint):\n\"\"\"Abstract base class for linear equality and inequality constraints.\n\n    Attributes:\n        features (list): list of feature keys (str) on which the constraint works on.\n        coefficients (list): list of coefficients (float) of the constraint.\n        rhs (float): Right-hand side of the constraint\n    \"\"\"\n\n    type: Literal[\"LinearConstraint\"] = \"LinearConstraint\"\n\n    features: TFeatureKeys\n    coefficients: TCoefficients\n    rhs: float\n\n    @validator(\"features\")\n    def validate_features_unique(cls, features):\n\"\"\"Validate that feature keys are unique.\"\"\"\n        if len(features) != len(set(features)):\n            raise ValueError(\"features must be unique\")\n        return features\n\n    @root_validator(pre=False, skip_on_failure=True)\n    def validate_list_lengths(cls, values):\n\"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\"\n        if len(values[\"features\"]) != len(values[\"coefficients\"]):\n            raise ValueError(\n                f'must provide same number of features and coefficients, got {len(values[\"features\"])} != {len(values[\"coefficients\"])}'\n            )\n        return values\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        return (\n            experiments[self.features] @ self.coefficients - self.rhs\n        ) / np.linalg.norm(self.coefficients)\n\n    # def lhs(self, df_data: pd.DataFrame) -&gt; float:\n    #     \"\"\"Evaluate the left-hand side of the constraint on each row of a dataframe\n\n    #     Args:\n    #         df_data (pd.DataFrame): Dataframe on which the left-hand side should be evaluated.\n\n    #     Returns:\n    #         np.array: 1-dim array with left-hand side of each row of the provided dataframe.\n    #     \"\"\"\n    #     cols = self.features\n    #     coefficients = self.coefficients\n    #     return np.sum(df_data[cols].values * np.array(coefficients), axis=1)\n\n    def __str__(self) -&gt; str:\n\"\"\"Generate string representation of the constraint.\n\n        Returns:\n            str: string representation of the constraint.\n        \"\"\"\n        return \" + \".join(\n            [f\"{self.coefficients[i]} * {feat}\" for i, feat in enumerate(self.features)]\n        )\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Generate string representation of the constraint.</p> <p>Returns:</p> Type Description <code>str</code> <p>string representation of the constraint.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def __str__(self) -&gt; str:\n\"\"\"Generate string representation of the constraint.\n\n    Returns:\n        str: string representation of the constraint.\n    \"\"\"\n    return \" + \".join(\n        [f\"{self.coefficients[i]} * {feat}\" for i, feat in enumerate(self.features)]\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint.validate_features_unique","title":"<code>validate_features_unique(features)</code>  <code>classmethod</code>","text":"<p>Validate that feature keys are unique.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>@validator(\"features\")\ndef validate_features_unique(cls, features):\n\"\"\"Validate that feature keys are unique.\"\"\"\n    if len(features) != len(set(features)):\n        raise ValueError(\"features must be unique\")\n    return features\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint.validate_list_lengths","title":"<code>validate_list_lengths(values)</code>  <code>classmethod</code>","text":"<p>Validate that length of the feature and coefficient lists have the same length.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>@root_validator(pre=False, skip_on_failure=True)\ndef validate_list_lengths(cls, values):\n\"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\"\n    if len(values[\"features\"]) != len(values[\"coefficients\"]):\n        raise ValueError(\n            f'must provide same number of features and coefficients, got {len(values[\"features\"])} != {len(values[\"coefficients\"])}'\n        )\n    return values\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearEqualityConstraint","title":"<code> LinearEqualityConstraint            (LinearConstraint)         </code>  <code>pydantic-model</code>","text":"<p>Linear equality constraint of the form <code>coefficients * x = rhs</code>.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>list</code> <p>list of feature keys (str) on which the constraint works on.</p> <code>coefficients</code> <code>list</code> <p>list of coefficients (float) of the constraint.</p> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>class LinearEqualityConstraint(LinearConstraint):\n\"\"\"Linear equality constraint of the form `coefficients * x = rhs`.\n\n    Attributes:\n        features (list): list of feature keys (str) on which the constraint works on.\n        coefficients (list): list of coefficients (float) of the constraint.\n        rhs (float): Right-hand side of the constraint\n    \"\"\"\n\n    type: Literal[\"LinearEqualityConstraint\"] = \"LinearEqualityConstraint\"\n\n    # def is_fulfilled(self, experiments: pd.DataFrame, complete: bool) -&gt; bool:\n    #     \"\"\"Check if the linear equality constraint is fulfilled for all the rows of the provided dataframe.\n\n    #     Args:\n    #         df_data (pd.DataFrame): Dataframe to evaluate constraint on.\n\n    #     Returns:\n    #         bool: True if fulfilled else False.\n    #     \"\"\"\n    #     fulfilled = np.isclose(self(experiments), 0)\n    #     if complete:\n    #         return fulfilled.all()\n    #     else:\n    #         pd.Series(fulfilled, index=experiments.index)\n\n    def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(np.isclose(self(experiments), 0), index=experiments.index)\n\n    def __str__(self) -&gt; str:\n\"\"\"Generate string representation of the constraint.\n\n        Returns:\n            str: string representation of the constraint.\n        \"\"\"\n        return super().__str__() + f\" = {self.rhs}\"\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearEqualityConstraint.is_fulfilled","title":"<code>is_fulfilled(self, experiments)</code>","text":"<p>Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe to check constraint fulfillment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if fulfilled else False</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    return pd.Series(np.isclose(self(experiments), 0), index=experiments.index)\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint","title":"<code> LinearInequalityConstraint            (LinearConstraint)         </code>  <code>pydantic-model</code>","text":"<p>Linear inequality constraint of the form <code>coefficients * x &lt;= rhs</code>.</p> <p>To instantiate a constraint of the form <code>coefficients * x &gt;= rhs</code> multiply coefficients and rhs by -1, or use the classmethod <code>from_greater_equal</code>.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>list</code> <p>list of feature keys (str) on which the constraint works on.</p> <code>coefficients</code> <code>list</code> <p>list of coefficients (float) of the constraint.</p> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>class LinearInequalityConstraint(LinearConstraint):\n\"\"\"Linear inequality constraint of the form `coefficients * x &lt;= rhs`.\n\n    To instantiate a constraint of the form `coefficients * x &gt;= rhs` multiply coefficients and rhs by -1, or\n    use the classmethod `from_greater_equal`.\n\n    Attributes:\n        features (list): list of feature keys (str) on which the constraint works on.\n        coefficients (list): list of coefficients (float) of the constraint.\n        rhs (float): Right-hand side of the constraint\n    \"\"\"\n\n    type: Literal[\"LinearInequalityConstraint\"] = \"LinearInequalityConstraint\"\n\n    def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        noise = 10e-6\n        return self(experiments) &lt;= 0 + noise\n\n    def as_smaller_equal(self) -&gt; Tuple[List[str], List[float], float]:\n\"\"\"Return attributes in the smaller equal convention\n\n        Returns:\n            Tuple[List[str], List[float], float]: features, coefficients, rhs\n        \"\"\"\n        return self.features, self.coefficients, self.rhs\n\n    def as_greater_equal(self) -&gt; Tuple[List[str], List[float], float]:\n\"\"\"Return attributes in the greater equal convention\n\n        Returns:\n            Tuple[List[str], List[float], float]: features, coefficients, rhs\n        \"\"\"\n        return self.features, [-1.0 * c for c in self.coefficients], -1.0 * self.rhs\n\n    # TODO: from_greater_equal should take the object as input\n    @classmethod\n    def from_greater_equal(\n        cls,\n        features: List[str],\n        coefficients: List[float],\n        rhs: float,\n    ):\n\"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &gt;= rhs`.\n\n        Args:\n            features (List[str]): List of feature keys.\n            coefficients (List[float]): List of coefficients.\n            rhs (float): Right-hand side of the constraint.\n        \"\"\"\n        return cls(\n            features=features,\n            coefficients=[-1.0 * c for c in coefficients],\n            rhs=-1.0 * rhs,\n        )\n\n    # TODO: from_smaller_equal should take the object as input\n    @classmethod\n    def from_smaller_equal(\n        cls,\n        features: List[str],\n        coefficients: List[float],\n        rhs: float,\n    ):\n\"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &lt;= rhs`.\n\n        Args:\n            features (List[str]): List of feature keys.\n            coefficients (List[float]): List of coefficients.\n            rhs (float): Right-hand side of the constraint.\n        \"\"\"\n        return cls(\n            features=features,\n            coefficients=coefficients,\n            rhs=rhs,\n        )\n\n    def __str__(self):\n\"\"\"Generate string representation of the constraint.\n\n        Returns:\n            str: string representation of the constraint.\n        \"\"\"\n        return super().__str__() + f\" &lt;= {self.rhs}\"\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.as_greater_equal","title":"<code>as_greater_equal(self)</code>","text":"<p>Return attributes in the greater equal convention</p> <p>Returns:</p> Type Description <code>Tuple[List[str], List[float], float]</code> <p>features, coefficients, rhs</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def as_greater_equal(self) -&gt; Tuple[List[str], List[float], float]:\n\"\"\"Return attributes in the greater equal convention\n\n    Returns:\n        Tuple[List[str], List[float], float]: features, coefficients, rhs\n    \"\"\"\n    return self.features, [-1.0 * c for c in self.coefficients], -1.0 * self.rhs\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.as_smaller_equal","title":"<code>as_smaller_equal(self)</code>","text":"<p>Return attributes in the smaller equal convention</p> <p>Returns:</p> Type Description <code>Tuple[List[str], List[float], float]</code> <p>features, coefficients, rhs</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def as_smaller_equal(self) -&gt; Tuple[List[str], List[float], float]:\n\"\"\"Return attributes in the smaller equal convention\n\n    Returns:\n        Tuple[List[str], List[float], float]: features, coefficients, rhs\n    \"\"\"\n    return self.features, self.coefficients, self.rhs\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.from_greater_equal","title":"<code>from_greater_equal(features, coefficients, rhs)</code>  <code>classmethod</code>","text":"<p>Class method to construct linear inequality constraint of the form <code>coefficients * x &gt;= rhs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>List of feature keys.</p> required <code>coefficients</code> <code>List[float]</code> <p>List of coefficients.</p> required <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> required Source code in <code>bofire/domain/constraints.py</code> <pre><code>@classmethod\ndef from_greater_equal(\n    cls,\n    features: List[str],\n    coefficients: List[float],\n    rhs: float,\n):\n\"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &gt;= rhs`.\n\n    Args:\n        features (List[str]): List of feature keys.\n        coefficients (List[float]): List of coefficients.\n        rhs (float): Right-hand side of the constraint.\n    \"\"\"\n    return cls(\n        features=features,\n        coefficients=[-1.0 * c for c in coefficients],\n        rhs=-1.0 * rhs,\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.from_smaller_equal","title":"<code>from_smaller_equal(features, coefficients, rhs)</code>  <code>classmethod</code>","text":"<p>Class method to construct linear inequality constraint of the form <code>coefficients * x &lt;= rhs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>List of feature keys.</p> required <code>coefficients</code> <code>List[float]</code> <p>List of coefficients.</p> required <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> required Source code in <code>bofire/domain/constraints.py</code> <pre><code>@classmethod\ndef from_smaller_equal(\n    cls,\n    features: List[str],\n    coefficients: List[float],\n    rhs: float,\n):\n\"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &lt;= rhs`.\n\n    Args:\n        features (List[str]): List of feature keys.\n        coefficients (List[float]): List of coefficients.\n        rhs (float): Right-hand side of the constraint.\n    \"\"\"\n    return cls(\n        features=features,\n        coefficients=coefficients,\n        rhs=rhs,\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.is_fulfilled","title":"<code>is_fulfilled(self, experiments)</code>","text":"<p>Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe to check constraint fulfillment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if fulfilled else False</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    noise = 10e-6\n    return self(experiments) &lt;= 0 + noise\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint","title":"<code> NChooseKConstraint            (Constraint)         </code>  <code>pydantic-model</code>","text":"<p>NChooseK constraint that defines how many ingredients are allowed in a formulation.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>List[str]</code> <p>List of feature keys to which the constraint applies.</p> <code>min_count</code> <code>int</code> <p>Minimal number of non-zero/active feature values.</p> <code>max_count</code> <code>int</code> <p>Maximum number of non-zero/active feature values.</p> <code>none_also_valid</code> <code>bool</code> <p>In case that min_count &gt; 0, this flag decides if zero active features are also allowed.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>class NChooseKConstraint(Constraint):\n\"\"\"NChooseK constraint that defines how many ingredients are allowed in a formulation.\n\n    Attributes:\n        features (List[str]): List of feature keys to which the constraint applies.\n        min_count (int): Minimal number of non-zero/active feature values.\n        max_count (int): Maximum number of non-zero/active feature values.\n        none_also_valid (bool): In case that min_count &gt; 0,\n            this flag decides if zero active features are also allowed.\n    \"\"\"\n\n    type: Literal[\"NChooseKConstraint\"] = \"NChooseKConstraint\"\n    features: TFeatureKeys\n    min_count: int\n    max_count: int\n    none_also_valid: bool\n\n    @validator(\"features\")\n    def validate_features_unique(cls, features: List[str]):\n\"\"\"Validates that provided feature keys are unique.\"\"\"\n        if len(features) != len(set(features)):\n            raise ValueError(\"features must be unique\")\n        return features\n\n    @root_validator(pre=False, skip_on_failure=True)\n    def validate_counts(cls, values):\n\"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\"\n        features = values[\"features\"]\n        min_count = values[\"min_count\"]\n        max_count = values[\"max_count\"]\n\n        if min_count &gt; len(features):\n            raise ValueError(\"min_count must be &lt;= # of features\")\n        if max_count &gt; len(features):\n            raise ValueError(\"max_count must be &lt;= # of features\")\n        if min_count &gt; max_count:\n            raise ValueError(\"min_values must be &lt;= max_values\")\n\n        return values\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        raise NotImplementedError\n\n    def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe.\n\n        Args:\n            df_data (pd.DataFrame): Dataframe to evaluate constraint on.\n\n        Returns:\n            bool: True if fulfilled else False.\n        \"\"\"\n        cols = self.features\n        sums = (experiments[cols] &gt; 0).sum(axis=1)\n\n        lower = sums &gt;= self.min_count\n        upper = sums &lt;= self.max_count\n\n        if not self.none_also_valid:\n            # return lower.all() and upper.all()\n            return pd.Series(np.logical_and(lower, upper), index=experiments.index)\n        else:\n            none = sums == 0\n            return pd.Series(\n                np.logical_or(none, np.logical_and(lower, upper)),\n                index=experiments.index,\n            )\n\n    def __str__(self):\n\"\"\"Generate string representation of the constraint.\n\n        Returns:\n            str: string representation of the constraint.\n        \"\"\"\n        res = (\n            \"of the features \"\n            + \", \".join(self.features)\n            + f\" between {self.min_count} and {self.max_count} must be used\"\n        )\n        if self.none_also_valid:\n            res += \" (none is also ok)\"\n        return res\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Generate string representation of the constraint.</p> <p>Returns:</p> Type Description <code>str</code> <p>string representation of the constraint.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def __str__(self):\n\"\"\"Generate string representation of the constraint.\n\n    Returns:\n        str: string representation of the constraint.\n    \"\"\"\n    res = (\n        \"of the features \"\n        + \", \".join(self.features)\n        + f\" between {self.min_count} and {self.max_count} must be used\"\n    )\n    if self.none_also_valid:\n        res += \" (none is also ok)\"\n    return res\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.is_fulfilled","title":"<code>is_fulfilled(self, experiments)</code>","text":"<p>Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df_data</code> <code>pd.DataFrame</code> <p>Dataframe to evaluate constraint on.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if fulfilled else False.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe.\n\n    Args:\n        df_data (pd.DataFrame): Dataframe to evaluate constraint on.\n\n    Returns:\n        bool: True if fulfilled else False.\n    \"\"\"\n    cols = self.features\n    sums = (experiments[cols] &gt; 0).sum(axis=1)\n\n    lower = sums &gt;= self.min_count\n    upper = sums &lt;= self.max_count\n\n    if not self.none_also_valid:\n        # return lower.all() and upper.all()\n        return pd.Series(np.logical_and(lower, upper), index=experiments.index)\n    else:\n        none = sums == 0\n        return pd.Series(\n            np.logical_or(none, np.logical_and(lower, upper)),\n            index=experiments.index,\n        )\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.validate_counts","title":"<code>validate_counts(values)</code>  <code>classmethod</code>","text":"<p>Validates if the minimum and maximum of allowed features are smaller than the overall number of features.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>@root_validator(pre=False, skip_on_failure=True)\ndef validate_counts(cls, values):\n\"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\"\n    features = values[\"features\"]\n    min_count = values[\"min_count\"]\n    max_count = values[\"max_count\"]\n\n    if min_count &gt; len(features):\n        raise ValueError(\"min_count must be &lt;= # of features\")\n    if max_count &gt; len(features):\n        raise ValueError(\"max_count must be &lt;= # of features\")\n    if min_count &gt; max_count:\n        raise ValueError(\"min_values must be &lt;= max_values\")\n\n    return values\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.validate_features_unique","title":"<code>validate_features_unique(features)</code>  <code>classmethod</code>","text":"<p>Validates that provided feature keys are unique.</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>@validator(\"features\")\ndef validate_features_unique(cls, features: List[str]):\n\"\"\"Validates that provided feature keys are unique.\"\"\"\n    if len(features) != len(set(features)):\n        raise ValueError(\"features must be unique\")\n    return features\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearEqualityConstraint","title":"<code> NonlinearEqualityConstraint            (NonlinearConstraint)         </code>  <code>pydantic-model</code>","text":"Source code in <code>bofire/domain/constraints.py</code> <pre><code>class NonlinearEqualityConstraint(NonlinearConstraint):\n    # TODO: add docstring to NonlinearEqualityConstraint\n    type: Literal[\"NonlinearEqualityConstraint\"] = \"NonlinearEqualityConstraint\"\n\n    def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        return pd.Series(np.isclose(self(experiments), 0), index=experiments.index)\n\n    def __str__(self):\n        return f\"{self.expression}==0\"\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearEqualityConstraint.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Return str(self).</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def __str__(self):\n    return f\"{self.expression}==0\"\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearEqualityConstraint.is_fulfilled","title":"<code>is_fulfilled(self, experiments)</code>","text":"<p>Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe to check constraint fulfillment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if fulfilled else False</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    return pd.Series(np.isclose(self(experiments), 0), index=experiments.index)\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearInequalityConstraint","title":"<code> NonlinearInequalityConstraint            (NonlinearConstraint)         </code>  <code>pydantic-model</code>","text":"Source code in <code>bofire/domain/constraints.py</code> <pre><code>class NonlinearInequalityConstraint(NonlinearConstraint):\n    # TODO: add docstring to NonlinearInequalityConstraint\n    type: Literal[\"NonlinearInequalityConstraint\"] = \"NonlinearInequalityConstraint\"\n\n    def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        # we allow here for numerical noise\n        noise = 10e-6\n        return self(experiments) &lt;= 0 + noise\n\n    def __str__(self):\n        return f\"{self.expression}&lt;=0\"\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearInequalityConstraint.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Return str(self).</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def __str__(self):\n    return f\"{self.expression}&lt;=0\"\n</code></pre>"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearInequalityConstraint.is_fulfilled","title":"<code>is_fulfilled(self, experiments)</code>","text":"<p>Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe to check constraint fulfillment.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if fulfilled else False</p> Source code in <code>bofire/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    # we allow here for numerical noise\n    noise = 10e-6\n    return self(experiments) &lt;= 0 + noise\n</code></pre>"},{"location":"ref-domain-util/","title":"Domain","text":""},{"location":"ref-domain-util/#bofire.domain.util.filter_by_attribute","title":"<code>filter_by_attribute(data, attribute_getter, includes=None, excludes=None, exact=False)</code>","text":"<p>Returns those data elements where the attribute is of one of the include types.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence</code> <p>to be filtered</p> required <code>attribute_getter</code> <code>Callable[[Type], Any]</code> <p>expects an item of the data list and returns the attribute to filter by</p> required <code>includes</code> <code>Union[Type, Sequence[Type]]</code> <p>attribute types that should be kept, sub-type are included by default, see exact</p> <code>None</code> <code>excludes</code> <code>Union[Type, Sequence[Type]]</code> <p>attribute types that will be excluded even if they are sub-types of or include types.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>true for not including subtypes</p> <code>False</code> <p>Returns:</p> Type Description <code>List</code> <p>list of data point with attributes as filtered for</p> Source code in <code>bofire/domain/util.py</code> <pre><code>def filter_by_attribute(\n    data: Sequence,\n    attribute_getter: Callable[[Type], Any],\n    includes: Union[Type, Sequence[Type]] = None,\n    excludes: Union[Type, Sequence[Type]] = None,\n    exact: bool = False,\n) -&gt; List:\n\"\"\"Returns those data elements where the attribute is of one of the include types.\n\n    Args:\n        data: to be filtered\n        attribute_getter: expects an item of the data list and returns the attribute to filter by\n        includes: attribute types that should be kept, sub-type are included by default, see exact\n        excludes: attribute types that will be excluded even if they are sub-types of or include types.\n        exact: true for not including subtypes\n\n    Returns:\n        list of data point with attributes as filtered for\n    \"\"\"\n    data_with_attr = []\n    for d in data:\n        try:\n            attribute_getter(d)\n            data_with_attr.append(d)\n        except AttributeError:\n            pass\n\n    filtered = filter_by_class(\n        data_with_attr,\n        includes=includes,\n        excludes=excludes,\n        exact=exact,\n        key=attribute_getter,\n    )\n    return filtered\n</code></pre>"},{"location":"ref-domain-util/#bofire.domain.util.filter_by_class","title":"<code>filter_by_class(data, includes=None, excludes=None, exact=False, key=&lt;function &lt;lambda&gt; at 0x7f49630cf4c0&gt;)</code>","text":"<p>Returns those data elements where are one of the include types.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence</code> <p>to be filtered</p> required <code>includes</code> <code>Union[Type, Sequence[Type]]</code> <p>attribute types that should be kept, sub-type are included by default, see exact</p> <code>None</code> <code>excludes</code> <code>Union[Type, Sequence[Type]]</code> <p>attribute types that will be excluded even if they are sub-types of or include types.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>true for not including subtypes</p> <code>False</code> <code>key</code> <code>Callable[[Type], Any]</code> <p>maps a data list item to something that is used for filtering, identity by default</p> <code>&lt;function &lt;lambda&gt; at 0x7f49630cf4c0&gt;</code> <p>Returns:</p> Type Description <code>List</code> <p>filtered list of data points</p> Source code in <code>bofire/domain/util.py</code> <pre><code>def filter_by_class(\n    data: Sequence,\n    includes: Union[Type, Sequence[Type]] = None,\n    excludes: Union[Type, Sequence[Type]] = None,\n    exact: bool = False,\n    key: Callable[[Type], Any] = lambda x: x,\n) -&gt; List:\n\"\"\"Returns those data elements where are one of the include types.\n\n    Args:\n        data: to be filtered\n        includes: attribute types that should be kept, sub-type are included by default, see exact\n        excludes: attribute types that will be excluded even if they are sub-types of or include types.\n        exact: true for not including subtypes\n        key: maps a data list item to something that is used for filtering, identity by default\n\n    Returns:\n        filtered list of data points\n    \"\"\"\n    if includes is None:\n        includes = []\n    if not isinstance(includes, collections.Sequence):\n        includes = [includes]\n    if excludes is None:\n        excludes = []\n    if not isinstance(excludes, collections.Sequence):\n        excludes = [excludes]\n\n    if len(includes) == len(excludes) == 0:\n        raise ValueError(\"no filter provided\")\n\n    if len(includes) == 0:\n        includes = [object]\n\n    includes_ = []\n    for incl in includes:\n        if get_origin(incl) is Union:\n            includes_ += get_args(incl)\n        else:\n            includes_.append(incl)\n    includes = includes_\n    excludes_ = []\n    for excl in excludes:\n        if get_origin(excl) is Union:\n            excludes_ += get_args(excl)\n        else:\n            excludes_.append(excl)\n    excludes = excludes_\n\n    if len([x for x in includes if x in excludes]) &gt; 0:\n        raise ValueError(\"includes and excludes overlap\")\n\n    if exact:\n        return [\n            d for d in data if type(key(d)) in includes and type(key(d)) not in excludes\n        ]\n    return [\n        d\n        for d in data\n        if isinstance(key(d), tuple(includes))\n        and not isinstance(key(d), tuple(excludes))\n    ]\n</code></pre>"},{"location":"ref-domain/","title":"Domain","text":""},{"location":"ref-domain/#bofire.domain.domain.Domain","title":"<code> Domain            (PydanticBaseModel)         </code>  <code>pydantic-model</code>","text":"Source code in <code>bofire/domain/domain.py</code> <pre><code>class Domain(PydanticBaseModel):\n\n    # The types describe what we expect to be passed as arguments.\n    # They will be converted to InputFeatures and OutputFeatures, respectively.\n    input_features: Union[Sequence[AnyInputFeature], InputFeatures] = Field(\n        default_factory=lambda: InputFeatures()\n    )\n    output_features: Union[Sequence[AnyOutputFeature], OutputFeatures] = Field(\n        default_factory=lambda: OutputFeatures()\n    )\n\n    constraints: Union[Sequence[AnyConstraint], Constraints] = Field(\n        default_factory=lambda: Constraints()\n    )\n\n    experiments: Optional[pd.DataFrame] = None\n    candidates: Optional[pd.DataFrame] = None\n\n\"\"\"Representation of the optimization problem/domain\n\n    Attributes:\n        input_features (List[InputFeature], optional): List of input features. Defaults to [].\n        output_features (List[OutputFeature], optional): List of output features. Defaults to [].\n        constraints (List[Constraint], optional): List of constraints. Defaults to [].\n    \"\"\"\n\n    @property\n    def outputs(self) -&gt; OutputFeatures:\n\"\"\"Returns output features as OutputFeatures\"\"\"\n        return cast(OutputFeatures, self.output_features)\n\n    @property\n    def inputs(self) -&gt; InputFeatures:\n\"\"\"Returns input features as InputFeatures\"\"\"\n        return cast(InputFeatures, self.input_features)\n\n    @property\n    def cnstrs(self) -&gt; Constraints:\n        return cast(Constraints, self.constraints)\n\n    @validator(\"input_features\", always=True, pre=True)\n    def validate_input_features_list(cls, v, values):\n        if isinstance(v, collections.abc.Sequence):\n            v = InputFeatures(features=v)\n            return v\n        if isinstance_or_union(v, AnyInputFeature):\n            return InputFeatures(features=[v])\n        else:\n            return v\n\n    @validator(\"output_features\", always=True, pre=True)\n    def validate_output_features_list(cls, v, values):\n        if isinstance(v, collections.abc.Sequence):\n            return OutputFeatures(features=v)\n        if isinstance_or_union(v, AnyOutputFeature):\n            return OutputFeatures(features=[v])\n        else:\n            return v\n\n    @validator(\"constraints\", always=True, pre=True)\n    def validate_constraints_list(cls, v, values):\n        if isinstance(v, list):\n            return Constraints(constraints=v)\n        if isinstance_or_union(v, AnyConstraint):\n            return Constraints(constraints=[v])\n        else:\n            return v\n\n    @validator(\"output_features\", always=True)\n    def validate_unique_feature_keys(cls, v: OutputFeatures, values) -&gt; OutputFeatures:\n\"\"\"Validates if provided input and output feature keys are unique\n\n        Args:\n            v (OutputFeatures): List of all output features of the domain.\n            value (Dict[str, InputFeatures]): Dict containing a list of input features as single entry.\n\n        Raises:\n            ValueError: Feature keys are not unique.\n\n        Returns:\n            OutputFeatures: Keeps output features as given.\n        \"\"\"\n        if \"input_features\" not in values:\n            return v\n        features = v + values[\"input_features\"]\n        keys = [f.key for f in features]\n        if len(set(keys)) != len(keys):\n            raise ValueError(\"feature keys are not unique\")\n        return v\n\n    @validator(\"constraints\", always=True)\n    def validate_constraints(cls, v, values):\n\"\"\"Validate if all features included in the constraints are also defined as features for the domain.\n\n        Args:\n            v (List[Constraint]): List of constraints or empty if no constraints are defined\n            values (List[InputFeature]): List of input features of the domain\n\n        Raises:\n            ValueError: Feature key in constraint is unknown.\n\n        Returns:\n            List[Constraint]: List of constraints defined for the domain\n        \"\"\"\n        if \"input_features\" not in values:\n            return v\n        keys = [f.key for f in values[\"input_features\"]]\n        for c in v:\n            if isinstance(c, LinearConstraint) or isinstance(c, NChooseKConstraint):\n                for f in c.features:\n                    if f not in keys:\n                        raise ValueError(f\"feature {f} in constraint unknown ({keys})\")\n        return v\n\n    @validator(\"constraints\", always=True)\n    def validate_linear_constraints(cls, v, values):\n\"\"\"Validate if all features included in linear constraints are continuous ones.\n\n        Args:\n            v (List[Constraint]): List of constraints or empty if no constraints are defined\n            values (List[InputFeature]): List of input features of the domain\n\n        Raises:\n            ValueError: _description_\n\n\n        Returns:\n           List[Constraint]: List of constraints defined for the domain\n        \"\"\"\n        if \"input_features\" not in values:\n            return v\n\n        # gather continuous input_features in dictionary\n        continuous_input_features_dict = {}\n        for f in values[\"input_features\"]:\n            if type(f) is ContinuousInput:\n                continuous_input_features_dict[f.key] = f\n\n        # check if non continuous input features appear in linear constraints\n        for c in v:\n            if isinstance(c, LinearConstraint):\n                for f in c.features:\n                    assert (\n                        f in continuous_input_features_dict\n                    ), f\"{f} must be continuous.\"\n        return v\n\n    @validator(\"constraints\", always=True)\n    def validate_lower_bounds_in_nchoosek_constraints(cls, v, values):\n\"\"\"Validate the lower bound as well if the chosen number of allowed features is continuous.\n\n        Args:\n            v (List[Constraint]): List of all constraints defined for the domain\n            values (List[InputFeature]): _description_\n\n        Returns:\n            List[Constraint]: List of constraints defined for the domain\n        \"\"\"\n        # gather continuous input_features in dictionary\n        continuous_input_features_dict = {}\n        for f in values[\"input_features\"]:\n            if type(f) is ContinuousInput:\n                continuous_input_features_dict[f.key] = f\n\n        # check if unfixed continuous features appearing in NChooseK constraints have lower bound of 0\n        for c in v:\n            if isinstance(c, NChooseKConstraint):\n                for f in c.features:\n                    assert (\n                        f in continuous_input_features_dict\n                    ), f\"{f} must be continuous.\"\n                    assert (\n                        continuous_input_features_dict[f].lower_bound == 0\n                    ), f\"lower bound of {f} must be 0 for NChooseK constraint.\"\n        return v\n\n    def get_feature_reps_df(self) -&gt; pd.DataFrame:\n\"\"\"Returns a pandas dataframe describing the features contained in the optimization domain.\"\"\"\n        df = pd.DataFrame(\n            index=self.get_feature_keys(Feature),\n            columns=[\"Type\", \"Description\"],\n            data={\n                \"Type\": [\n                    feat.__class__.__name__ for feat in self.get_features(Feature)\n                ],\n                \"Description\": [feat.__str__() for feat in self.get_features(Feature)],\n            },\n        )\n        return df\n\n    def get_constraint_reps_df(self):\n\"\"\"Provides a tabular overwiev of all constraints within the domain\n\n        Returns:\n            pd.DataFrame: DataFrame listing all constraints of the domain with a description\n        \"\"\"\n        df = pd.DataFrame(\n            index=range(len(self.constraints)),\n            columns=[\"Type\", \"Description\"],\n            data={\n                \"Type\": [feat.__class__.__name__ for feat in self.constraints],\n                \"Description\": [\n                    constraint.__str__() for constraint in self.constraints\n                ],\n            },\n        )\n        return df\n\n    def get_features(\n        self,\n        includes: Union[Type[Feature], List[Type[Feature]]] = Feature,\n        excludes: Union[Type[Feature], List[Type[Feature]], None] = None,\n        exact: bool = False,\n    ) -&gt; Features:\n\"\"\"get features of the domain\n\n        Args:\n            includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n            excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n            by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None.\n\n        Returns:\n            List[Feature]: List of features in the domain fitting to the passed requirements.\n        \"\"\"\n        assert isinstance(self.input_features, InputFeatures)\n        features = self.input_features + self.output_features\n        return features.get(includes, excludes, exact)\n\n    def get_feature_keys(\n        self,\n        includes: Union[Type, List[Type]] = Feature,\n        excludes: Union[Type, List[Type]] = None,\n        exact: bool = False,\n    ) -&gt; List[str]:\n\"\"\"Method to get feature keys of the domain\n\n        Args:\n            includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n            excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            List[str]: List of feature keys fitting to the passed requirements.\n        \"\"\"\n        return [\n            f.key\n            for f in self.get_features(\n                includes=includes,\n                excludes=excludes,\n                exact=exact,\n            )\n        ]\n\n    def get_feature(self, key: str):\n\"\"\"get a specific feature by its key\n\n        Args:\n            key (str): Feature key\n\n        Returns:\n            Feature: The feature with the passed key\n        \"\"\"\n        assert isinstance(self.input_features, InputFeatures)\n        return {f.key: f for f in self.input_features + self.output_features}[key]\n\n    # getting list of fixed values\n    def get_nchoosek_combinations(self):\n\"\"\"get all possible NChooseK combinations\n\n        Returns:\n            Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination.\n             unused_features_list is a list of lists containing features unused in each NChooseK combination.\n        \"\"\"\n\n        if len(self.cnstrs.get(NChooseKConstraint)) == 0:\n            used_continuous_features = self.get_feature_keys(ContinuousInput)\n            return used_continuous_features, []\n\n        used_features_list_all = []\n\n        # loops through each NChooseK constraint\n        for con in self.cnstrs.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            used_features_list = []\n\n            for n in range(con.min_count, con.max_count + 1):\n                used_features_list.extend(itertools.combinations(con.features, n))\n\n            if con.none_also_valid:\n                used_features_list.append(tuple([]))\n\n            used_features_list_all.append(used_features_list)\n\n        used_features_list_all = list(\n            itertools.product(*used_features_list_all)\n        )  # product between NChooseK constraints\n\n        # format into a list of used features\n        used_features_list_formatted = []\n        for used_features_list in used_features_list_all:\n\n            used_features_list_flattened = [\n                item for sublist in used_features_list for item in sublist\n            ]\n            used_features_list_formatted.append(list(set(used_features_list_flattened)))\n\n        # sort lists\n        used_features_list_sorted = []\n        for used_features in used_features_list_formatted:\n            used_features_list_sorted.append(sorted(used_features))\n\n        # drop duplicates\n        used_features_list_no_dup = []\n        for used_features in used_features_list_sorted:\n            if used_features not in used_features_list_no_dup:\n                used_features_list_no_dup.append(used_features)\n\n        # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\")\n\n        # remove combinations not fulfilling constraints\n        used_features_list_final = []\n        for combo in used_features_list_no_dup:\n            fulfil_constraints = (\n                []\n            )  # list of bools tracking if constraints are fulfilled\n            for con in self.cnstrs.get(NChooseKConstraint):\n                assert isinstance(con, NChooseKConstraint)\n                count = 0  # count of features in combo that are in con.features\n                for f in combo:\n                    if f in con.features:\n                        count += 1\n                if count &gt;= con.min_count and count &lt;= con.max_count:\n                    fulfil_constraints.append(True)\n                elif count == 0 and con.none_also_valid:\n                    fulfil_constraints.append(True)\n                else:\n                    fulfil_constraints.append(False)\n            if np.all(fulfil_constraints):\n                used_features_list_final.append(combo)\n\n        # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\")\n\n        # features unused\n        features_in_cc = []\n        for con in self.cnstrs.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            features_in_cc.extend(con.features)\n        features_in_cc = list(set(features_in_cc))\n        features_in_cc.sort()\n        unused_features_list = []\n        for used_features in used_features_list_final:\n            unused_features_list.append(\n                [f_key for f_key in features_in_cc if f_key not in used_features]\n            )\n\n        # postprocess\n        # used_features_list_final2 = []\n        # unused_features_list2 = []\n        # for used, unused in zip(used_features_list_final,unused_features_list):\n        #     if len(used) == 3:\n        #         used_features_list_final2.append(used), unused_features_list2.append(unused)\n\n        return used_features_list_final, unused_features_list\n\n    def coerce_invalids(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Coerces all invalid output measurements to np.nan\n\n        Args:\n            experiments (pd.DataFrame): Dataframe containing experimental data\n\n        Returns:\n            pd.DataFrame: coerced dataframe\n        \"\"\"\n        # coerce invalid to nan\n        for feat in self.get_feature_keys(OutputFeature):\n            experiments.loc[experiments[f\"valid_{feat}\"] == 0, feat] = np.nan\n        return experiments\n\n    def aggregate_by_duplicates(\n        self, experiments: pd.DataFrame, prec: int, delimiter: str = \"-\"\n    ) -&gt; Tuple[pd.DataFrame, list]:\n\"\"\"Aggregate the dataframe by duplicate experiments\n\n        Duplicates are identified based on the experiments with the same input features. Continuous input features\n        are rounded before identifying the duplicates. Aggregation is performed by taking the average of the\n        involved output features.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe containing experimental data\n            prec (int): Precision of the rounding of the continuous input features\n            delimiter (str, optional): Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\".\n\n        Returns:\n            Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates\n        \"\"\"\n        # prepare the parent frame\n\n        preprocessed = self.outputs.preprocess_experiments_any_valid_output(experiments)\n        assert preprocessed is not None\n        experiments = preprocessed.copy()\n        if \"labcode\" not in experiments.columns:\n            experiments[\"labcode\"] = [\n                str(i + 1).zfill(int(np.ceil(np.log10(experiments.shape[0]))))\n                for i in range(experiments.shape[0])\n            ]\n\n        # round it\n        experiments[self.get_feature_keys(ContinuousInput)] = experiments[\n            self.get_feature_keys(ContinuousInput)\n        ].round(prec)\n\n        # coerce invalid to nan\n        experiments = self.coerce_invalids(experiments)\n\n        # group and aggregate\n        agg: Dict[str, Any] = {\n            feat: \"mean\" for feat in self.get_feature_keys(ContinuousOutput)\n        }\n        agg[\"labcode\"] = lambda x: delimiter.join(sorted(x.tolist()))\n        for feat in self.get_feature_keys(OutputFeature):\n            agg[f\"valid_{feat}\"] = lambda x: 1\n\n        grouped = experiments.groupby(self.get_feature_keys(InputFeature))\n        duplicated_labcodes = [\n            sorted(group.labcode.to_numpy().tolist())\n            for _, group in grouped\n            if group.shape[0] &gt; 1\n        ]\n\n        experiments = grouped.aggregate(agg).reset_index(drop=False)\n        for feat in self.get_feature_keys(OutputFeature):\n            experiments.loc[experiments[feat].isna(), f\"valid_{feat}\"] = 0\n\n        experiments = experiments.sort_values(by=\"labcode\")\n        experiments = experiments.reset_index(drop=True)\n        return experiments, sorted(duplicated_labcodes)\n\n    def validate_experiments(\n        self,\n        experiments: pd.DataFrame,\n        strict: bool = False,\n    ) -&gt; pd.DataFrame:\n\"\"\"checks the experimental data on validity\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n\n        Raises:\n            ValueError: empty dataframe\n            ValueError: the column for a specific feature is missing the provided data\n            ValueError: there are labcodes with null value\n            ValueError: there are labcodes with nan value\n            ValueError: labcodes are not unique\n            ValueError: the provided columns do no match to the defined domain\n            ValueError: the provided columns do no match to the defined domain\n            ValueError: inputFeature with null values\n            ValueError: inputFeature with nan values\n\n        Returns:\n            pd.DataFrame: The provided dataframe with experimental data\n        \"\"\"\n\n        if len(experiments) == 0:\n            raise ValueError(\"no experiments provided (empty dataframe)\")\n        # check that each feature is a col\n        feature_keys = self.get_feature_keys()\n        for feature_key in feature_keys:\n            if feature_key not in experiments:\n                raise ValueError(f\"no col in experiments for feature {feature_key}\")\n        # add valid_{key} cols if missing\n        valid_keys = [\n            f\"valid_{output_feature_key}\"\n            for output_feature_key in self.get_feature_keys(OutputFeature)\n        ]\n        for valid_key in valid_keys:\n            if valid_key not in experiments:\n                experiments[valid_key] = True\n        # check all cols\n        expected = feature_keys + valid_keys\n        cols = list(experiments.columns)\n        # we allow here for a column named labcode used to identify experiments\n        if \"labcode\" in cols:\n            # test that labcodes are not na\n            if experiments.labcode.isnull().to_numpy().any():\n                raise ValueError(\"there are labcodes with null value\")\n            if experiments.labcode.isna().to_numpy().any():\n                raise ValueError(\"there are labcodes with nan value\")\n            # test that labcodes are distinct\n            if (\n                len(set(experiments.labcode.to_numpy().tolist()))\n                != experiments.shape[0]\n            ):\n                raise ValueError(\"labcodes are not unique\")\n            # we remove the labcode from the cols list to proceed as before\n            cols.remove(\"labcode\")\n        if len(expected) != len(cols):\n            raise ValueError(f\"expected the following cols: `{expected}`, got `{cols}`\")\n        if len(set(expected + cols)) != len(cols):\n            raise ValueError(f\"expected the following cols: `{expected}`, got `{cols}`\")\n        # check values of continuous input features\n        if experiments[self.get_feature_keys(InputFeature)].isnull().to_numpy().any():\n            raise ValueError(\"there are null values\")\n        if experiments[self.get_feature_keys(InputFeature)].isna().to_numpy().any():\n            raise ValueError(\"there are na values\")\n        # run the individual validators\n        for feat in self.get_features(InputFeature):\n            assert isinstance(feat, InputFeature)\n            feat.validate_experimental(experiments[feat.key], strict=strict)\n        return experiments\n\n    def describe_experiments(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n\n        Returns:\n            pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n        \"\"\"\n        data = {}\n        for feat in self.get_feature_keys(OutputFeature):\n            data[feat] = [\n                experiments.loc[experiments[feat].notna()].shape[0],\n                experiments.loc[experiments[feat].notna(), \"valid_%s\" % feat].sum(),\n            ]\n        preprocessed = self.outputs.preprocess_experiments_all_valid_outputs(\n            experiments\n        )\n        assert preprocessed is not None\n        data[\"all\"] = [\n            experiments.shape[0],\n            preprocessed.shape[0],\n        ]\n        return pd.DataFrame.from_dict(\n            data, orient=\"index\", columns=[\"measured\", \"valid\"]\n        )\n\n    def validate_candidates(\n        self, candidates: pd.DataFrame, only_inputs: bool = False\n    ) -&gt; pd.DataFrame:\n\"\"\"Method to check the validty of porposed candidates\n\n        Args:\n            candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates)\n            only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False.\n\n        Raises:\n            ValueError: when a column is missing for a defined input feature\n            ValueError: when a column is missing for a defined output feature\n            ValueError: when a non-numerical value is proposed\n            ValueError: when the constraints are not fulfilled\n            ValueError: when an additional column is found\n\n        Returns:\n            pd.DataFrame: dataframe with suggested experiments (candidates)\n        \"\"\"\n        # check that each input feature has a col and is valid in itself\n        assert isinstance(self.input_features, InputFeatures)\n        self.input_features.validate_inputs(candidates)\n        # check if all constraints are fulfilled\n        if not self.cnstrs.is_fulfilled(candidates).all():\n            raise ValueError(\"Constraints not fulfilled.\")\n        # for each continuous output feature with an attached objective object\n        if not only_inputs:\n            assert isinstance(self.output_features, OutputFeatures)\n            for key in self.output_features.get_keys_by_objective(Objective):\n                # check that pred, sd, and des cols are specified and numerical\n                for col in [f\"{key}_pred\", f\"{key}_sd\", f\"{key}_des\"]:\n                    if col not in candidates:\n                        raise ValueError(f\"missing column {col}\")\n                    if (not is_numeric(candidates[col])) and (\n                        not candidates[col].isnull().to_numpy().all()\n                    ):\n                        raise ValueError(\n                            f\"not all values of output feature `{key}` are numerical\"\n                        )\n            # validate no additional cols exist\n            if_count = len(self.get_features(InputFeature))\n            of_count = len(self.output_features.get_keys_by_objective(Objective))\n            # input features, prediction, standard deviation and reward for each output feature, 3 additional usefull infos: reward, aquisition function, strategy\n            if len(candidates.columns) != if_count + 3 * of_count:\n                raise ValueError(\"additional columns found\")\n        return candidates\n\n    @property\n    def experiment_column_names(self):\n\"\"\"the columns in the experimental dataframe\n\n        Returns:\n            List[str]: List of columns in the experiment dataframe (output feature keys + valid_output feature keys)\n        \"\"\"\n        return self.get_feature_keys() + [\n            f\"valid_{output_feature_key}\"\n            for output_feature_key in self.get_feature_keys(OutputFeature)\n        ]\n\n    @property\n    def candidate_column_names(self):\n\"\"\"the columns in the candidate dataframe\n\n        Returns:\n            List[str]: List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des)\n        \"\"\"\n        assert isinstance(self.output_features, OutputFeatures)\n        return (\n            self.get_feature_keys(InputFeature)\n            + [\n                f\"{output_feature_key}_pred\"\n                for output_feature_key in self.output_features.get_keys_by_objective(\n                    Objective\n                )\n            ]\n            + [\n                f\"{output_feature_key}_sd\"\n                for output_feature_key in self.output_features.get_keys_by_objective(\n                    Objective\n                )\n            ]\n            + [\n                f\"{output_feature_key}_des\"\n                for output_feature_key in self.output_features.get_keys_by_objective(\n                    Objective\n                )\n            ]\n        )\n\n    def set_candidates(self, candidates: pd.DataFrame):\n        candidates = self.validate_candidates(candidates)\n        self.candidates = candidates\n\n    def add_candidates(self, candidates: pd.DataFrame):\n        candidates = self.validate_candidates(candidates)\n        if candidates is None:\n            self.candidates = candidates\n        else:\n            self.candidates = pd.concat(\n                (self.candidates, candidates), ignore_index=True\n            )\n\n    @property\n    def num_candidates(self) -&gt; int:\n        if self.candidates is None:\n            return 0\n        return len(self.candidates)\n\n    def set_experiments(self, experiments: pd.DataFrame):\n        experiments = self.validate_experiments(experiments)\n        self.experiments = experiments\n\n    def add_experiments(self, experiments: pd.DataFrame):\n        experiments = self.validate_experiments(experiments)\n        if experiments is None:\n            self.experiments = None\n        elif self.experiments is None:\n            self.experiments = experiments\n        else:\n            self.experiments = pd.concat(\n                (self.experiments, experiments), ignore_index=True\n            )\n\n    def _set_constraints_unvalidated(\n        self, constraints: Union[Sequence[AnyConstraint], Constraints]\n    ):\n\"\"\"Hack for reduce_domain\"\"\"\n        self.constraints = Constraints(constraints=[])\n        if isinstance(constraints, Constraints):\n            constraints = constraints.constraints\n        self.constraints.constraints = constraints\n\n    @property\n    def num_experiments(self) -&gt; int:\n        if self.experiments is None:\n            return 0\n        return len(self.experiments)\n\n    @staticmethod\n    def from_dict(dict_: dict):\n        return parse_obj_as(Domain, dict_)\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.candidate_column_names","title":"<code>candidate_column_names</code>  <code>property</code> <code>readonly</code>","text":"<p>the columns in the candidate dataframe</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des)</p>"},{"location":"ref-domain/#bofire.domain.domain.Domain.experiment_column_names","title":"<code>experiment_column_names</code>  <code>property</code> <code>readonly</code>","text":"<p>the columns in the experimental dataframe</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of columns in the experiment dataframe (output feature keys + valid_output feature keys)</p>"},{"location":"ref-domain/#bofire.domain.domain.Domain.inputs","title":"<code>inputs: InputFeatures</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns input features as InputFeatures</p>"},{"location":"ref-domain/#bofire.domain.domain.Domain.outputs","title":"<code>outputs: OutputFeatures</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns output features as OutputFeatures</p>"},{"location":"ref-domain/#bofire.domain.domain.Domain.aggregate_by_duplicates","title":"<code>aggregate_by_duplicates(self, experiments, prec, delimiter='-')</code>","text":"<p>Aggregate the dataframe by duplicate experiments</p> <p>Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe containing experimental data</p> required <code>prec</code> <code>int</code> <p>Precision of the rounding of the continuous input features</p> required <code>delimiter</code> <code>str</code> <p>Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\".</p> <code>'-'</code> <p>Returns:</p> Type Description <code>Tuple[pd.DataFrame, list]</code> <p>Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def aggregate_by_duplicates(\n    self, experiments: pd.DataFrame, prec: int, delimiter: str = \"-\"\n) -&gt; Tuple[pd.DataFrame, list]:\n\"\"\"Aggregate the dataframe by duplicate experiments\n\n    Duplicates are identified based on the experiments with the same input features. Continuous input features\n    are rounded before identifying the duplicates. Aggregation is performed by taking the average of the\n    involved output features.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe containing experimental data\n        prec (int): Precision of the rounding of the continuous input features\n        delimiter (str, optional): Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\".\n\n    Returns:\n        Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates\n    \"\"\"\n    # prepare the parent frame\n\n    preprocessed = self.outputs.preprocess_experiments_any_valid_output(experiments)\n    assert preprocessed is not None\n    experiments = preprocessed.copy()\n    if \"labcode\" not in experiments.columns:\n        experiments[\"labcode\"] = [\n            str(i + 1).zfill(int(np.ceil(np.log10(experiments.shape[0]))))\n            for i in range(experiments.shape[0])\n        ]\n\n    # round it\n    experiments[self.get_feature_keys(ContinuousInput)] = experiments[\n        self.get_feature_keys(ContinuousInput)\n    ].round(prec)\n\n    # coerce invalid to nan\n    experiments = self.coerce_invalids(experiments)\n\n    # group and aggregate\n    agg: Dict[str, Any] = {\n        feat: \"mean\" for feat in self.get_feature_keys(ContinuousOutput)\n    }\n    agg[\"labcode\"] = lambda x: delimiter.join(sorted(x.tolist()))\n    for feat in self.get_feature_keys(OutputFeature):\n        agg[f\"valid_{feat}\"] = lambda x: 1\n\n    grouped = experiments.groupby(self.get_feature_keys(InputFeature))\n    duplicated_labcodes = [\n        sorted(group.labcode.to_numpy().tolist())\n        for _, group in grouped\n        if group.shape[0] &gt; 1\n    ]\n\n    experiments = grouped.aggregate(agg).reset_index(drop=False)\n    for feat in self.get_feature_keys(OutputFeature):\n        experiments.loc[experiments[feat].isna(), f\"valid_{feat}\"] = 0\n\n    experiments = experiments.sort_values(by=\"labcode\")\n    experiments = experiments.reset_index(drop=True)\n    return experiments, sorted(duplicated_labcodes)\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.coerce_invalids","title":"<code>coerce_invalids(self, experiments)</code>","text":"<p>Coerces all invalid output measurements to np.nan</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe containing experimental data</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>coerced dataframe</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def coerce_invalids(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Coerces all invalid output measurements to np.nan\n\n    Args:\n        experiments (pd.DataFrame): Dataframe containing experimental data\n\n    Returns:\n        pd.DataFrame: coerced dataframe\n    \"\"\"\n    # coerce invalid to nan\n    for feat in self.get_feature_keys(OutputFeature):\n        experiments.loc[experiments[f\"valid_{feat}\"] == 0, feat] = np.nan\n    return experiments\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.describe_experiments","title":"<code>describe_experiments(self, experiments)</code>","text":"<p>Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe with experimental data</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def describe_experiments(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n\n    Returns:\n        pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n    \"\"\"\n    data = {}\n    for feat in self.get_feature_keys(OutputFeature):\n        data[feat] = [\n            experiments.loc[experiments[feat].notna()].shape[0],\n            experiments.loc[experiments[feat].notna(), \"valid_%s\" % feat].sum(),\n        ]\n    preprocessed = self.outputs.preprocess_experiments_all_valid_outputs(\n        experiments\n    )\n    assert preprocessed is not None\n    data[\"all\"] = [\n        experiments.shape[0],\n        preprocessed.shape[0],\n    ]\n    return pd.DataFrame.from_dict(\n        data, orient=\"index\", columns=[\"measured\", \"valid\"]\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_constraint_reps_df","title":"<code>get_constraint_reps_df(self)</code>","text":"<p>Provides a tabular overwiev of all constraints within the domain</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>DataFrame listing all constraints of the domain with a description</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def get_constraint_reps_df(self):\n\"\"\"Provides a tabular overwiev of all constraints within the domain\n\n    Returns:\n        pd.DataFrame: DataFrame listing all constraints of the domain with a description\n    \"\"\"\n    df = pd.DataFrame(\n        index=range(len(self.constraints)),\n        columns=[\"Type\", \"Description\"],\n        data={\n            \"Type\": [feat.__class__.__name__ for feat in self.constraints],\n            \"Description\": [\n                constraint.__str__() for constraint in self.constraints\n            ],\n        },\n    )\n    return df\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_feature","title":"<code>get_feature(self, key)</code>","text":"<p>get a specific feature by its key</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Feature key</p> required <p>Returns:</p> Type Description <code>Feature</code> <p>The feature with the passed key</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def get_feature(self, key: str):\n\"\"\"get a specific feature by its key\n\n    Args:\n        key (str): Feature key\n\n    Returns:\n        Feature: The feature with the passed key\n    \"\"\"\n    assert isinstance(self.input_features, InputFeatures)\n    return {f.key: f for f in self.input_features + self.output_features}[key]\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_feature_keys","title":"<code>get_feature_keys(self, includes=&lt;class 'bofire.domain.features.Feature'&gt;, excludes=None, exact=False)</code>","text":"<p>Method to get feature keys of the domain</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be returned. Defaults to Feature.</p> <code>&lt;class 'bofire.domain.features.Feature'&gt;</code> <code>excludes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of feature keys fitting to the passed requirements.</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def get_feature_keys(\n    self,\n    includes: Union[Type, List[Type]] = Feature,\n    excludes: Union[Type, List[Type]] = None,\n    exact: bool = False,\n) -&gt; List[str]:\n\"\"\"Method to get feature keys of the domain\n\n    Args:\n        includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n        excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        List[str]: List of feature keys fitting to the passed requirements.\n    \"\"\"\n    return [\n        f.key\n        for f in self.get_features(\n            includes=includes,\n            excludes=excludes,\n            exact=exact,\n        )\n    ]\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_feature_reps_df","title":"<code>get_feature_reps_df(self)</code>","text":"<p>Returns a pandas dataframe describing the features contained in the optimization domain.</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def get_feature_reps_df(self) -&gt; pd.DataFrame:\n\"\"\"Returns a pandas dataframe describing the features contained in the optimization domain.\"\"\"\n    df = pd.DataFrame(\n        index=self.get_feature_keys(Feature),\n        columns=[\"Type\", \"Description\"],\n        data={\n            \"Type\": [\n                feat.__class__.__name__ for feat in self.get_features(Feature)\n            ],\n            \"Description\": [feat.__str__() for feat in self.get_features(Feature)],\n        },\n    )\n    return df\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_features","title":"<code>get_features(self, includes=&lt;class 'bofire.domain.features.Feature'&gt;, excludes=None, exact=False)</code>","text":"<p>get features of the domain</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be returned. Defaults to Feature.</p> <code>&lt;class 'bofire.domain.features.Feature'&gt;</code> <code>excludes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <code>by_attribute</code> <code>str</code> <p>If set it is filtered by the attribute specified in by <code>by_attribute</code>. Defaults to None.</p> required <p>Returns:</p> Type Description <code>List[Feature]</code> <p>List of features in the domain fitting to the passed requirements.</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def get_features(\n    self,\n    includes: Union[Type[Feature], List[Type[Feature]]] = Feature,\n    excludes: Union[Type[Feature], List[Type[Feature]], None] = None,\n    exact: bool = False,\n) -&gt; Features:\n\"\"\"get features of the domain\n\n    Args:\n        includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n        excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n        by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None.\n\n    Returns:\n        List[Feature]: List of features in the domain fitting to the passed requirements.\n    \"\"\"\n    assert isinstance(self.input_features, InputFeatures)\n    features = self.input_features + self.output_features\n    return features.get(includes, excludes, exact)\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_nchoosek_combinations","title":"<code>get_nchoosek_combinations(self)</code>","text":"<p>get all possible NChooseK combinations</p> <p>Returns:</p> Type Description <code>Tuple(used_features_list, unused_features_list)</code> <p>used_features_list is a list of lists containing features used in each NChooseK combination.  unused_features_list is a list of lists containing features unused in each NChooseK combination.</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def get_nchoosek_combinations(self):\n\"\"\"get all possible NChooseK combinations\n\n    Returns:\n        Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination.\n         unused_features_list is a list of lists containing features unused in each NChooseK combination.\n    \"\"\"\n\n    if len(self.cnstrs.get(NChooseKConstraint)) == 0:\n        used_continuous_features = self.get_feature_keys(ContinuousInput)\n        return used_continuous_features, []\n\n    used_features_list_all = []\n\n    # loops through each NChooseK constraint\n    for con in self.cnstrs.get(NChooseKConstraint):\n        assert isinstance(con, NChooseKConstraint)\n        used_features_list = []\n\n        for n in range(con.min_count, con.max_count + 1):\n            used_features_list.extend(itertools.combinations(con.features, n))\n\n        if con.none_also_valid:\n            used_features_list.append(tuple([]))\n\n        used_features_list_all.append(used_features_list)\n\n    used_features_list_all = list(\n        itertools.product(*used_features_list_all)\n    )  # product between NChooseK constraints\n\n    # format into a list of used features\n    used_features_list_formatted = []\n    for used_features_list in used_features_list_all:\n\n        used_features_list_flattened = [\n            item for sublist in used_features_list for item in sublist\n        ]\n        used_features_list_formatted.append(list(set(used_features_list_flattened)))\n\n    # sort lists\n    used_features_list_sorted = []\n    for used_features in used_features_list_formatted:\n        used_features_list_sorted.append(sorted(used_features))\n\n    # drop duplicates\n    used_features_list_no_dup = []\n    for used_features in used_features_list_sorted:\n        if used_features not in used_features_list_no_dup:\n            used_features_list_no_dup.append(used_features)\n\n    # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\")\n\n    # remove combinations not fulfilling constraints\n    used_features_list_final = []\n    for combo in used_features_list_no_dup:\n        fulfil_constraints = (\n            []\n        )  # list of bools tracking if constraints are fulfilled\n        for con in self.cnstrs.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            count = 0  # count of features in combo that are in con.features\n            for f in combo:\n                if f in con.features:\n                    count += 1\n            if count &gt;= con.min_count and count &lt;= con.max_count:\n                fulfil_constraints.append(True)\n            elif count == 0 and con.none_also_valid:\n                fulfil_constraints.append(True)\n            else:\n                fulfil_constraints.append(False)\n        if np.all(fulfil_constraints):\n            used_features_list_final.append(combo)\n\n    # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\")\n\n    # features unused\n    features_in_cc = []\n    for con in self.cnstrs.get(NChooseKConstraint):\n        assert isinstance(con, NChooseKConstraint)\n        features_in_cc.extend(con.features)\n    features_in_cc = list(set(features_in_cc))\n    features_in_cc.sort()\n    unused_features_list = []\n    for used_features in used_features_list_final:\n        unused_features_list.append(\n            [f_key for f_key in features_in_cc if f_key not in used_features]\n        )\n\n    # postprocess\n    # used_features_list_final2 = []\n    # unused_features_list2 = []\n    # for used, unused in zip(used_features_list_final,unused_features_list):\n    #     if len(used) == 3:\n    #         used_features_list_final2.append(used), unused_features_list2.append(unused)\n\n    return used_features_list_final, unused_features_list\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_candidates","title":"<code>validate_candidates(self, candidates, only_inputs=False)</code>","text":"<p>Method to check the validty of porposed candidates</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>pd.DataFrame</code> <p>Dataframe with suggested new experiments (candidates)</p> required <code>only_inputs</code> <code>bool,optional</code> <p>If True, only the input columns are validated. Defaults to False.</p> <code>False</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when a column is missing for a defined input feature</p> <code>ValueError</code> <p>when a column is missing for a defined output feature</p> <code>ValueError</code> <p>when a non-numerical value is proposed</p> <code>ValueError</code> <p>when the constraints are not fulfilled</p> <code>ValueError</code> <p>when an additional column is found</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>dataframe with suggested experiments (candidates)</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def validate_candidates(\n    self, candidates: pd.DataFrame, only_inputs: bool = False\n) -&gt; pd.DataFrame:\n\"\"\"Method to check the validty of porposed candidates\n\n    Args:\n        candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates)\n        only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False.\n\n    Raises:\n        ValueError: when a column is missing for a defined input feature\n        ValueError: when a column is missing for a defined output feature\n        ValueError: when a non-numerical value is proposed\n        ValueError: when the constraints are not fulfilled\n        ValueError: when an additional column is found\n\n    Returns:\n        pd.DataFrame: dataframe with suggested experiments (candidates)\n    \"\"\"\n    # check that each input feature has a col and is valid in itself\n    assert isinstance(self.input_features, InputFeatures)\n    self.input_features.validate_inputs(candidates)\n    # check if all constraints are fulfilled\n    if not self.cnstrs.is_fulfilled(candidates).all():\n        raise ValueError(\"Constraints not fulfilled.\")\n    # for each continuous output feature with an attached objective object\n    if not only_inputs:\n        assert isinstance(self.output_features, OutputFeatures)\n        for key in self.output_features.get_keys_by_objective(Objective):\n            # check that pred, sd, and des cols are specified and numerical\n            for col in [f\"{key}_pred\", f\"{key}_sd\", f\"{key}_des\"]:\n                if col not in candidates:\n                    raise ValueError(f\"missing column {col}\")\n                if (not is_numeric(candidates[col])) and (\n                    not candidates[col].isnull().to_numpy().all()\n                ):\n                    raise ValueError(\n                        f\"not all values of output feature `{key}` are numerical\"\n                    )\n        # validate no additional cols exist\n        if_count = len(self.get_features(InputFeature))\n        of_count = len(self.output_features.get_keys_by_objective(Objective))\n        # input features, prediction, standard deviation and reward for each output feature, 3 additional usefull infos: reward, aquisition function, strategy\n        if len(candidates.columns) != if_count + 3 * of_count:\n            raise ValueError(\"additional columns found\")\n    return candidates\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_constraints","title":"<code>validate_constraints(v, values)</code>  <code>classmethod</code>","text":"<p>Validate if all features included in the constraints are also defined as features for the domain.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>List[Constraint]</code> <p>List of constraints or empty if no constraints are defined</p> required <code>values</code> <code>List[InputFeature]</code> <p>List of input features of the domain</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>Feature key in constraint is unknown.</p> <p>Returns:</p> Type Description <code>List[Constraint]</code> <p>List of constraints defined for the domain</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>@validator(\"constraints\", always=True)\ndef validate_constraints(cls, v, values):\n\"\"\"Validate if all features included in the constraints are also defined as features for the domain.\n\n    Args:\n        v (List[Constraint]): List of constraints or empty if no constraints are defined\n        values (List[InputFeature]): List of input features of the domain\n\n    Raises:\n        ValueError: Feature key in constraint is unknown.\n\n    Returns:\n        List[Constraint]: List of constraints defined for the domain\n    \"\"\"\n    if \"input_features\" not in values:\n        return v\n    keys = [f.key for f in values[\"input_features\"]]\n    for c in v:\n        if isinstance(c, LinearConstraint) or isinstance(c, NChooseKConstraint):\n            for f in c.features:\n                if f not in keys:\n                    raise ValueError(f\"feature {f} in constraint unknown ({keys})\")\n    return v\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_experiments","title":"<code>validate_experiments(self, experiments, strict=False)</code>","text":"<p>checks the experimental data on validity</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe with experimental data</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>empty dataframe</p> <code>ValueError</code> <p>the column for a specific feature is missing the provided data</p> <code>ValueError</code> <p>there are labcodes with null value</p> <code>ValueError</code> <p>there are labcodes with nan value</p> <code>ValueError</code> <p>labcodes are not unique</p> <code>ValueError</code> <p>the provided columns do no match to the defined domain</p> <code>ValueError</code> <p>the provided columns do no match to the defined domain</p> <code>ValueError</code> <p>inputFeature with null values</p> <code>ValueError</code> <p>inputFeature with nan values</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>The provided dataframe with experimental data</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def validate_experiments(\n    self,\n    experiments: pd.DataFrame,\n    strict: bool = False,\n) -&gt; pd.DataFrame:\n\"\"\"checks the experimental data on validity\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n\n    Raises:\n        ValueError: empty dataframe\n        ValueError: the column for a specific feature is missing the provided data\n        ValueError: there are labcodes with null value\n        ValueError: there are labcodes with nan value\n        ValueError: labcodes are not unique\n        ValueError: the provided columns do no match to the defined domain\n        ValueError: the provided columns do no match to the defined domain\n        ValueError: inputFeature with null values\n        ValueError: inputFeature with nan values\n\n    Returns:\n        pd.DataFrame: The provided dataframe with experimental data\n    \"\"\"\n\n    if len(experiments) == 0:\n        raise ValueError(\"no experiments provided (empty dataframe)\")\n    # check that each feature is a col\n    feature_keys = self.get_feature_keys()\n    for feature_key in feature_keys:\n        if feature_key not in experiments:\n            raise ValueError(f\"no col in experiments for feature {feature_key}\")\n    # add valid_{key} cols if missing\n    valid_keys = [\n        f\"valid_{output_feature_key}\"\n        for output_feature_key in self.get_feature_keys(OutputFeature)\n    ]\n    for valid_key in valid_keys:\n        if valid_key not in experiments:\n            experiments[valid_key] = True\n    # check all cols\n    expected = feature_keys + valid_keys\n    cols = list(experiments.columns)\n    # we allow here for a column named labcode used to identify experiments\n    if \"labcode\" in cols:\n        # test that labcodes are not na\n        if experiments.labcode.isnull().to_numpy().any():\n            raise ValueError(\"there are labcodes with null value\")\n        if experiments.labcode.isna().to_numpy().any():\n            raise ValueError(\"there are labcodes with nan value\")\n        # test that labcodes are distinct\n        if (\n            len(set(experiments.labcode.to_numpy().tolist()))\n            != experiments.shape[0]\n        ):\n            raise ValueError(\"labcodes are not unique\")\n        # we remove the labcode from the cols list to proceed as before\n        cols.remove(\"labcode\")\n    if len(expected) != len(cols):\n        raise ValueError(f\"expected the following cols: `{expected}`, got `{cols}`\")\n    if len(set(expected + cols)) != len(cols):\n        raise ValueError(f\"expected the following cols: `{expected}`, got `{cols}`\")\n    # check values of continuous input features\n    if experiments[self.get_feature_keys(InputFeature)].isnull().to_numpy().any():\n        raise ValueError(\"there are null values\")\n    if experiments[self.get_feature_keys(InputFeature)].isna().to_numpy().any():\n        raise ValueError(\"there are na values\")\n    # run the individual validators\n    for feat in self.get_features(InputFeature):\n        assert isinstance(feat, InputFeature)\n        feat.validate_experimental(experiments[feat.key], strict=strict)\n    return experiments\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_linear_constraints","title":"<code>validate_linear_constraints(v, values)</code>  <code>classmethod</code>","text":"<p>Validate if all features included in linear constraints are continuous ones.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>List[Constraint]</code> <p>List of constraints or empty if no constraints are defined</p> required <code>values</code> <code>List[InputFeature]</code> <p>List of input features of the domain</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>description</p> <p>Returns:</p> Type Description <code>List[Constraint]</code> <p>List of constraints defined for the domain</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>@validator(\"constraints\", always=True)\ndef validate_linear_constraints(cls, v, values):\n\"\"\"Validate if all features included in linear constraints are continuous ones.\n\n    Args:\n        v (List[Constraint]): List of constraints or empty if no constraints are defined\n        values (List[InputFeature]): List of input features of the domain\n\n    Raises:\n        ValueError: _description_\n\n\n    Returns:\n       List[Constraint]: List of constraints defined for the domain\n    \"\"\"\n    if \"input_features\" not in values:\n        return v\n\n    # gather continuous input_features in dictionary\n    continuous_input_features_dict = {}\n    for f in values[\"input_features\"]:\n        if type(f) is ContinuousInput:\n            continuous_input_features_dict[f.key] = f\n\n    # check if non continuous input features appear in linear constraints\n    for c in v:\n        if isinstance(c, LinearConstraint):\n            for f in c.features:\n                assert (\n                    f in continuous_input_features_dict\n                ), f\"{f} must be continuous.\"\n    return v\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_lower_bounds_in_nchoosek_constraints","title":"<code>validate_lower_bounds_in_nchoosek_constraints(v, values)</code>  <code>classmethod</code>","text":"<p>Validate the lower bound as well if the chosen number of allowed features is continuous.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>List[Constraint]</code> <p>List of all constraints defined for the domain</p> required <code>values</code> <code>List[InputFeature]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[Constraint]</code> <p>List of constraints defined for the domain</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>@validator(\"constraints\", always=True)\ndef validate_lower_bounds_in_nchoosek_constraints(cls, v, values):\n\"\"\"Validate the lower bound as well if the chosen number of allowed features is continuous.\n\n    Args:\n        v (List[Constraint]): List of all constraints defined for the domain\n        values (List[InputFeature]): _description_\n\n    Returns:\n        List[Constraint]: List of constraints defined for the domain\n    \"\"\"\n    # gather continuous input_features in dictionary\n    continuous_input_features_dict = {}\n    for f in values[\"input_features\"]:\n        if type(f) is ContinuousInput:\n            continuous_input_features_dict[f.key] = f\n\n    # check if unfixed continuous features appearing in NChooseK constraints have lower bound of 0\n    for c in v:\n        if isinstance(c, NChooseKConstraint):\n            for f in c.features:\n                assert (\n                    f in continuous_input_features_dict\n                ), f\"{f} must be continuous.\"\n                assert (\n                    continuous_input_features_dict[f].lower_bound == 0\n                ), f\"lower bound of {f} must be 0 for NChooseK constraint.\"\n    return v\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_unique_feature_keys","title":"<code>validate_unique_feature_keys(v, values)</code>  <code>classmethod</code>","text":"<p>Validates if provided input and output feature keys are unique</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>OutputFeatures</code> <p>List of all output features of the domain.</p> required <code>value</code> <code>Dict[str, InputFeatures]</code> <p>Dict containing a list of input features as single entry.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>Feature keys are not unique.</p> <p>Returns:</p> Type Description <code>OutputFeatures</code> <p>Keeps output features as given.</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>@validator(\"output_features\", always=True)\ndef validate_unique_feature_keys(cls, v: OutputFeatures, values) -&gt; OutputFeatures:\n\"\"\"Validates if provided input and output feature keys are unique\n\n    Args:\n        v (OutputFeatures): List of all output features of the domain.\n        value (Dict[str, InputFeatures]): Dict containing a list of input features as single entry.\n\n    Raises:\n        ValueError: Feature keys are not unique.\n\n    Returns:\n        OutputFeatures: Keeps output features as given.\n    \"\"\"\n    if \"input_features\" not in values:\n        return v\n    features = v + values[\"input_features\"]\n    keys = [f.key for f in features]\n    if len(set(keys)) != len(keys):\n        raise ValueError(\"feature keys are not unique\")\n    return v\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.DomainError","title":"<code> DomainError            (Exception)         </code>","text":"<p>A class defining a specific domain error</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>class DomainError(Exception):\n\"\"\"A class defining a specific domain error\"\"\"\n\n    pass\n</code></pre>"},{"location":"ref-domain/#bofire.domain.domain.get_subdomain","title":"<code>get_subdomain(domain, feature_keys)</code>","text":"<p>removes all features not defined as argument creating a subdomain of the provided domain</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>the original domain wherefrom a subdomain should be created</p> required <code>feature_keys</code> <code>List</code> <p>List of features that shall be included in the subdomain</p> required <p>Exceptions:</p> Type Description <code>Assert</code> <p>when in total less than 2 features are provided</p> <code>ValueError</code> <p>when a provided feature key is not present in the provided domain</p> <code>Assert</code> <p>when no output feature is provided</p> <code>Assert</code> <p>when no input feature is provided</p> <code>ValueError</code> <p>description</p> <p>Returns:</p> Type Description <code>Domain</code> <p>A new domain containing only parts of the original domain</p> Source code in <code>bofire/domain/domain.py</code> <pre><code>def get_subdomain(\n    domain: Domain,\n    feature_keys: List,\n) -&gt; Domain:\n\"\"\"removes all features not defined as argument creating a subdomain of the provided domain\n\n    Args:\n        domain (Domain): the original domain wherefrom a subdomain should be created\n        feature_keys (List): List of features that shall be included in the subdomain\n\n    Raises:\n        Assert: when in total less than 2 features are provided\n        ValueError: when a provided feature key is not present in the provided domain\n        Assert: when no output feature is provided\n        Assert: when no input feature is provided\n        ValueError: _description_\n\n    Returns:\n        Domain: A new domain containing only parts of the original domain\n    \"\"\"\n    assert len(feature_keys) &gt;= 2, \"At least two features have to be provided.\"\n    output_features = []\n    input_features = []\n    for key in feature_keys:\n        try:\n            feat = domain.get_feature(key)\n        except KeyError:\n            raise ValueError(f\"Feature {key} not present in domain.\")\n        if isinstance(feat, InputFeature):\n            input_features.append(feat)\n        else:\n            output_features.append(feat)\n    assert len(output_features) &gt; 0, \"At least one output feature has to be provided.\"\n    assert len(input_features) &gt; 0, \"At least one input feature has to be provided.\"\n    input_features = InputFeatures(features=input_features)\n    # loop over constraints and make sure that all features used in constraints are in the input_feature_keys\n    for c in domain.constraints:\n        # TODO: fix type hint\n        for key in c.features:  # type: ignore\n            if key not in input_features.get_keys():\n                raise ValueError(\n                    f\"Removed input feature {key} is used in a constraint.\"\n                )\n    subdomain = deepcopy(domain)\n    subdomain.input_features = input_features\n    subdomain.output_features = output_features\n    return subdomain\n</code></pre>"},{"location":"ref-features/","title":"Domain","text":""},{"location":"ref-features/#bofire.domain.features.AnyOutputFeature","title":"<code> AnyOutputFeature            (OutputFeature)         </code>  <code>pydantic-model</code>","text":"<p>The base class for a continuous output feature</p> <p>Attributes:</p> Name Type Description <code>objective</code> <code>objective</code> <p>objective of the feature indicating in which direction it should be optimzed. Defaults to <code>MaximizeObjective</code>.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class ContinuousOutput(OutputFeature):\n\"\"\"The base class for a continuous output feature\n\n    Attributes:\n        objective (objective, optional): objective of the feature indicating in which direction it should be optimzed. Defaults to `MaximizeObjective`.\n    \"\"\"\n\n    type: Literal[\"ContinuousOutput\"] = \"ContinuousOutput\"\n    objective: Optional[AnyObjective] = Field(\n        default_factory=lambda: MaximizeObjective(w=1.0)\n    )\n\n    def plot(\n        self,\n        lower: float,\n        upper: float,\n        experiments: Optional[pd.DataFrame] = None,\n        plot_details: bool = True,\n        line_options: Optional[Dict] = None,\n        scatter_options: Optional[Dict] = None,\n        label_options: Optional[Dict] = None,\n        title_options: Optional[Dict] = None,\n    ):\n\"\"\"Plot the assigned objective.\n\n        Args:\n            lower (float): lower bound for the plot\n            upper (float): upper bound for the plot\n            experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None.\n        \"\"\"\n        if self.objective is None:\n            raise ValueError(\n                f\"No objective assigned for ContinuousOutputFeauture with key {self.key}.\"\n            )\n\n        line_options = line_options or {}\n        scatter_options = scatter_options or {}\n        label_options = label_options or {}\n        title_options = title_options or {}\n\n        line_options[\"color\"] = line_options.get(\"color\", \"black\")\n        scatter_options[\"color\"] = scatter_options.get(\"color\", \"red\")\n\n        x = pd.Series(np.linspace(lower, upper, 5000))\n        reward = self.objective.__call__(x)\n        fig, ax = plt.subplots()\n        ax.plot(x, reward, **line_options)\n        # TODO: validate dataframe\n        if experiments is not None:\n            x_data = experiments.loc[experiments[self.key].notna(), self.key].values\n            ax.scatter(\n                x_data,  # type: ignore\n                self.objective.__call__(x_data),  # type: ignore\n                **scatter_options,\n            )\n        ax.set_title(\"Objective %s\" % self.key, **title_options)\n        ax.set_ylabel(\"Objective\", **label_options)\n        ax.set_xlabel(self.key, **label_options)\n        if plot_details:\n            ax = self.objective.plot_details(ax=ax)\n        return fig, ax\n\n    def __str__(self) -&gt; str:\n        return \"ContinuousOutputFeature\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.AnyOutputFeature.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Return str(self).</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def __str__(self) -&gt; str:\n    return \"ContinuousOutputFeature\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.AnyOutputFeature.plot","title":"<code>plot(self, lower, upper, experiments=None, plot_details=True, line_options=None, scatter_options=None, label_options=None, title_options=None)</code>","text":"<p>Plot the assigned objective.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>float</code> <p>lower bound for the plot</p> required <code>upper</code> <code>float</code> <p>upper bound for the plot</p> required <code>experiments</code> <code>Optional[pd.DataFrame]</code> <p>If provided, scatter also the historical data in the plot. Defaults to None.</p> <code>None</code> Source code in <code>bofire/domain/features.py</code> <pre><code>def plot(\n    self,\n    lower: float,\n    upper: float,\n    experiments: Optional[pd.DataFrame] = None,\n    plot_details: bool = True,\n    line_options: Optional[Dict] = None,\n    scatter_options: Optional[Dict] = None,\n    label_options: Optional[Dict] = None,\n    title_options: Optional[Dict] = None,\n):\n\"\"\"Plot the assigned objective.\n\n    Args:\n        lower (float): lower bound for the plot\n        upper (float): upper bound for the plot\n        experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None.\n    \"\"\"\n    if self.objective is None:\n        raise ValueError(\n            f\"No objective assigned for ContinuousOutputFeauture with key {self.key}.\"\n        )\n\n    line_options = line_options or {}\n    scatter_options = scatter_options or {}\n    label_options = label_options or {}\n    title_options = title_options or {}\n\n    line_options[\"color\"] = line_options.get(\"color\", \"black\")\n    scatter_options[\"color\"] = scatter_options.get(\"color\", \"red\")\n\n    x = pd.Series(np.linspace(lower, upper, 5000))\n    reward = self.objective.__call__(x)\n    fig, ax = plt.subplots()\n    ax.plot(x, reward, **line_options)\n    # TODO: validate dataframe\n    if experiments is not None:\n        x_data = experiments.loc[experiments[self.key].notna(), self.key].values\n        ax.scatter(\n            x_data,  # type: ignore\n            self.objective.__call__(x_data),  # type: ignore\n            **scatter_options,\n        )\n    ax.set_title(\"Objective %s\" % self.key, **title_options)\n    ax.set_ylabel(\"Objective\", **label_options)\n    ax.set_xlabel(self.key, **label_options)\n    if plot_details:\n        ax = self.objective.plot_details(ax=ax)\n    return fig, ax\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput","title":"<code> CategoricalDescriptorInput            (CategoricalInput)         </code>  <code>pydantic-model</code>","text":"<p>Class for categorical input features with descriptors</p> <p>Attributes:</p> Name Type Description <code>categories</code> <code>List[str]</code> <p>Names of the categories.</p> <code>allowed</code> <code>List[bool]</code> <p>List of bools indicating if a category is allowed within the optimization.</p> <code>descriptors</code> <code>List[str]</code> <p>List of strings representing the names of the descriptors.</p> <code>values</code> <code>List[List[float]]</code> <p>List of lists representing the descriptor values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class CategoricalDescriptorInput(CategoricalInput):\n\"\"\"Class for categorical input features with descriptors\n\n    Attributes:\n        categories (List[str]): Names of the categories.\n        allowed (List[bool]): List of bools indicating if a category is allowed within the optimization.\n        descriptors (List[str]): List of strings representing the names of the descriptors.\n        values (List[List[float]]): List of lists representing the descriptor values.\n    \"\"\"\n\n    type: Literal[\"CategoricalDescriptorInput\"] = \"CategoricalDescriptorInput\"\n    descriptors: TDescriptors\n    values: TCategoricalDescriptorVals\n\n    @validator(\"descriptors\")\n    def validate_descriptors(cls, descriptors):\n\"\"\"validates that descriptors have unique names\n\n        Args:\n            categories (List[str]): List of descriptor names\n\n        Raises:\n            ValueError: when descriptors have non-unique names\n\n        Returns:\n            List[str]: List of the descriptors\n        \"\"\"\n        descriptors = [name2key(name) for name in descriptors]\n        if len(descriptors) != len(set(descriptors)):\n            raise ValueError(\"descriptors must be unique\")\n        return descriptors\n\n    @validator(\"values\")\n    def validate_values(cls, v, values):\n\"\"\"validates the compatability of passed values for the descriptors and the defined categories\n\n        Args:\n            v (List[List[float]]): Nested list with descriptor values\n            values (Dict): Dictionary with attributes\n\n        Raises:\n            ValueError: when values have different length than categories\n            ValueError: when rows in values have different length than descriptors\n            ValueError: when a descriptor shows no variance in the data\n\n        Returns:\n            List[List[float]]: Nested list with descriptor values\n        \"\"\"\n        if len(v) != len(values[\"categories\"]):\n            raise ValueError(\"values must have same length as categories\")\n        for row in v:\n            if len(row) != len(values[\"descriptors\"]):\n                raise ValueError(\"rows in values must have same length as descriptors\")\n        a = np.array(v)\n        for i, d in enumerate(values[\"descriptors\"]):\n            if len(set(a[:, i])) == 1:\n                raise ValueError(\"No variation for descriptor {d}.\")\n        return v\n\n    def to_df(self):\n\"\"\"tabular overview of the feature as DataFrame\n\n        Returns:\n            pd.DataFrame: tabular overview of the feature as DataFrame\n        \"\"\"\n        data = {cat: values for cat, values in zip(self.categories, self.values)}\n        return pd.DataFrame.from_dict(data, orient=\"index\", columns=self.descriptors)\n\n    def fixed_value(\n        self, transform_type: Optional[TTransform] = None\n    ) -&gt; Union[List[str], List[float], None]:\n\"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n        Returns:\n            List[str]: List of categories or None\n        \"\"\"\n        if transform_type != CategoricalEncodingEnum.DESCRIPTOR:\n            return super().fixed_value(transform_type)\n        else:\n            val = self.get_allowed_categories()[0]\n            return self.to_descriptor_encoding(pd.Series([val])).values[0].tolist()\n\n    def get_bounds(\n        self, transform_type: TTransform, values: Optional[pd.Series] = None\n    ) -&gt; Tuple[List[float], List[float]]:\n        if transform_type != CategoricalEncodingEnum.DESCRIPTOR:\n            return super().get_bounds(transform_type, values)\n        else:\n            # in case that values is None, we return the optimization bounds\n            # else we return the complete bounds\n            if values is None:\n                df = self.to_df().loc[self.get_allowed_categories()]\n            else:\n                df = self.to_df()\n            lower = df.min().values.tolist()  # type: ignore\n            upper = df.max().values.tolist()  # type: ignore\n            return lower, upper\n\n    def validate_experimental(\n        self, values: pd.Series, strict: bool = False\n    ) -&gt; pd.Series:\n\"\"\"Method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.\n\n        Raises:\n            ValueError: when an entry is not in the list of allowed categories\n            ValueError: when there is no variation in a feature provided by the experimental data\n            ValueError: when no variation is present or planed for a given descriptor\n\n        Returns:\n            pd.Series: A dataFrame with experiments\n        \"\"\"\n        values = super().validate_experimental(values, strict)\n        if strict:\n            lower, upper = self.get_bounds(\n                transform_type=CategoricalEncodingEnum.DESCRIPTOR, values=values\n            )\n            for i, desc in enumerate(self.descriptors):\n                if lower[i] == upper[i]:\n                    raise ValueError(\n                        f\"No variation present or planned for descriptor {desc} for feature {self.key}. Remove the descriptor.\"\n                    )\n        return values\n\n    @classmethod\n    def from_df(cls, key: str, df: pd.DataFrame):\n\"\"\"Creates a feature from a dataframe\n\n        Args:\n            key (str): The name of the feature\n            df (pd.DataFrame): Categories as rows and descriptors as columns\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        return cls(\n            key=key,\n            categories=list(df.index),\n            allowed=[True for _ in range(len(df))],\n            descriptors=list(df.columns),\n            values=df.values.tolist(),\n        )\n\n    def to_descriptor_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Converts values to descriptor encoding.\n\n        Args:\n            values (pd.Series): Values to transform.\n\n        Returns:\n            pd.DataFrame: Descriptor encoded dataframe.\n        \"\"\"\n        return pd.DataFrame(\n            data=values.map(\n                {cat: value for cat, value in zip(self.categories, self.values)}\n            ).values.tolist(),  # type: ignore\n            columns=[f\"{self.key}{_CAT_SEP}{d}\" for d in self.descriptors],\n            index=values.index,\n        )\n\n    def from_descriptor_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Converts values back from descriptor encoding.\n\n        Args:\n            values (pd.DataFrame): Descriptor encoded dataframe.\n\n        Raises:\n            ValueError: If descriptor columns not found in the dataframe.\n\n        Returns:\n            pd.Series: Series with categorical values.\n        \"\"\"\n        cat_cols = [f\"{self.key}{_CAT_SEP}{d}\" for d in self.descriptors]\n        # we allow here explicitly that the dataframe can have more columns than needed to have it\n        # easier in the backtransform.\n        if np.any([c not in values.columns for c in cat_cols]):\n            raise ValueError(\n                f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\"\n            )\n        s = pd.DataFrame(\n            data=np.sqrt(\n                np.sum(\n                    (\n                        values[cat_cols].to_numpy()[:, np.newaxis, :]\n                        - self.to_df().to_numpy()\n                    )\n                    ** 2,\n                    axis=2,\n                )\n            ),\n            columns=self.categories,\n            index=values.index,\n        ).idxmin(1)\n        s.name = self.key\n        return s\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.fixed_value","title":"<code>fixed_value(self, transform_type=None)</code>","text":"<p>Returns the categories to which the feature is fixed, None if the feature is not fixed</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of categories or None</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def fixed_value(\n    self, transform_type: Optional[TTransform] = None\n) -&gt; Union[List[str], List[float], None]:\n\"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n    Returns:\n        List[str]: List of categories or None\n    \"\"\"\n    if transform_type != CategoricalEncodingEnum.DESCRIPTOR:\n        return super().fixed_value(transform_type)\n    else:\n        val = self.get_allowed_categories()[0]\n        return self.to_descriptor_encoding(pd.Series([val])).values[0].tolist()\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.from_descriptor_encoding","title":"<code>from_descriptor_encoding(self, values)</code>","text":"<p>Converts values back from descriptor encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.DataFrame</code> <p>Descriptor encoded dataframe.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If descriptor columns not found in the dataframe.</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>Series with categorical values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def from_descriptor_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Converts values back from descriptor encoding.\n\n    Args:\n        values (pd.DataFrame): Descriptor encoded dataframe.\n\n    Raises:\n        ValueError: If descriptor columns not found in the dataframe.\n\n    Returns:\n        pd.Series: Series with categorical values.\n    \"\"\"\n    cat_cols = [f\"{self.key}{_CAT_SEP}{d}\" for d in self.descriptors]\n    # we allow here explicitly that the dataframe can have more columns than needed to have it\n    # easier in the backtransform.\n    if np.any([c not in values.columns for c in cat_cols]):\n        raise ValueError(\n            f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\"\n        )\n    s = pd.DataFrame(\n        data=np.sqrt(\n            np.sum(\n                (\n                    values[cat_cols].to_numpy()[:, np.newaxis, :]\n                    - self.to_df().to_numpy()\n                )\n                ** 2,\n                axis=2,\n            )\n        ),\n        columns=self.categories,\n        index=values.index,\n    ).idxmin(1)\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.from_df","title":"<code>from_df(key, df)</code>  <code>classmethod</code>","text":"<p>Creates a feature from a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The name of the feature</p> required <code>df</code> <code>pd.DataFrame</code> <p>Categories as rows and descriptors as columns</p> required <p>Returns:</p> Type Description <code>_type_</code> <p>description</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@classmethod\ndef from_df(cls, key: str, df: pd.DataFrame):\n\"\"\"Creates a feature from a dataframe\n\n    Args:\n        key (str): The name of the feature\n        df (pd.DataFrame): Categories as rows and descriptors as columns\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    return cls(\n        key=key,\n        categories=list(df.index),\n        allowed=[True for _ in range(len(df))],\n        descriptors=list(df.columns),\n        values=df.values.tolist(),\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.get_bounds","title":"<code>get_bounds(self, transform_type, values=None)</code>","text":"<p>Returns the bounds of an input feature depending on the requested transform type.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>Optional[TTransform]</code> <p>The requested transform type. Defaults to None.</p> required <code>values</code> <code>Optional[pd.Series]</code> <p>If values are provided the bounds are returned taking the most extreme values for the feature into account. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>List of lower bound values, list of upper bound values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_bounds(\n    self, transform_type: TTransform, values: Optional[pd.Series] = None\n) -&gt; Tuple[List[float], List[float]]:\n    if transform_type != CategoricalEncodingEnum.DESCRIPTOR:\n        return super().get_bounds(transform_type, values)\n    else:\n        # in case that values is None, we return the optimization bounds\n        # else we return the complete bounds\n        if values is None:\n            df = self.to_df().loc[self.get_allowed_categories()]\n        else:\n            df = self.to_df()\n        lower = df.min().values.tolist()  # type: ignore\n        upper = df.max().values.tolist()  # type: ignore\n        return lower, upper\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.to_descriptor_encoding","title":"<code>to_descriptor_encoding(self, values)</code>","text":"<p>Converts values to descriptor encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>Values to transform.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Descriptor encoded dataframe.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def to_descriptor_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Converts values to descriptor encoding.\n\n    Args:\n        values (pd.Series): Values to transform.\n\n    Returns:\n        pd.DataFrame: Descriptor encoded dataframe.\n    \"\"\"\n    return pd.DataFrame(\n        data=values.map(\n            {cat: value for cat, value in zip(self.categories, self.values)}\n        ).values.tolist(),  # type: ignore\n        columns=[f\"{self.key}{_CAT_SEP}{d}\" for d in self.descriptors],\n        index=values.index,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.to_df","title":"<code>to_df(self)</code>","text":"<p>tabular overview of the feature as DataFrame</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>tabular overview of the feature as DataFrame</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def to_df(self):\n\"\"\"tabular overview of the feature as DataFrame\n\n    Returns:\n        pd.DataFrame: tabular overview of the feature as DataFrame\n    \"\"\"\n    data = {cat: values for cat, values in zip(self.categories, self.values)}\n    return pd.DataFrame.from_dict(data, orient=\"index\", columns=self.descriptors)\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.validate_descriptors","title":"<code>validate_descriptors(descriptors)</code>  <code>classmethod</code>","text":"<p>validates that descriptors have unique names</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>List[str]</code> <p>List of descriptor names</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when descriptors have non-unique names</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of the descriptors</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@validator(\"descriptors\")\ndef validate_descriptors(cls, descriptors):\n\"\"\"validates that descriptors have unique names\n\n    Args:\n        categories (List[str]): List of descriptor names\n\n    Raises:\n        ValueError: when descriptors have non-unique names\n\n    Returns:\n        List[str]: List of the descriptors\n    \"\"\"\n    descriptors = [name2key(name) for name in descriptors]\n    if len(descriptors) != len(set(descriptors)):\n        raise ValueError(\"descriptors must be unique\")\n    return descriptors\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.validate_experimental","title":"<code>validate_experimental(self, values, strict=False)</code>","text":"<p>Method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when an entry is not in the list of allowed categories</p> <code>ValueError</code> <p>when there is no variation in a feature provided by the experimental data</p> <code>ValueError</code> <p>when no variation is present or planed for a given descriptor</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>A dataFrame with experiments</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_experimental(\n    self, values: pd.Series, strict: bool = False\n) -&gt; pd.Series:\n\"\"\"Method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.\n\n    Raises:\n        ValueError: when an entry is not in the list of allowed categories\n        ValueError: when there is no variation in a feature provided by the experimental data\n        ValueError: when no variation is present or planed for a given descriptor\n\n    Returns:\n        pd.Series: A dataFrame with experiments\n    \"\"\"\n    values = super().validate_experimental(values, strict)\n    if strict:\n        lower, upper = self.get_bounds(\n            transform_type=CategoricalEncodingEnum.DESCRIPTOR, values=values\n        )\n        for i, desc in enumerate(self.descriptors):\n            if lower[i] == upper[i]:\n                raise ValueError(\n                    f\"No variation present or planned for descriptor {desc} for feature {self.key}. Remove the descriptor.\"\n                )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.validate_values","title":"<code>validate_values(v, values)</code>  <code>classmethod</code>","text":"<p>validates the compatability of passed values for the descriptors and the defined categories</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>List[List[float]]</code> <p>Nested list with descriptor values</p> required <code>values</code> <code>Dict</code> <p>Dictionary with attributes</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when values have different length than categories</p> <code>ValueError</code> <p>when rows in values have different length than descriptors</p> <code>ValueError</code> <p>when a descriptor shows no variance in the data</p> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>Nested list with descriptor values</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@validator(\"values\")\ndef validate_values(cls, v, values):\n\"\"\"validates the compatability of passed values for the descriptors and the defined categories\n\n    Args:\n        v (List[List[float]]): Nested list with descriptor values\n        values (Dict): Dictionary with attributes\n\n    Raises:\n        ValueError: when values have different length than categories\n        ValueError: when rows in values have different length than descriptors\n        ValueError: when a descriptor shows no variance in the data\n\n    Returns:\n        List[List[float]]: Nested list with descriptor values\n    \"\"\"\n    if len(v) != len(values[\"categories\"]):\n        raise ValueError(\"values must have same length as categories\")\n    for row in v:\n        if len(row) != len(values[\"descriptors\"]):\n            raise ValueError(\"rows in values must have same length as descriptors\")\n    a = np.array(v)\n    for i, d in enumerate(values[\"descriptors\"]):\n        if len(set(a[:, i])) == 1:\n            raise ValueError(\"No variation for descriptor {d}.\")\n    return v\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput","title":"<code> CategoricalInput            (InputFeature)         </code>  <code>pydantic-model</code>","text":"<p>Base class for all categorical input features.</p> <p>Attributes:</p> Name Type Description <code>categories</code> <code>List[str]</code> <p>Names of the categories.</p> <code>allowed</code> <code>List[bool]</code> <p>List of bools indicating if a category is allowed within the optimization.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class CategoricalInput(InputFeature):\n\"\"\"Base class for all categorical input features.\n\n    Attributes:\n        categories (List[str]): Names of the categories.\n        allowed (List[bool]): List of bools indicating if a category is allowed within the optimization.\n    \"\"\"\n\n    type: Literal[\"CategoricalInput\"] = \"CategoricalInput\"\n    categories: TCategoryVals\n    allowed: TAllowedVals = None\n\n    @validator(\"categories\")\n    def validate_categories_unique(cls, categories):\n\"\"\"validates that categories have unique names\n\n        Args:\n            categories (List[str]): List of category names\n\n        Raises:\n            ValueError: when categories have non-unique names\n\n        Returns:\n            List[str]: List of the categories\n        \"\"\"\n        categories = [name2key(name) for name in categories]\n        if len(categories) != len(set(categories)):\n            raise ValueError(\"categories must be unique\")\n        return categories\n\n    @root_validator(pre=False, skip_on_failure=True)\n    def init_allowed(cls, values):\n\"\"\"validates the list of allowed/not allowed categories\n\n        Args:\n            values (Dict): Dictionary with attributes\n\n        Raises:\n            ValueError: when the number of allowences does not fit to the number of categories\n            ValueError: when no category is allowed\n\n        Returns:\n            Dict: Dictionary with attributes\n        \"\"\"\n        if \"categories\" not in values or values[\"categories\"] is None:\n            return values\n        if \"allowed\" not in values or values[\"allowed\"] is None:\n            values[\"allowed\"] = [True for _ in range(len(values[\"categories\"]))]\n        if len(values[\"allowed\"]) != len(values[\"categories\"]):\n            raise ValueError(\"allowed must have same length as categories\")\n        if sum(values[\"allowed\"]) == 0:\n            raise ValueError(\"no category is allowed\")\n        return values\n\n    def is_fixed(self) -&gt; bool:\n\"\"\"Returns True if there is only one allowed category.\n\n        Returns:\n            [bool]: True if there is only one allowed category\n        \"\"\"\n        if self.allowed is None:\n            return False\n        return sum(self.allowed) == 1\n\n    def fixed_value(\n        self, transform_type: Optional[TTransform] = None\n    ) -&gt; Union[List[str], List[float], None]:\n\"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n        Returns:\n            List[str]: List of categories or None\n        \"\"\"\n        if self.is_fixed():\n            val = self.get_allowed_categories()[0]\n            if transform_type is None:\n                return [val]\n            elif transform_type == CategoricalEncodingEnum.ONE_HOT:\n                return self.to_onehot_encoding(pd.Series([val])).values[0].tolist()\n            elif transform_type == CategoricalEncodingEnum.DUMMY:\n                return self.to_dummy_encoding(pd.Series([val])).values[0].tolist()\n            elif transform_type == CategoricalEncodingEnum.ORDINAL:\n                return self.to_ordinal_encoding(pd.Series([val])).tolist()\n            else:\n                raise ValueError(\n                    f\"Unkwon transform type {transform_type} for categorical input {self.key}\"\n                )\n        else:\n            return None\n\n    def get_allowed_categories(self):\n\"\"\"Returns the allowed categories.\n\n        Returns:\n            list of str: The allowed categories\n        \"\"\"\n        if self.allowed is None:\n            return []\n        return [c for c, a in zip(self.categories, self.allowed) if a]\n\n    def validate_experimental(\n        self, values: pd.Series, strict: bool = False\n    ) -&gt; pd.Series:\n\"\"\"Method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.\n\n        Raises:\n            ValueError: when an entry is not in the list of allowed categories\n            ValueError: when there is no variation in a feature provided by the experimental data\n\n        Returns:\n            pd.Series: A dataFrame with experiments\n        \"\"\"\n        if sum(values.isin(self.categories)) != len(values):\n            raise ValueError(\n                f\"invalid values for `{self.key}`, allowed are: `{self.categories}`\"\n            )\n        if strict:\n            possible_categories = self.get_possible_categories(values)\n            if len(possible_categories) != len(self.categories):\n                raise ValueError(\n                    f\"Categories {list(set(self.categories)-set(possible_categories))} of feature {self.key} not used. Remove them.\"\n                )\n        return values\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Method to validate the suggested candidates\n\n        Args:\n            values (pd.Series): A dataFrame with candidates\n\n        Raises:\n            ValueError: when not all values for a feature are one of the allowed categories\n\n        Returns:\n            pd.Series: The passed dataFrame with candidates\n        \"\"\"\n        if sum(values.isin(self.get_allowed_categories())) != len(values):\n            raise ValueError(\n                f\"not all values of input feature `{self.key}` are a valid allowed category from {self.get_allowed_categories()}\"\n            )\n        return values\n\n    def get_forbidden_categories(self):\n\"\"\"Returns the non-allowed categories\n\n        Returns:\n            List[str]: List of the non-allowed categories\n        \"\"\"\n        return list(set(self.categories) - set(self.get_allowed_categories()))\n\n    def get_possible_categories(self, values: pd.Series) -&gt; list:\n\"\"\"Return the superset of categories that have been used in the experimental dataset and\n        that can be used in the optimization\n\n        Args:\n            values (pd.Series): Series with the values for this feature\n\n        Returns:\n            list: list of possible categories\n        \"\"\"\n        return sorted(\n            list(set(list(set(values.tolist())) + self.get_allowed_categories()))\n        )\n\n    def to_onehot_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Converts values to a one-hot encoding.\n\n        Args:\n            values (pd.Series): Series to be transformed.\n\n        Returns:\n            pd.DataFrame: One-hot transformed data frame.\n        \"\"\"\n        return pd.DataFrame(\n            {f\"{self.key}{_CAT_SEP}{c}\": values == c for c in self.categories},\n            dtype=float,\n            index=values.index,\n        )\n\n    def from_onehot_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Converts values back from one-hot encoding.\n\n        Args:\n            values (pd.DataFrame): One-hot encoded values.\n\n        Raises:\n            ValueError: If one-hot columns not present in `values`.\n\n        Returns:\n            pd.Series: Series with categorical values.\n        \"\"\"\n        cat_cols = [f\"{self.key}{_CAT_SEP}{c}\" for c in self.categories]\n        # we allow here explicitly that the dataframe can have more columns than needed to have it\n        # easier in the backtransform.\n        if np.any([c not in values.columns for c in cat_cols]):\n            raise ValueError(\n                f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\"\n            )\n        s = values[cat_cols].idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n        s.name = self.key\n        return s\n\n    def to_dummy_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Converts values to a dummy-hot encoding, dropping the first categorical level.\n\n        Args:\n            values (pd.Series): Series to be transformed.\n\n        Returns:\n            pd.DataFrame: Dummy-hot transformed data frame.\n        \"\"\"\n        return pd.DataFrame(\n            {f\"{self.key}{_CAT_SEP}{c}\": values == c for c in self.categories[1:]},\n            dtype=float,\n            index=values.index,\n        )\n\n    def from_dummy_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Convert points back from dummy encoding.\n\n        Args:\n            values (pd.DataFrame): Dummy-hot encoded values.\n\n        Raises:\n            ValueError: If one-hot columns not present in `values`.\n\n        Returns:\n            pd.Series: Series with categorical values.\n        \"\"\"\n        cat_cols = [f\"{self.key}{_CAT_SEP}{c}\" for c in self.categories]\n        # we allow here explicitly that the dataframe can have more columns than needed to have it\n        # easier in the backtransform.\n        if np.any([c not in values.columns for c in cat_cols[1:]]):\n            raise ValueError(\n                f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols[1:]}.\"\n            )\n        values = values.copy()\n        values[cat_cols[0]] = 1 - values[cat_cols[1:]].sum(axis=1)\n        s = values[cat_cols].idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n        s.name = self.key\n        return s\n\n    def to_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Converts values to an ordinal integer based encoding.\n\n        Args:\n            values (pd.Series): Series to be transformed.\n\n        Returns:\n            pd.Series: Ordinal encoded values.\n        \"\"\"\n        enc = pd.Series(range(len(self.categories)), index=list(self.categories))\n        s = enc[values]\n        s.index = values.index\n        s.name = self.key\n        return s\n\n    def from_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Convertes values back from ordinal encoding.\n\n        Args:\n            values (pd.Series): Ordinal encoded series.\n\n        Returns:\n            pd.Series: Series with categorical values.\n        \"\"\"\n        enc = np.array(self.categories)\n        return pd.Series(enc[values], index=values.index, name=self.key)\n\n    def sample(self, n: int) -&gt; pd.Series:\n\"\"\"Draw random samples from the feature.\n\n        Args:\n            n (int): number of samples.\n\n        Returns:\n            pd.Series: drawn samples.\n        \"\"\"\n        return pd.Series(\n            name=self.key, data=np.random.choice(self.get_allowed_categories(), n)\n        )\n\n    def get_bounds(\n        self,\n        transform_type: TTransform,\n        values: Optional[pd.Series] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        assert isinstance(transform_type, CategoricalEncodingEnum)\n        if transform_type == CategoricalEncodingEnum.ORDINAL:\n            return [0], [len(self.categories) - 1]\n        if transform_type == CategoricalEncodingEnum.ONE_HOT:\n            # in the case that values are None, we return the bounds\n            # based on the optimization bounds, else we return the true\n            # bounds as this is for model fitting.\n            if values is None:\n                lower = [0.0 for _ in self.categories]\n                upper = [\n                    1.0 if self.allowed[i] is True else 0.0  # type: ignore\n                    for i, _ in enumerate(self.categories)\n                ]\n            else:\n                lower = [0.0 for _ in self.categories]\n                upper = [1.0 for _ in self.categories]\n            return lower, upper\n        if transform_type == CategoricalEncodingEnum.DUMMY:\n            lower = [0.0 for _ in range(len(self.categories) - 1)]\n            upper = [1.0 for _ in range(len(self.categories) - 1)]\n            return lower, upper\n        if transform_type == CategoricalEncodingEnum.DESCRIPTOR:\n            raise ValueError(\n                f\"Invalid descriptor transform for categorical {self.key}.\"\n            )\n        else:\n            raise ValueError(\n                f\"Invalid transform_type {transform_type} provided for categorical {self.key}.\"\n            )\n\n    def __str__(self) -&gt; str:\n\"\"\"Returns the number of categories as str\n\n        Returns:\n            str: Number of categories\n        \"\"\"\n        return f\"{len(self.categories)} categories\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Returns the number of categories as str</p> <p>Returns:</p> Type Description <code>str</code> <p>Number of categories</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def __str__(self) -&gt; str:\n\"\"\"Returns the number of categories as str\n\n    Returns:\n        str: Number of categories\n    \"\"\"\n    return f\"{len(self.categories)} categories\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.fixed_value","title":"<code>fixed_value(self, transform_type=None)</code>","text":"<p>Returns the categories to which the feature is fixed, None if the feature is not fixed</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of categories or None</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def fixed_value(\n    self, transform_type: Optional[TTransform] = None\n) -&gt; Union[List[str], List[float], None]:\n\"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n    Returns:\n        List[str]: List of categories or None\n    \"\"\"\n    if self.is_fixed():\n        val = self.get_allowed_categories()[0]\n        if transform_type is None:\n            return [val]\n        elif transform_type == CategoricalEncodingEnum.ONE_HOT:\n            return self.to_onehot_encoding(pd.Series([val])).values[0].tolist()\n        elif transform_type == CategoricalEncodingEnum.DUMMY:\n            return self.to_dummy_encoding(pd.Series([val])).values[0].tolist()\n        elif transform_type == CategoricalEncodingEnum.ORDINAL:\n            return self.to_ordinal_encoding(pd.Series([val])).tolist()\n        else:\n            raise ValueError(\n                f\"Unkwon transform type {transform_type} for categorical input {self.key}\"\n            )\n    else:\n        return None\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.from_dummy_encoding","title":"<code>from_dummy_encoding(self, values)</code>","text":"<p>Convert points back from dummy encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.DataFrame</code> <p>Dummy-hot encoded values.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If one-hot columns not present in <code>values</code>.</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>Series with categorical values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def from_dummy_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Convert points back from dummy encoding.\n\n    Args:\n        values (pd.DataFrame): Dummy-hot encoded values.\n\n    Raises:\n        ValueError: If one-hot columns not present in `values`.\n\n    Returns:\n        pd.Series: Series with categorical values.\n    \"\"\"\n    cat_cols = [f\"{self.key}{_CAT_SEP}{c}\" for c in self.categories]\n    # we allow here explicitly that the dataframe can have more columns than needed to have it\n    # easier in the backtransform.\n    if np.any([c not in values.columns for c in cat_cols[1:]]):\n        raise ValueError(\n            f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols[1:]}.\"\n        )\n    values = values.copy()\n    values[cat_cols[0]] = 1 - values[cat_cols[1:]].sum(axis=1)\n    s = values[cat_cols].idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.from_onehot_encoding","title":"<code>from_onehot_encoding(self, values)</code>","text":"<p>Converts values back from one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.DataFrame</code> <p>One-hot encoded values.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If one-hot columns not present in <code>values</code>.</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>Series with categorical values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def from_onehot_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n\"\"\"Converts values back from one-hot encoding.\n\n    Args:\n        values (pd.DataFrame): One-hot encoded values.\n\n    Raises:\n        ValueError: If one-hot columns not present in `values`.\n\n    Returns:\n        pd.Series: Series with categorical values.\n    \"\"\"\n    cat_cols = [f\"{self.key}{_CAT_SEP}{c}\" for c in self.categories]\n    # we allow here explicitly that the dataframe can have more columns than needed to have it\n    # easier in the backtransform.\n    if np.any([c not in values.columns for c in cat_cols]):\n        raise ValueError(\n            f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\"\n        )\n    s = values[cat_cols].idxmax(1).str.split(_CAT_SEP, expand=True)[1]\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.from_ordinal_encoding","title":"<code>from_ordinal_encoding(self, values)</code>","text":"<p>Convertes values back from ordinal encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>Ordinal encoded series.</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>Series with categorical values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def from_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Convertes values back from ordinal encoding.\n\n    Args:\n        values (pd.Series): Ordinal encoded series.\n\n    Returns:\n        pd.Series: Series with categorical values.\n    \"\"\"\n    enc = np.array(self.categories)\n    return pd.Series(enc[values], index=values.index, name=self.key)\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.get_allowed_categories","title":"<code>get_allowed_categories(self)</code>","text":"<p>Returns the allowed categories.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>The allowed categories</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_allowed_categories(self):\n\"\"\"Returns the allowed categories.\n\n    Returns:\n        list of str: The allowed categories\n    \"\"\"\n    if self.allowed is None:\n        return []\n    return [c for c, a in zip(self.categories, self.allowed) if a]\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.get_bounds","title":"<code>get_bounds(self, transform_type, values=None)</code>","text":"<p>Returns the bounds of an input feature depending on the requested transform type.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>Optional[TTransform]</code> <p>The requested transform type. Defaults to None.</p> required <code>values</code> <code>Optional[pd.Series]</code> <p>If values are provided the bounds are returned taking the most extreme values for the feature into account. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>List of lower bound values, list of upper bound values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_bounds(\n    self,\n    transform_type: TTransform,\n    values: Optional[pd.Series] = None,\n) -&gt; Tuple[List[float], List[float]]:\n    assert isinstance(transform_type, CategoricalEncodingEnum)\n    if transform_type == CategoricalEncodingEnum.ORDINAL:\n        return [0], [len(self.categories) - 1]\n    if transform_type == CategoricalEncodingEnum.ONE_HOT:\n        # in the case that values are None, we return the bounds\n        # based on the optimization bounds, else we return the true\n        # bounds as this is for model fitting.\n        if values is None:\n            lower = [0.0 for _ in self.categories]\n            upper = [\n                1.0 if self.allowed[i] is True else 0.0  # type: ignore\n                for i, _ in enumerate(self.categories)\n            ]\n        else:\n            lower = [0.0 for _ in self.categories]\n            upper = [1.0 for _ in self.categories]\n        return lower, upper\n    if transform_type == CategoricalEncodingEnum.DUMMY:\n        lower = [0.0 for _ in range(len(self.categories) - 1)]\n        upper = [1.0 for _ in range(len(self.categories) - 1)]\n        return lower, upper\n    if transform_type == CategoricalEncodingEnum.DESCRIPTOR:\n        raise ValueError(\n            f\"Invalid descriptor transform for categorical {self.key}.\"\n        )\n    else:\n        raise ValueError(\n            f\"Invalid transform_type {transform_type} provided for categorical {self.key}.\"\n        )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.get_forbidden_categories","title":"<code>get_forbidden_categories(self)</code>","text":"<p>Returns the non-allowed categories</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of the non-allowed categories</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_forbidden_categories(self):\n\"\"\"Returns the non-allowed categories\n\n    Returns:\n        List[str]: List of the non-allowed categories\n    \"\"\"\n    return list(set(self.categories) - set(self.get_allowed_categories()))\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.get_possible_categories","title":"<code>get_possible_categories(self, values)</code>","text":"<p>Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>Series with the values for this feature</p> required <p>Returns:</p> Type Description <code>list</code> <p>list of possible categories</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_possible_categories(self, values: pd.Series) -&gt; list:\n\"\"\"Return the superset of categories that have been used in the experimental dataset and\n    that can be used in the optimization\n\n    Args:\n        values (pd.Series): Series with the values for this feature\n\n    Returns:\n        list: list of possible categories\n    \"\"\"\n    return sorted(\n        list(set(list(set(values.tolist())) + self.get_allowed_categories()))\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.init_allowed","title":"<code>init_allowed(values)</code>  <code>classmethod</code>","text":"<p>validates the list of allowed/not allowed categories</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dictionary with attributes</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when the number of allowences does not fit to the number of categories</p> <code>ValueError</code> <p>when no category is allowed</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with attributes</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@root_validator(pre=False, skip_on_failure=True)\ndef init_allowed(cls, values):\n\"\"\"validates the list of allowed/not allowed categories\n\n    Args:\n        values (Dict): Dictionary with attributes\n\n    Raises:\n        ValueError: when the number of allowences does not fit to the number of categories\n        ValueError: when no category is allowed\n\n    Returns:\n        Dict: Dictionary with attributes\n    \"\"\"\n    if \"categories\" not in values or values[\"categories\"] is None:\n        return values\n    if \"allowed\" not in values or values[\"allowed\"] is None:\n        values[\"allowed\"] = [True for _ in range(len(values[\"categories\"]))]\n    if len(values[\"allowed\"]) != len(values[\"categories\"]):\n        raise ValueError(\"allowed must have same length as categories\")\n    if sum(values[\"allowed\"]) == 0:\n        raise ValueError(\"no category is allowed\")\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.is_fixed","title":"<code>is_fixed(self)</code>","text":"<p>Returns True if there is only one allowed category.</p> <p>Returns:</p> Type Description <code>[bool]</code> <p>True if there is only one allowed category</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def is_fixed(self) -&gt; bool:\n\"\"\"Returns True if there is only one allowed category.\n\n    Returns:\n        [bool]: True if there is only one allowed category\n    \"\"\"\n    if self.allowed is None:\n        return False\n    return sum(self.allowed) == 1\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.sample","title":"<code>sample(self, n)</code>","text":"<p>Draw random samples from the feature.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples.</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>drawn samples.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def sample(self, n: int) -&gt; pd.Series:\n\"\"\"Draw random samples from the feature.\n\n    Args:\n        n (int): number of samples.\n\n    Returns:\n        pd.Series: drawn samples.\n    \"\"\"\n    return pd.Series(\n        name=self.key, data=np.random.choice(self.get_allowed_categories(), n)\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.to_dummy_encoding","title":"<code>to_dummy_encoding(self, values)</code>","text":"<p>Converts values to a dummy-hot encoding, dropping the first categorical level.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>Series to be transformed.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dummy-hot transformed data frame.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def to_dummy_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Converts values to a dummy-hot encoding, dropping the first categorical level.\n\n    Args:\n        values (pd.Series): Series to be transformed.\n\n    Returns:\n        pd.DataFrame: Dummy-hot transformed data frame.\n    \"\"\"\n    return pd.DataFrame(\n        {f\"{self.key}{_CAT_SEP}{c}\": values == c for c in self.categories[1:]},\n        dtype=float,\n        index=values.index,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.to_onehot_encoding","title":"<code>to_onehot_encoding(self, values)</code>","text":"<p>Converts values to a one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>Series to be transformed.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>One-hot transformed data frame.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def to_onehot_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n\"\"\"Converts values to a one-hot encoding.\n\n    Args:\n        values (pd.Series): Series to be transformed.\n\n    Returns:\n        pd.DataFrame: One-hot transformed data frame.\n    \"\"\"\n    return pd.DataFrame(\n        {f\"{self.key}{_CAT_SEP}{c}\": values == c for c in self.categories},\n        dtype=float,\n        index=values.index,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.to_ordinal_encoding","title":"<code>to_ordinal_encoding(self, values)</code>","text":"<p>Converts values to an ordinal integer based encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>Series to be transformed.</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>Ordinal encoded values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def to_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Converts values to an ordinal integer based encoding.\n\n    Args:\n        values (pd.Series): Series to be transformed.\n\n    Returns:\n        pd.Series: Ordinal encoded values.\n    \"\"\"\n    enc = pd.Series(range(len(self.categories)), index=list(self.categories))\n    s = enc[values]\n    s.index = values.index\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.validate_candidental","title":"<code>validate_candidental(self, values)</code>","text":"<p>Method to validate the suggested candidates</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>A dataFrame with candidates</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when not all values for a feature are one of the allowed categories</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>The passed dataFrame with candidates</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Method to validate the suggested candidates\n\n    Args:\n        values (pd.Series): A dataFrame with candidates\n\n    Raises:\n        ValueError: when not all values for a feature are one of the allowed categories\n\n    Returns:\n        pd.Series: The passed dataFrame with candidates\n    \"\"\"\n    if sum(values.isin(self.get_allowed_categories())) != len(values):\n        raise ValueError(\n            f\"not all values of input feature `{self.key}` are a valid allowed category from {self.get_allowed_categories()}\"\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.validate_categories_unique","title":"<code>validate_categories_unique(categories)</code>  <code>classmethod</code>","text":"<p>validates that categories have unique names</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>List[str]</code> <p>List of category names</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when categories have non-unique names</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of the categories</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@validator(\"categories\")\ndef validate_categories_unique(cls, categories):\n\"\"\"validates that categories have unique names\n\n    Args:\n        categories (List[str]): List of category names\n\n    Raises:\n        ValueError: when categories have non-unique names\n\n    Returns:\n        List[str]: List of the categories\n    \"\"\"\n    categories = [name2key(name) for name in categories]\n    if len(categories) != len(set(categories)):\n        raise ValueError(\"categories must be unique\")\n    return categories\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.validate_experimental","title":"<code>validate_experimental(self, values, strict=False)</code>","text":"<p>Method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when an entry is not in the list of allowed categories</p> <code>ValueError</code> <p>when there is no variation in a feature provided by the experimental data</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>A dataFrame with experiments</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_experimental(\n    self, values: pd.Series, strict: bool = False\n) -&gt; pd.Series:\n\"\"\"Method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.\n\n    Raises:\n        ValueError: when an entry is not in the list of allowed categories\n        ValueError: when there is no variation in a feature provided by the experimental data\n\n    Returns:\n        pd.Series: A dataFrame with experiments\n    \"\"\"\n    if sum(values.isin(self.categories)) != len(values):\n        raise ValueError(\n            f\"invalid values for `{self.key}`, allowed are: `{self.categories}`\"\n        )\n    if strict:\n        possible_categories = self.get_possible_categories(values)\n        if len(possible_categories) != len(self.categories):\n            raise ValueError(\n                f\"Categories {list(set(self.categories)-set(possible_categories))} of feature {self.key} not used. Remove them.\"\n            )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput","title":"<code> ContinuousDescriptorInput            (ContinuousInput)         </code>  <code>pydantic-model</code>","text":"<p>Class for continuous input features with descriptors</p> <p>Attributes:</p> Name Type Description <code>lower_bound</code> <code>float</code> <p>Lower bound of the feature in the optimization.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound of the feature in the optimization.</p> <code>descriptors</code> <code>List[str]</code> <p>Names of the descriptors.</p> <code>values</code> <code>List[float]</code> <p>Values of the descriptors.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class ContinuousDescriptorInput(ContinuousInput):\n\"\"\"Class for continuous input features with descriptors\n\n    Attributes:\n        lower_bound (float): Lower bound of the feature in the optimization.\n        upper_bound (float): Upper bound of the feature in the optimization.\n        descriptors (List[str]): Names of the descriptors.\n        values (List[float]): Values of the descriptors.\n    \"\"\"\n\n    type: Literal[\"ContinuousDescriptorInput\"] = \"ContinuousDescriptorInput\"\n    descriptors: TDescriptors\n    values: TDiscreteVals\n\n    @validator(\"descriptors\")\n    def descriptors_to_keys(cls, descriptors):\n\"\"\"validates the descriptor names and transforms it to valid keys\n\n        Args:\n            descriptors (List[str]): List of descriptor names\n\n        Returns:\n            List[str]: List of valid keys\n        \"\"\"\n        return [name2key(name) for name in descriptors]\n\n    @root_validator(pre=False, skip_on_failure=True)\n    def validate_list_lengths(cls, values):\n\"\"\"compares the length of the defined descriptors list with the provided values\n\n        Args:\n            values (Dict): Dictionary with all attribues\n\n        Raises:\n            ValueError: when the number of descriptors does not math the number of provided values\n\n        Returns:\n            Dict: Dict with the attributes\n        \"\"\"\n        if len(values[\"descriptors\"]) != len(values[\"values\"]):\n            raise ValueError(\n                'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}'\n            )\n        return values\n\n    def to_df(self) -&gt; pd.DataFrame:\n\"\"\"tabular overview of the feature as DataFrame\n\n        Returns:\n            pd.DataFrame: tabular overview of the feature as DataFrame\n        \"\"\"\n        return pd.DataFrame(\n            data=[self.values], index=[self.key], columns=self.descriptors\n        )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput.descriptors_to_keys","title":"<code>descriptors_to_keys(descriptors)</code>  <code>classmethod</code>","text":"<p>validates the descriptor names and transforms it to valid keys</p> <p>Parameters:</p> Name Type Description Default <code>descriptors</code> <code>List[str]</code> <p>List of descriptor names</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of valid keys</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@validator(\"descriptors\")\ndef descriptors_to_keys(cls, descriptors):\n\"\"\"validates the descriptor names and transforms it to valid keys\n\n    Args:\n        descriptors (List[str]): List of descriptor names\n\n    Returns:\n        List[str]: List of valid keys\n    \"\"\"\n    return [name2key(name) for name in descriptors]\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput.to_df","title":"<code>to_df(self)</code>","text":"<p>tabular overview of the feature as DataFrame</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>tabular overview of the feature as DataFrame</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n\"\"\"tabular overview of the feature as DataFrame\n\n    Returns:\n        pd.DataFrame: tabular overview of the feature as DataFrame\n    \"\"\"\n    return pd.DataFrame(\n        data=[self.values], index=[self.key], columns=self.descriptors\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput.validate_list_lengths","title":"<code>validate_list_lengths(values)</code>  <code>classmethod</code>","text":"<p>compares the length of the defined descriptors list with the provided values</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dictionary with all attribues</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when the number of descriptors does not math the number of provided values</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with the attributes</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@root_validator(pre=False, skip_on_failure=True)\ndef validate_list_lengths(cls, values):\n\"\"\"compares the length of the defined descriptors list with the provided values\n\n    Args:\n        values (Dict): Dictionary with all attribues\n\n    Raises:\n        ValueError: when the number of descriptors does not math the number of provided values\n\n    Returns:\n        Dict: Dict with the attributes\n    \"\"\"\n    if len(values[\"descriptors\"]) != len(values[\"values\"]):\n        raise ValueError(\n            'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}'\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousInput","title":"<code> ContinuousInput            (NumericalInput)         </code>  <code>pydantic-model</code>","text":"<p>Base class for all continuous input features.</p> <p>Attributes:</p> Name Type Description <code>lower_bound</code> <code>float</code> <p>Lower bound of the feature in the optimization.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound of the feature in the optimization.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class ContinuousInput(NumericalInput):\n\"\"\"Base class for all continuous input features.\n\n    Attributes:\n        lower_bound (float): Lower bound of the feature in the optimization.\n        upper_bound (float): Upper bound of the feature in the optimization.\n    \"\"\"\n\n    type: Literal[\"ContinuousInput\"] = \"ContinuousInput\"\n    lower_bound: float\n    upper_bound: float\n\n    @root_validator(pre=False, skip_on_failure=True)\n    def validate_lower_upper(cls, values):\n\"\"\"Validates that the lower bound is lower than the upper bound\n\n        Args:\n            values (Dict): Dictionary with attributes key, lower and upper bound\n\n        Raises:\n            ValueError: when the lower bound is higher than the upper bound\n\n        Returns:\n            Dict: The attributes as dictionary\n        \"\"\"\n        if values[\"lower_bound\"] &gt; values[\"upper_bound\"]:\n            raise ValueError(\n                f'lower bound must be &lt;= upper bound, got {values[\"lower_bound\"]} &gt; {values[\"upper_bound\"]}'\n            )\n        return values\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Method to validate the suggested candidates\n\n        Args:\n            values (pd.Series): A dataFrame with candidates\n\n        Raises:\n            ValueError: when non numerical values are passed\n            ValueError: when values are larger than the upper bound of the feature\n            ValueError: when values are lower than the lower bound of the feature\n\n        Returns:\n            pd.Series: The passed dataFrame with candidates\n        \"\"\"\n        noise = 10e-6\n        super().validate_candidental(values)\n        if (values &lt; self.lower_bound - noise).any():\n            raise ValueError(\n                f\"not all values of input feature `{self.key}`are larger than lower bound `{self.lower_bound}` \"\n            )\n        if (values &gt; self.upper_bound + noise).any():\n            raise ValueError(\n                f\"not all values of input feature `{self.key}`are smaller than upper bound `{self.upper_bound}` \"\n            )\n        return values\n\n    def sample(self, n: int) -&gt; pd.Series:\n\"\"\"Draw random samples from the feature.\n\n        Args:\n            n (int): number of samples.\n\n        Returns:\n            pd.Series: drawn samples.\n        \"\"\"\n        return pd.Series(\n            name=self.key,\n            data=np.random.uniform(self.lower_bound, self.upper_bound, n),\n        )\n\n    def __str__(self) -&gt; str:\n\"\"\"Method to return a string of lower and upper bound\n\n        Returns:\n            str: String of a list with lower and upper bound\n        \"\"\"\n        return f\"[{self.lower_bound},{self.upper_bound}]\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Method to return a string of lower and upper bound</p> <p>Returns:</p> Type Description <code>str</code> <p>String of a list with lower and upper bound</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def __str__(self) -&gt; str:\n\"\"\"Method to return a string of lower and upper bound\n\n    Returns:\n        str: String of a list with lower and upper bound\n    \"\"\"\n    return f\"[{self.lower_bound},{self.upper_bound}]\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.sample","title":"<code>sample(self, n)</code>","text":"<p>Draw random samples from the feature.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples.</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>drawn samples.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def sample(self, n: int) -&gt; pd.Series:\n\"\"\"Draw random samples from the feature.\n\n    Args:\n        n (int): number of samples.\n\n    Returns:\n        pd.Series: drawn samples.\n    \"\"\"\n    return pd.Series(\n        name=self.key,\n        data=np.random.uniform(self.lower_bound, self.upper_bound, n),\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.validate_candidental","title":"<code>validate_candidental(self, values)</code>","text":"<p>Method to validate the suggested candidates</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>A dataFrame with candidates</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when non numerical values are passed</p> <code>ValueError</code> <p>when values are larger than the upper bound of the feature</p> <code>ValueError</code> <p>when values are lower than the lower bound of the feature</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>The passed dataFrame with candidates</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Method to validate the suggested candidates\n\n    Args:\n        values (pd.Series): A dataFrame with candidates\n\n    Raises:\n        ValueError: when non numerical values are passed\n        ValueError: when values are larger than the upper bound of the feature\n        ValueError: when values are lower than the lower bound of the feature\n\n    Returns:\n        pd.Series: The passed dataFrame with candidates\n    \"\"\"\n    noise = 10e-6\n    super().validate_candidental(values)\n    if (values &lt; self.lower_bound - noise).any():\n        raise ValueError(\n            f\"not all values of input feature `{self.key}`are larger than lower bound `{self.lower_bound}` \"\n        )\n    if (values &gt; self.upper_bound + noise).any():\n        raise ValueError(\n            f\"not all values of input feature `{self.key}`are smaller than upper bound `{self.upper_bound}` \"\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.validate_lower_upper","title":"<code>validate_lower_upper(values)</code>  <code>classmethod</code>","text":"<p>Validates that the lower bound is lower than the upper bound</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dictionary with attributes key, lower and upper bound</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when the lower bound is higher than the upper bound</p> <p>Returns:</p> Type Description <code>Dict</code> <p>The attributes as dictionary</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@root_validator(pre=False, skip_on_failure=True)\ndef validate_lower_upper(cls, values):\n\"\"\"Validates that the lower bound is lower than the upper bound\n\n    Args:\n        values (Dict): Dictionary with attributes key, lower and upper bound\n\n    Raises:\n        ValueError: when the lower bound is higher than the upper bound\n\n    Returns:\n        Dict: The attributes as dictionary\n    \"\"\"\n    if values[\"lower_bound\"] &gt; values[\"upper_bound\"]:\n        raise ValueError(\n            f'lower bound must be &lt;= upper bound, got {values[\"lower_bound\"]} &gt; {values[\"upper_bound\"]}'\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousOutput","title":"<code> ContinuousOutput            (OutputFeature)         </code>  <code>pydantic-model</code>","text":"<p>The base class for a continuous output feature</p> <p>Attributes:</p> Name Type Description <code>objective</code> <code>objective</code> <p>objective of the feature indicating in which direction it should be optimzed. Defaults to <code>MaximizeObjective</code>.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class ContinuousOutput(OutputFeature):\n\"\"\"The base class for a continuous output feature\n\n    Attributes:\n        objective (objective, optional): objective of the feature indicating in which direction it should be optimzed. Defaults to `MaximizeObjective`.\n    \"\"\"\n\n    type: Literal[\"ContinuousOutput\"] = \"ContinuousOutput\"\n    objective: Optional[AnyObjective] = Field(\n        default_factory=lambda: MaximizeObjective(w=1.0)\n    )\n\n    def plot(\n        self,\n        lower: float,\n        upper: float,\n        experiments: Optional[pd.DataFrame] = None,\n        plot_details: bool = True,\n        line_options: Optional[Dict] = None,\n        scatter_options: Optional[Dict] = None,\n        label_options: Optional[Dict] = None,\n        title_options: Optional[Dict] = None,\n    ):\n\"\"\"Plot the assigned objective.\n\n        Args:\n            lower (float): lower bound for the plot\n            upper (float): upper bound for the plot\n            experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None.\n        \"\"\"\n        if self.objective is None:\n            raise ValueError(\n                f\"No objective assigned for ContinuousOutputFeauture with key {self.key}.\"\n            )\n\n        line_options = line_options or {}\n        scatter_options = scatter_options or {}\n        label_options = label_options or {}\n        title_options = title_options or {}\n\n        line_options[\"color\"] = line_options.get(\"color\", \"black\")\n        scatter_options[\"color\"] = scatter_options.get(\"color\", \"red\")\n\n        x = pd.Series(np.linspace(lower, upper, 5000))\n        reward = self.objective.__call__(x)\n        fig, ax = plt.subplots()\n        ax.plot(x, reward, **line_options)\n        # TODO: validate dataframe\n        if experiments is not None:\n            x_data = experiments.loc[experiments[self.key].notna(), self.key].values\n            ax.scatter(\n                x_data,  # type: ignore\n                self.objective.__call__(x_data),  # type: ignore\n                **scatter_options,\n            )\n        ax.set_title(\"Objective %s\" % self.key, **title_options)\n        ax.set_ylabel(\"Objective\", **label_options)\n        ax.set_xlabel(self.key, **label_options)\n        if plot_details:\n            ax = self.objective.plot_details(ax=ax)\n        return fig, ax\n\n    def __str__(self) -&gt; str:\n        return \"ContinuousOutputFeature\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousOutput.__str__","title":"<code>__str__(self)</code>  <code>special</code>","text":"<p>Return str(self).</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def __str__(self) -&gt; str:\n    return \"ContinuousOutputFeature\"\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.ContinuousOutput.plot","title":"<code>plot(self, lower, upper, experiments=None, plot_details=True, line_options=None, scatter_options=None, label_options=None, title_options=None)</code>","text":"<p>Plot the assigned objective.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>float</code> <p>lower bound for the plot</p> required <code>upper</code> <code>float</code> <p>upper bound for the plot</p> required <code>experiments</code> <code>Optional[pd.DataFrame]</code> <p>If provided, scatter also the historical data in the plot. Defaults to None.</p> <code>None</code> Source code in <code>bofire/domain/features.py</code> <pre><code>def plot(\n    self,\n    lower: float,\n    upper: float,\n    experiments: Optional[pd.DataFrame] = None,\n    plot_details: bool = True,\n    line_options: Optional[Dict] = None,\n    scatter_options: Optional[Dict] = None,\n    label_options: Optional[Dict] = None,\n    title_options: Optional[Dict] = None,\n):\n\"\"\"Plot the assigned objective.\n\n    Args:\n        lower (float): lower bound for the plot\n        upper (float): upper bound for the plot\n        experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None.\n    \"\"\"\n    if self.objective is None:\n        raise ValueError(\n            f\"No objective assigned for ContinuousOutputFeauture with key {self.key}.\"\n        )\n\n    line_options = line_options or {}\n    scatter_options = scatter_options or {}\n    label_options = label_options or {}\n    title_options = title_options or {}\n\n    line_options[\"color\"] = line_options.get(\"color\", \"black\")\n    scatter_options[\"color\"] = scatter_options.get(\"color\", \"red\")\n\n    x = pd.Series(np.linspace(lower, upper, 5000))\n    reward = self.objective.__call__(x)\n    fig, ax = plt.subplots()\n    ax.plot(x, reward, **line_options)\n    # TODO: validate dataframe\n    if experiments is not None:\n        x_data = experiments.loc[experiments[self.key].notna(), self.key].values\n        ax.scatter(\n            x_data,  # type: ignore\n            self.objective.__call__(x_data),  # type: ignore\n            **scatter_options,\n        )\n    ax.set_title(\"Objective %s\" % self.key, **title_options)\n    ax.set_ylabel(\"Objective\", **label_options)\n    ax.set_xlabel(self.key, **label_options)\n    if plot_details:\n        ax = self.objective.plot_details(ax=ax)\n    return fig, ax\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.DiscreteInput","title":"<code> DiscreteInput            (NumericalInput)         </code>  <code>pydantic-model</code>","text":"<p>Feature with discretized ordinal values allowed in the optimization.</p> <p>Attributes:</p> Name Type Description <code>key(str)</code> <p>key of the feature.</p> <code>values(List[float])</code> <p>the discretized allowed values during the optimization.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class DiscreteInput(NumericalInput):\n\"\"\"Feature with discretized ordinal values allowed in the optimization.\n\n    Attributes:\n        key(str): key of the feature.\n        values(List[float]): the discretized allowed values during the optimization.\n    \"\"\"\n\n    type: Literal[\"DiscreteInput\"] = \"DiscreteInput\"\n    values: TDiscreteVals\n\n    @validator(\"values\")\n    def validate_values_unique(cls, values):\n\"\"\"Validates that provided values are unique.\n\n        Args:\n            values (List[float]): List of values\n\n        Raises:\n            ValueError: when values are non-unique.\n\n        Returns:\n            List[values]: Sorted list of values\n        \"\"\"\n        if len(values) != len(set(values)):\n            raise ValueError(\"Discrete values must be unique\")\n        return sorted(values)\n\n    @property\n    def lower_bound(self) -&gt; float:\n\"\"\"Lower bound of the set of allowed values\"\"\"\n        return min(self.values)\n\n    @property\n    def upper_bound(self) -&gt; float:\n\"\"\"Upper bound of the set of allowed values\"\"\"\n        return max(self.values)\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Method to validate the provided candidates.\n\n        Args:\n            values (pd.Series): suggested candidates for the feature\n\n        Raises:\n            ValueError: Raises error when one of the provided values is not contained in the list of allowed values.\n\n        Returns:\n            pd.Series: _uggested candidates for the feature\n        \"\"\"\n        super().validate_candidental(values)\n        if not np.isin(values.to_numpy(), np.array(self.values)).all():\n            raise ValueError(\n                f\"Not allowed values in candidates for feature {self.key}.\"\n            )\n        return values\n\n    def sample(self, n: int) -&gt; pd.Series:\n\"\"\"Draw random samples from the feature.\n\n        Args:\n            n (int): number of samples.\n\n        Returns:\n            pd.Series: drawn samples.\n        \"\"\"\n        return pd.Series(name=self.key, data=np.random.choice(self.values, n))\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.lower_bound","title":"<code>lower_bound: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Lower bound of the set of allowed values</p>"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.upper_bound","title":"<code>upper_bound: float</code>  <code>property</code> <code>readonly</code>","text":"<p>Upper bound of the set of allowed values</p>"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.sample","title":"<code>sample(self, n)</code>","text":"<p>Draw random samples from the feature.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples.</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>drawn samples.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def sample(self, n: int) -&gt; pd.Series:\n\"\"\"Draw random samples from the feature.\n\n    Args:\n        n (int): number of samples.\n\n    Returns:\n        pd.Series: drawn samples.\n    \"\"\"\n    return pd.Series(name=self.key, data=np.random.choice(self.values, n))\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.validate_candidental","title":"<code>validate_candidental(self, values)</code>","text":"<p>Method to validate the provided candidates.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>suggested candidates for the feature</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>Raises error when one of the provided values is not contained in the list of allowed values.</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>_uggested candidates for the feature</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Method to validate the provided candidates.\n\n    Args:\n        values (pd.Series): suggested candidates for the feature\n\n    Raises:\n        ValueError: Raises error when one of the provided values is not contained in the list of allowed values.\n\n    Returns:\n        pd.Series: _uggested candidates for the feature\n    \"\"\"\n    super().validate_candidental(values)\n    if not np.isin(values.to_numpy(), np.array(self.values)).all():\n        raise ValueError(\n            f\"Not allowed values in candidates for feature {self.key}.\"\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.validate_values_unique","title":"<code>validate_values_unique(values)</code>  <code>classmethod</code>","text":"<p>Validates that provided values are unique.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[float]</code> <p>List of values</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when values are non-unique.</p> <p>Returns:</p> Type Description <code>List[values]</code> <p>Sorted list of values</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@validator(\"values\")\ndef validate_values_unique(cls, values):\n\"\"\"Validates that provided values are unique.\n\n    Args:\n        values (List[float]): List of values\n\n    Raises:\n        ValueError: when values are non-unique.\n\n    Returns:\n        List[values]: Sorted list of values\n    \"\"\"\n    if len(values) != len(set(values)):\n        raise ValueError(\"Discrete values must be unique\")\n    return sorted(values)\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.Feature","title":"<code> Feature            (KeyModel)         </code>  <code>pydantic-model</code>","text":"<p>The base class for all features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class Feature(KeyModel):\n\"\"\"The base class for all features.\"\"\"\n\n    type: str\n\n    def __lt__(self, other) -&gt; bool:\n\"\"\"\n        Method to compare two models to get them in the desired order.\n        Return True if other is larger than self, else False. (see FEATURE_ORDER)\n\n        Args:\n            other: The other class to compare to self\n\n        Returns:\n            bool: True if the other class is larger than self, else False\n        \"\"\"\n        # TODO: add order of base class to FEATURE_ORDER and remove type: ignore\n        order_self = FEATURE_ORDER[type(self)]  # type: ignore\n        order_other = FEATURE_ORDER[type(other)]\n        if order_self == order_other:\n            return self.key &lt; other.key\n        else:\n            return order_self &lt; order_other\n\n    @staticmethod\n    def from_dict(dict_: dict):\n        return parse_obj_as(AnyFeature, dict_)\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.Feature.__lt__","title":"<code>__lt__(self, other)</code>  <code>special</code>","text":"<p>Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER)</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>The other class to compare to self</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the other class is larger than self, else False</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def __lt__(self, other) -&gt; bool:\n\"\"\"\n    Method to compare two models to get them in the desired order.\n    Return True if other is larger than self, else False. (see FEATURE_ORDER)\n\n    Args:\n        other: The other class to compare to self\n\n    Returns:\n        bool: True if the other class is larger than self, else False\n    \"\"\"\n    # TODO: add order of base class to FEATURE_ORDER and remove type: ignore\n    order_self = FEATURE_ORDER[type(self)]  # type: ignore\n    order_other = FEATURE_ORDER[type(other)]\n    if order_self == order_other:\n        return self.key &lt; other.key\n    else:\n        return order_self &lt; order_other\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.Features","title":"<code> Features            (PydanticBaseModel)         </code>  <code>pydantic-model</code>","text":"<p>Container of features, both input and output features are allowed.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>List(Features</code> <p>list of the features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class Features(PydanticBaseModel):\n\"\"\"Container of features, both input and output features are allowed.\n\n    Attributes:\n        features (List(Features)): list of the features.\n    \"\"\"\n\n    features: FeatureSequence = Field(default_factory=lambda: [])\n\n    def __iter__(self):\n        return iter(self.features)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, i):\n        return self.features[i]\n\n    def __add__(self, other: Union[Sequence[AnyFeature], Features]):\n        if isinstance(other, Features):\n            other_feature_seq = other.features\n        else:\n            other_feature_seq = other\n        new_feature_seq = list(itertools.chain(self.features, other_feature_seq))\n\n        def is_feats_of_type(feats, ftype_collection, ftype_element):\n            return isinstance(feats, ftype_collection) or (\n                not isinstance(feats, Features)\n                and (len(feats) &gt; 0 and isinstance(feats[0], ftype_element))\n            )\n\n        def is_infeats(feats):\n            return is_feats_of_type(feats, InputFeatures, InputFeature)\n\n        def is_outfeats(feats):\n            return is_feats_of_type(feats, OutputFeatures, OutputFeature)\n\n        if is_infeats(self) and is_infeats(other):\n            return InputFeatures(\n                features=cast(Tuple[AnyInputFeature, ...], new_feature_seq)\n            )\n        if is_outfeats(self) and is_outfeats(other):\n            return OutputFeatures(\n                features=cast(Tuple[AnyOutputFeature, ...], new_feature_seq)\n            )\n        return Features(features=new_feature_seq)\n\n    def get_by_key(self, key: str) -&gt; AnyFeature:\n\"\"\"Get a feature by its key.\n\n        Args:\n            key (str): Feature key of the feature of interest\n\n        Returns:\n            Feature: Feature of interest\n        \"\"\"\n        return {f.key: f for f in self.features}[key]\n\n    def get(\n        self,\n        includes: Union[Type, List[Type]] = AnyFeature,\n        excludes: Union[Type, List[Type]] = None,\n        exact: bool = False,\n    ) -&gt; Features:\n\"\"\"get features of the domain\n\n        Args:\n            includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n            excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n            by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None.\n\n        Returns:\n            List[Feature]: List of features in the domain fitting to the passed requirements.\n        \"\"\"\n        return self.__class__(\n            features=sorted(\n                filter_by_class(\n                    self.features,\n                    includes=includes,\n                    excludes=excludes,\n                    exact=exact,\n                )\n            )\n        )\n\n    def get_keys(\n        self,\n        includes: Union[Type, List[Type]] = AnyFeature,\n        excludes: Union[Type, List[Type]] = None,\n        exact: bool = False,\n    ) -&gt; List[str]:\n\"\"\"Method to get feature keys of the domain\n\n        Args:\n            includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n            excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            List[str]: List of feature keys fitting to the passed requirements.\n        \"\"\"\n        return [\n            f.key\n            for f in self.get(\n                includes=includes,\n                excludes=excludes,\n                exact=exact,\n            )\n        ]\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.Features.get","title":"<code>get(self, includes=typing.Union[bofire.domain.features.DiscreteInput, bofire.domain.features.ContinuousInput, bofire.domain.features.ContinuousDescriptorInput, bofire.domain.features.CategoricalInput, bofire.domain.features.CategoricalDescriptorInput, bofire.domain.features.ContinuousOutput], excludes=None, exact=False)</code>","text":"<p>get features of the domain</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be returned. Defaults to Feature.</p> <code>typing.Union[bofire.domain.features.DiscreteInput, bofire.domain.features.ContinuousInput, bofire.domain.features.ContinuousDescriptorInput, bofire.domain.features.CategoricalInput, bofire.domain.features.CategoricalDescriptorInput, bofire.domain.features.ContinuousOutput]</code> <code>excludes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <code>by_attribute</code> <code>str</code> <p>If set it is filtered by the attribute specified in by <code>by_attribute</code>. Defaults to None.</p> required <p>Returns:</p> Type Description <code>List[Feature]</code> <p>List of features in the domain fitting to the passed requirements.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get(\n    self,\n    includes: Union[Type, List[Type]] = AnyFeature,\n    excludes: Union[Type, List[Type]] = None,\n    exact: bool = False,\n) -&gt; Features:\n\"\"\"get features of the domain\n\n    Args:\n        includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n        excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n        by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None.\n\n    Returns:\n        List[Feature]: List of features in the domain fitting to the passed requirements.\n    \"\"\"\n    return self.__class__(\n        features=sorted(\n            filter_by_class(\n                self.features,\n                includes=includes,\n                excludes=excludes,\n                exact=exact,\n            )\n        )\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.Features.get_by_key","title":"<code>get_by_key(self, key)</code>","text":"<p>Get a feature by its key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Feature key of the feature of interest</p> required <p>Returns:</p> Type Description <code>Feature</code> <p>Feature of interest</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_by_key(self, key: str) -&gt; AnyFeature:\n\"\"\"Get a feature by its key.\n\n    Args:\n        key (str): Feature key of the feature of interest\n\n    Returns:\n        Feature: Feature of interest\n    \"\"\"\n    return {f.key: f for f in self.features}[key]\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.Features.get_keys","title":"<code>get_keys(self, includes=typing.Union[bofire.domain.features.DiscreteInput, bofire.domain.features.ContinuousInput, bofire.domain.features.ContinuousDescriptorInput, bofire.domain.features.CategoricalInput, bofire.domain.features.CategoricalDescriptorInput, bofire.domain.features.ContinuousOutput], excludes=None, exact=False)</code>","text":"<p>Method to get feature keys of the domain</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be returned. Defaults to Feature.</p> <code>typing.Union[bofire.domain.features.DiscreteInput, bofire.domain.features.ContinuousInput, bofire.domain.features.ContinuousDescriptorInput, bofire.domain.features.CategoricalInput, bofire.domain.features.CategoricalDescriptorInput, bofire.domain.features.ContinuousOutput]</code> <code>excludes</code> <code>Union[Type, List[Type]]</code> <p>Feature class or list of specific feature classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of feature keys fitting to the passed requirements.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_keys(\n    self,\n    includes: Union[Type, List[Type]] = AnyFeature,\n    excludes: Union[Type, List[Type]] = None,\n    exact: bool = False,\n) -&gt; List[str]:\n\"\"\"Method to get feature keys of the domain\n\n    Args:\n        includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature.\n        excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        List[str]: List of feature keys fitting to the passed requirements.\n    \"\"\"\n    return [\n        f.key\n        for f in self.get(\n            includes=includes,\n            excludes=excludes,\n            exact=exact,\n        )\n    ]\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeature","title":"<code> InputFeature            (Feature)         </code>  <code>pydantic-model</code>","text":"<p>Base class for all input features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class InputFeature(Feature):\n\"\"\"Base class for all input features.\"\"\"\n\n    type: Literal[\"InputFeature\"] = \"InputFeature\"\n\n    @abstractmethod\n    def is_fixed(self) -&gt; bool:\n\"\"\"Indicates if a variable is set to a fixed value.\n\n        Returns:\n            bool: True if fixed, els False.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def fixed_value(\n        self, transform_type: Optional[TTransform] = None\n    ) -&gt; Union[None, List[str], List[float]]:\n\"\"\"Method to return the fixed value in case of a fixed feature.\n\n        Returns:\n            Union[None,str,float]: None in case the feature is not fixed, else the fixed value.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_experimental(\n        self, values: pd.Series, strict: bool = False\n    ) -&gt; pd.Series:\n\"\"\"Abstract method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.\n\n        Returns:\n            pd.Series: The passed dataFrame with experiments\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Abstract method to validate the suggested candidates\n\n        Args:\n            values (pd.Series): A dataFrame with candidates\n\n        Returns:\n            pd.Series: The passed dataFrame with candidates\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def sample(self, n: int) -&gt; pd.Series:\n\"\"\"Sample a series of allowed values.\n\n        Args:\n            n (int): Number of samples\n\n        Returns:\n            pd.Series: Sampled values.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_bounds(\n        self,\n        transform_type: Optional[TTransform] = None,\n        values: Optional[pd.Series] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n\"\"\"Returns the bounds of an input feature depending on the requested transform type.\n\n        Args:\n            transform_type (Optional[TTransform], optional): The requested transform type. Defaults to None.\n            values (Optional[pd.Series], optional): If values are provided the bounds are returned taking\n                the most extreme values for the feature into account. Defaults to None.\n\n        Returns:\n            Tuple[List[float], List[float]]: List of lower bound values, list of upper bound values.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeature.fixed_value","title":"<code>fixed_value(self, transform_type=None)</code>","text":"<p>Method to return the fixed value in case of a fixed feature.</p> <p>Returns:</p> Type Description <code>Union[None,str,float]</code> <p>None in case the feature is not fixed, else the fixed value.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@abstractmethod\ndef fixed_value(\n    self, transform_type: Optional[TTransform] = None\n) -&gt; Union[None, List[str], List[float]]:\n\"\"\"Method to return the fixed value in case of a fixed feature.\n\n    Returns:\n        Union[None,str,float]: None in case the feature is not fixed, else the fixed value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeature.get_bounds","title":"<code>get_bounds(self, transform_type=None, values=None)</code>","text":"<p>Returns the bounds of an input feature depending on the requested transform type.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>Optional[TTransform]</code> <p>The requested transform type. Defaults to None.</p> <code>None</code> <code>values</code> <code>Optional[pd.Series]</code> <p>If values are provided the bounds are returned taking the most extreme values for the feature into account. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>List of lower bound values, list of upper bound values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@abstractmethod\ndef get_bounds(\n    self,\n    transform_type: Optional[TTransform] = None,\n    values: Optional[pd.Series] = None,\n) -&gt; Tuple[List[float], List[float]]:\n\"\"\"Returns the bounds of an input feature depending on the requested transform type.\n\n    Args:\n        transform_type (Optional[TTransform], optional): The requested transform type. Defaults to None.\n        values (Optional[pd.Series], optional): If values are provided the bounds are returned taking\n            the most extreme values for the feature into account. Defaults to None.\n\n    Returns:\n        Tuple[List[float], List[float]]: List of lower bound values, list of upper bound values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeature.is_fixed","title":"<code>is_fixed(self)</code>","text":"<p>Indicates if a variable is set to a fixed value.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if fixed, els False.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@abstractmethod\ndef is_fixed(self) -&gt; bool:\n\"\"\"Indicates if a variable is set to a fixed value.\n\n    Returns:\n        bool: True if fixed, els False.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeature.sample","title":"<code>sample(self, n)</code>","text":"<p>Sample a series of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>Sampled values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@abstractmethod\ndef sample(self, n: int) -&gt; pd.Series:\n\"\"\"Sample a series of allowed values.\n\n    Args:\n        n (int): Number of samples\n\n    Returns:\n        pd.Series: Sampled values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeature.validate_candidental","title":"<code>validate_candidental(self, values)</code>","text":"<p>Abstract method to validate the suggested candidates</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>A dataFrame with candidates</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>The passed dataFrame with candidates</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@abstractmethod\ndef validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Abstract method to validate the suggested candidates\n\n    Args:\n        values (pd.Series): A dataFrame with candidates\n\n    Returns:\n        pd.Series: The passed dataFrame with candidates\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeature.validate_experimental","title":"<code>validate_experimental(self, values, strict=False)</code>","text":"<p>Abstract method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>pd.Series</code> <p>The passed dataFrame with experiments</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@abstractmethod\ndef validate_experimental(\n    self, values: pd.Series, strict: bool = False\n) -&gt; pd.Series:\n\"\"\"Abstract method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.\n\n    Returns:\n        pd.Series: The passed dataFrame with experiments\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures","title":"<code> InputFeatures            (Features)         </code>  <code>pydantic-model</code>","text":"<p>Container of input features, only input features are allowed.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>List(InputFeatures</code> <p>list of the features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class InputFeatures(Features):\n\"\"\"Container of input features, only input features are allowed.\n\n    Attributes:\n        features (List(InputFeatures)): list of the features.\n    \"\"\"\n\n    features: Sequence[AnyInputFeature] = Field(default_factory=lambda: [])\n\n    def get_fixed(self) -&gt; \"InputFeatures\":\n\"\"\"Gets all features in `self` that are fixed and returns them as new `InputFeatures` object.\n\n        Returns:\n            InputFeatures: Input features object containing only fixed features.\n        \"\"\"\n        return InputFeatures(features=[feat for feat in self if feat.is_fixed()])  # type: ignore\n\n    def get_free(self) -&gt; \"InputFeatures\":\n\"\"\"Gets all features in `self` that are not fixed and returns them as new `InputFeatures` object.\n\n        Returns:\n            InputFeatures: Input features object containing only non-fixed features.\n        \"\"\"\n        return InputFeatures(features=[feat for feat in self if not feat.is_fixed()])  # type: ignore\n\n    @validate_arguments\n    def sample(\n        self,\n        n: Tnum_samples = 1,\n        method: SamplingMethodEnum = SamplingMethodEnum.UNIFORM,\n    ) -&gt; pd.DataFrame:\n\"\"\"Draw sobol samples\n\n        Args:\n            n (int, optional): Number of samples, has to be larger than 0. Defaults to 1.\n            method (SamplingMethodEnum, optional): Method to use, implemented methods are `UNIFORM`, `SOBOL` and `LHS`.\n                Defaults to `UNIFORM`.\n\n        Returns:\n            pd.DataFrame: Dataframe containing the samples.\n        \"\"\"\n        if method == SamplingMethodEnum.UNIFORM:\n            return self.validate_inputs(\n                pd.concat([feat.sample(n) for feat in self.get(InputFeature)], axis=1)  # type: ignore\n            )\n        free_features = self.get_free()\n        if method == SamplingMethodEnum.SOBOL:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                X = Sobol(len(free_features)).random(n)\n        else:\n            X = LatinHypercube(len(free_features)).random(n)\n        res = []\n        for i, feat in enumerate(free_features):\n            if isinstance(feat, ContinuousInput):\n                x = feat.from_unit_range(X[:, i])\n            elif isinstance(feat, (DiscreteInput, CategoricalInput)):\n                if isinstance(feat, DiscreteInput):\n                    levels = feat.values\n                else:\n                    levels = feat.get_allowed_categories()\n                bins = np.linspace(0, 1, len(levels) + 1)\n                idx = np.digitize(X[:, i], bins) - 1\n                x = np.array(levels)[idx]\n            else:\n                raise (ValueError(f\"Unknown input feature with key {feat.key}\"))\n            res.append(pd.Series(x, name=feat.key))\n        samples = pd.concat(res, axis=1)\n        for feat in self.get_fixed():\n            samples[feat.key] = feat.fixed_value()[0]  # type: ignore\n        return self.validate_inputs(samples)[self.get_keys(InputFeature)]\n\n    # validate candidates, TODO rename and tidy up\n    def validate_inputs(self, inputs: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Validate a pandas dataframe with input feature values.\n\n        Args:\n            inputs (pd.Dataframe): Inputs to validate.\n\n        Raises:\n            ValueError: Raises a Valueerror if a feature based validation raises an exception.\n\n        Returns:\n            pd.Dataframe: Validated dataframe\n        \"\"\"\n        for feature in self:\n            if feature.key not in inputs:\n                raise ValueError(f\"no col for input feature `{feature.key}`\")\n            feature.validate_candidental(inputs[feature.key])  # type: ignore\n        return inputs\n\n    def validate_experiments(\n        self, experiments: pd.DataFrame, strict=False\n    ) -&gt; pd.DataFrame:\n        for feature in self:\n            if feature.key not in experiments:\n                raise ValueError(f\"no col for input feature `{feature.key}`\")\n            feature.validate_experimental(experiments[feature.key], strict=strict)  # type: ignore\n        return experiments\n\n    def get_categorical_combinations(\n        self,\n        include: Type[Feature] = InputFeature,\n        exclude: Optional[Type[InputFeature]] = None,\n    ):\n\"\"\"get a list of tuples pairing the feature keys with a list of valid categories\n\n        Args:\n            include (Feature, optional): Features to be included. Defaults to InputFeature.\n            exclude (Feature, optional): Features to be excluded, e.g. subclasses of the included features. Defaults to None.\n\n        Returns:\n            List[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str)\n        \"\"\"\n        features = [\n            f\n            for f in self.get(includes=include, excludes=exclude)\n            if isinstance(f, CategoricalInput) and not f.is_fixed()\n        ]\n        list_of_lists = [\n            [(f.key, cat) for cat in f.get_allowed_categories()] for f in features\n        ]\n        return list(itertools.product(*list_of_lists))\n\n    # transformation related methods\n    def _get_transform_info(\n        self, specs: TInputTransformSpecs\n    ) -&gt; Tuple[Dict[str, Tuple[int]], Dict[str, Tuple[str]]]:\n\"\"\"Generates two dictionaries. The first one specifies which key is mapped to\n        which column indices when applying `transform`. The second one specifies\n        which key is mapped to which transformed keys.\n\n        Args:\n            specs (TInputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n\n        Returns:\n            Dict[str, Tuple[int]]: Dictionary mapping feature keys to column indices.\n            Dict[str, Tuple[str]]: Dictionary mapping feature keys to transformed feature\n                keys.\n        \"\"\"\n        self._validate_transform_specs(specs)\n        features2idx = {}\n        features2names = {}\n        counter = 0\n        for _, feat in enumerate(self.get()):\n            if feat.key not in specs.keys():\n                features2idx[feat.key] = (counter,)\n                features2names[feat.key] = (feat.key,)\n                counter += 1\n            elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n                assert isinstance(feat, CategoricalInput)\n                features2idx[feat.key] = tuple(\n                    (np.array(range(len(feat.categories))) + counter).tolist()\n                )\n                features2names[feat.key] = tuple(\n                    [f\"{feat.key}{_CAT_SEP}{c}\" for c in feat.categories]\n                )\n                counter += len(feat.categories)\n            elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n                features2idx[feat.key] = (counter,)\n                features2names[feat.key] = (feat.key,)\n                counter += 1\n            elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n                assert isinstance(feat, CategoricalInput)\n                features2idx[feat.key] = tuple(\n                    (np.array(range(len(feat.categories) - 1)) + counter).tolist()\n                )\n                features2names[feat.key] = tuple(\n                    [f\"{feat.key}{_CAT_SEP}{c}\" for c in feat.categories[1:]]\n                )\n                counter += len(feat.categories) - 1\n            elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n                assert isinstance(feat, CategoricalDescriptorInput)\n                features2idx[feat.key] = tuple(\n                    (np.array(range(len(feat.descriptors))) + counter).tolist()\n                )\n                features2names[feat.key] = tuple(\n                    [f\"{feat.key}{_CAT_SEP}{d}\" for d in feat.descriptors]\n                )\n                counter += len(feat.descriptors)\n        return features2idx, features2names\n\n    def transform(\n        self, experiments: pd.DataFrame, specs: TInputTransformSpecs\n    ) -&gt; pd.DataFrame:\n\"\"\"Transform a dataframe to the represenation specified in `specs`.\n\n        Currently only input categoricals are supported.\n\n        Args:\n            experiments (pd.DataFrame): Data dataframe to be transformed.\n            specs (TInputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n\n        Returns:\n            pd.DataFrame: Transformed dataframe. Only input features are included.\n        \"\"\"\n        specs = self._validate_transform_specs(specs)\n        transformed = []\n        for feat in self.get():\n            s = experiments[feat.key]\n            if feat.key not in specs.keys():\n                transformed.append(s)\n            elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.to_onehot_encoding(s))\n            elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.to_ordinal_encoding(s))\n            elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.to_dummy_encoding(s))\n            elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n                assert isinstance(feat, CategoricalDescriptorInput)\n                transformed.append(feat.to_descriptor_encoding(s))\n        return pd.concat(transformed, axis=1)\n\n    def inverse_transform(\n        self, experiments: pd.DataFrame, specs: TInputTransformSpecs\n    ) -&gt; pd.DataFrame:\n\"\"\"Transform a dataframe back to the original representations.\n\n        The original applied transformation has to be provided via the specs dictionary.\n        Currently only input categoricals are supported.\n\n        Args:\n            experiments (pd.DataFrame): Transformed data dataframe.\n            specs (TInputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n\n        Returns:\n            pd.DataFrame: Back transformed dataframe. Only input features are included.\n        \"\"\"\n        self._validate_transform_specs(specs=specs)\n        transformed = []\n        for feat in self.get():\n            if feat.key not in specs.keys():\n                transformed.append(experiments[feat.key])\n            elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.from_onehot_encoding(experiments))\n            elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.from_ordinal_encoding(experiments[feat.key]))\n            elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.from_dummy_encoding(experiments))\n            elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n                assert isinstance(feat, CategoricalDescriptorInput)\n                transformed.append(feat.from_descriptor_encoding(experiments))\n        return pd.concat(transformed, axis=1)\n\n    def _validate_transform_specs(self, specs: TInputTransformSpecs):\n\"\"\"Checks the validity of the transform specs .\n\n        Args:\n            specs (TInputTransformSpecs): Transform specs to be validated.\n        \"\"\"\n        # first check that the keys in the specs dict are correct also correct feature keys\n        if len(set(specs.keys()) - set(self.get_keys(CategoricalInput))) &gt; 0:\n            raise ValueError(\"Unknown features specified in transform specs.\")\n        # next check that all values are of type CategoricalEncodingEnum\n        if not (\n            all([isinstance(enc, CategoricalEncodingEnum) for enc in specs.values()])\n        ):\n            raise ValueError(\"Unknown transform specified.\")\n        # next check that only Categoricalwithdescriptor have the value DESCRIPTOR\n        descriptor_keys = [\n            key\n            for key, value in specs.items()\n            if value == CategoricalEncodingEnum.DESCRIPTOR\n        ]\n        if (\n            len(set(descriptor_keys) - set(self.get_keys(CategoricalDescriptorInput)))\n            &gt; 0\n        ):\n            raise ValueError(\"Wrong features types assigned to DESCRIPTOR transform.\")\n        return specs\n\n    def get_bounds(\n        self,\n        specs: TInputTransformSpecs,\n        experiments: Optional[pd.DataFrame] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n\"\"\"Returns the boundaries of the optimization problem based on the transformations\n        defined in the  `specs` dictionary.\n\n        Args:\n            specs (TInputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n            experiments (Optional[pd.DataFrame], optional): Dataframe with input features.\n                If provided the real feature bounds are returned based on both the opt.\n                feature bounds and the extreme points in the dataframe. Defaults to None,\n\n        Raises:\n            ValueError: If a feature type is not known.\n            ValueError: If no transformation is provided for a categorical feature.\n\n        Returns:\n            Tuple[List[float], List[float]]: list with lower bounds, list with upper bounds.\n        \"\"\"\n        self._validate_transform_specs(specs=specs)\n\n        lower = []\n        upper = []\n\n        for feat in self.get():\n            l, u = feat.get_bounds(  # type: ignore\n                transform_type=specs.get(feat.key),  # type: ignore\n                values=experiments[feat.key] if experiments is not None else None,\n            )\n            lower += l\n            upper += u\n        return lower, upper\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.get_bounds","title":"<code>get_bounds(self, specs, experiments=None)</code>","text":"<p>Returns the boundaries of the optimization problem based on the transformations defined in the  <code>specs</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>TInputTransformSpecs</code> <p>Dictionary specifying which input feature is transformed by which encoder.</p> required <code>experiments</code> <code>Optional[pd.DataFrame]</code> <p>Dataframe with input features. If provided the real feature bounds are returned based on both the opt. feature bounds and the extreme points in the dataframe. Defaults to None,</p> <code>None</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If a feature type is not known.</p> <code>ValueError</code> <p>If no transformation is provided for a categorical feature.</p> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>list with lower bounds, list with upper bounds.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_bounds(\n    self,\n    specs: TInputTransformSpecs,\n    experiments: Optional[pd.DataFrame] = None,\n) -&gt; Tuple[List[float], List[float]]:\n\"\"\"Returns the boundaries of the optimization problem based on the transformations\n    defined in the  `specs` dictionary.\n\n    Args:\n        specs (TInputTransformSpecs): Dictionary specifying which\n            input feature is transformed by which encoder.\n        experiments (Optional[pd.DataFrame], optional): Dataframe with input features.\n            If provided the real feature bounds are returned based on both the opt.\n            feature bounds and the extreme points in the dataframe. Defaults to None,\n\n    Raises:\n        ValueError: If a feature type is not known.\n        ValueError: If no transformation is provided for a categorical feature.\n\n    Returns:\n        Tuple[List[float], List[float]]: list with lower bounds, list with upper bounds.\n    \"\"\"\n    self._validate_transform_specs(specs=specs)\n\n    lower = []\n    upper = []\n\n    for feat in self.get():\n        l, u = feat.get_bounds(  # type: ignore\n            transform_type=specs.get(feat.key),  # type: ignore\n            values=experiments[feat.key] if experiments is not None else None,\n        )\n        lower += l\n        upper += u\n    return lower, upper\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.get_categorical_combinations","title":"<code>get_categorical_combinations(self, include=&lt;class 'bofire.domain.features.InputFeature'&gt;, exclude=None)</code>","text":"<p>get a list of tuples pairing the feature keys with a list of valid categories</p> <p>Parameters:</p> Name Type Description Default <code>include</code> <code>Feature</code> <p>Features to be included. Defaults to InputFeature.</p> <code>&lt;class 'bofire.domain.features.InputFeature'&gt;</code> <code>exclude</code> <code>Feature</code> <p>Features to be excluded, e.g. subclasses of the included features. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[(str, List[str])]</code> <p>Returns a list of tuples pairing the feature keys with a list of valid categories (str)</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_categorical_combinations(\n    self,\n    include: Type[Feature] = InputFeature,\n    exclude: Optional[Type[InputFeature]] = None,\n):\n\"\"\"get a list of tuples pairing the feature keys with a list of valid categories\n\n    Args:\n        include (Feature, optional): Features to be included. Defaults to InputFeature.\n        exclude (Feature, optional): Features to be excluded, e.g. subclasses of the included features. Defaults to None.\n\n    Returns:\n        List[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str)\n    \"\"\"\n    features = [\n        f\n        for f in self.get(includes=include, excludes=exclude)\n        if isinstance(f, CategoricalInput) and not f.is_fixed()\n    ]\n    list_of_lists = [\n        [(f.key, cat) for cat in f.get_allowed_categories()] for f in features\n    ]\n    return list(itertools.product(*list_of_lists))\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.get_fixed","title":"<code>get_fixed(self)</code>","text":"<p>Gets all features in <code>self</code> that are fixed and returns them as new <code>InputFeatures</code> object.</p> <p>Returns:</p> Type Description <code>InputFeatures</code> <p>Input features object containing only fixed features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_fixed(self) -&gt; \"InputFeatures\":\n\"\"\"Gets all features in `self` that are fixed and returns them as new `InputFeatures` object.\n\n    Returns:\n        InputFeatures: Input features object containing only fixed features.\n    \"\"\"\n    return InputFeatures(features=[feat for feat in self if feat.is_fixed()])  # type: ignore\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.get_free","title":"<code>get_free(self)</code>","text":"<p>Gets all features in <code>self</code> that are not fixed and returns them as new <code>InputFeatures</code> object.</p> <p>Returns:</p> Type Description <code>InputFeatures</code> <p>Input features object containing only non-fixed features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_free(self) -&gt; \"InputFeatures\":\n\"\"\"Gets all features in `self` that are not fixed and returns them as new `InputFeatures` object.\n\n    Returns:\n        InputFeatures: Input features object containing only non-fixed features.\n    \"\"\"\n    return InputFeatures(features=[feat for feat in self if not feat.is_fixed()])  # type: ignore\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.inverse_transform","title":"<code>inverse_transform(self, experiments, specs)</code>","text":"<p>Transform a dataframe back to the original representations.</p> <p>The original applied transformation has to be provided via the specs dictionary. Currently only input categoricals are supported.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Transformed data dataframe.</p> required <code>specs</code> <code>TInputTransformSpecs</code> <p>Dictionary specifying which input feature is transformed by which encoder.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Back transformed dataframe. Only input features are included.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def inverse_transform(\n    self, experiments: pd.DataFrame, specs: TInputTransformSpecs\n) -&gt; pd.DataFrame:\n\"\"\"Transform a dataframe back to the original representations.\n\n    The original applied transformation has to be provided via the specs dictionary.\n    Currently only input categoricals are supported.\n\n    Args:\n        experiments (pd.DataFrame): Transformed data dataframe.\n        specs (TInputTransformSpecs): Dictionary specifying which\n            input feature is transformed by which encoder.\n\n    Returns:\n        pd.DataFrame: Back transformed dataframe. Only input features are included.\n    \"\"\"\n    self._validate_transform_specs(specs=specs)\n    transformed = []\n    for feat in self.get():\n        if feat.key not in specs.keys():\n            transformed.append(experiments[feat.key])\n        elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.from_onehot_encoding(experiments))\n        elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.from_ordinal_encoding(experiments[feat.key]))\n        elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.from_dummy_encoding(experiments))\n        elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n            assert isinstance(feat, CategoricalDescriptorInput)\n            transformed.append(feat.from_descriptor_encoding(experiments))\n    return pd.concat(transformed, axis=1)\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.sample","title":"<code>sample(self, n=1, method=&lt;SamplingMethodEnum.UNIFORM: 'UNIFORM'&gt;)</code>","text":"<p>Draw sobol samples</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples, has to be larger than 0. Defaults to 1.</p> <code>1</code> <code>method</code> <code>SamplingMethodEnum</code> <p>Method to use, implemented methods are <code>UNIFORM</code>, <code>SOBOL</code> and <code>LHS</code>. Defaults to <code>UNIFORM</code>.</p> <code>&lt;SamplingMethodEnum.UNIFORM: 'UNIFORM'&gt;</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dataframe containing the samples.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>@validate_arguments\ndef sample(\n    self,\n    n: Tnum_samples = 1,\n    method: SamplingMethodEnum = SamplingMethodEnum.UNIFORM,\n) -&gt; pd.DataFrame:\n\"\"\"Draw sobol samples\n\n    Args:\n        n (int, optional): Number of samples, has to be larger than 0. Defaults to 1.\n        method (SamplingMethodEnum, optional): Method to use, implemented methods are `UNIFORM`, `SOBOL` and `LHS`.\n            Defaults to `UNIFORM`.\n\n    Returns:\n        pd.DataFrame: Dataframe containing the samples.\n    \"\"\"\n    if method == SamplingMethodEnum.UNIFORM:\n        return self.validate_inputs(\n            pd.concat([feat.sample(n) for feat in self.get(InputFeature)], axis=1)  # type: ignore\n        )\n    free_features = self.get_free()\n    if method == SamplingMethodEnum.SOBOL:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            X = Sobol(len(free_features)).random(n)\n    else:\n        X = LatinHypercube(len(free_features)).random(n)\n    res = []\n    for i, feat in enumerate(free_features):\n        if isinstance(feat, ContinuousInput):\n            x = feat.from_unit_range(X[:, i])\n        elif isinstance(feat, (DiscreteInput, CategoricalInput)):\n            if isinstance(feat, DiscreteInput):\n                levels = feat.values\n            else:\n                levels = feat.get_allowed_categories()\n            bins = np.linspace(0, 1, len(levels) + 1)\n            idx = np.digitize(X[:, i], bins) - 1\n            x = np.array(levels)[idx]\n        else:\n            raise (ValueError(f\"Unknown input feature with key {feat.key}\"))\n        res.append(pd.Series(x, name=feat.key))\n    samples = pd.concat(res, axis=1)\n    for feat in self.get_fixed():\n        samples[feat.key] = feat.fixed_value()[0]  # type: ignore\n    return self.validate_inputs(samples)[self.get_keys(InputFeature)]\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.transform","title":"<code>transform(self, experiments, specs)</code>","text":"<p>Transform a dataframe to the represenation specified in <code>specs</code>.</p> <p>Currently only input categoricals are supported.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Data dataframe to be transformed.</p> required <code>specs</code> <code>TInputTransformSpecs</code> <p>Dictionary specifying which input feature is transformed by which encoder.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Transformed dataframe. Only input features are included.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def transform(\n    self, experiments: pd.DataFrame, specs: TInputTransformSpecs\n) -&gt; pd.DataFrame:\n\"\"\"Transform a dataframe to the represenation specified in `specs`.\n\n    Currently only input categoricals are supported.\n\n    Args:\n        experiments (pd.DataFrame): Data dataframe to be transformed.\n        specs (TInputTransformSpecs): Dictionary specifying which\n            input feature is transformed by which encoder.\n\n    Returns:\n        pd.DataFrame: Transformed dataframe. Only input features are included.\n    \"\"\"\n    specs = self._validate_transform_specs(specs)\n    transformed = []\n    for feat in self.get():\n        s = experiments[feat.key]\n        if feat.key not in specs.keys():\n            transformed.append(s)\n        elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.to_onehot_encoding(s))\n        elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.to_ordinal_encoding(s))\n        elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.to_dummy_encoding(s))\n        elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n            assert isinstance(feat, CategoricalDescriptorInput)\n            transformed.append(feat.to_descriptor_encoding(s))\n    return pd.concat(transformed, axis=1)\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.InputFeatures.validate_inputs","title":"<code>validate_inputs(self, inputs)</code>","text":"<p>Validate a pandas dataframe with input feature values.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>pd.Dataframe</code> <p>Inputs to validate.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>Raises a Valueerror if a feature based validation raises an exception.</p> <p>Returns:</p> Type Description <code>pd.Dataframe</code> <p>Validated dataframe</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_inputs(self, inputs: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Validate a pandas dataframe with input feature values.\n\n    Args:\n        inputs (pd.Dataframe): Inputs to validate.\n\n    Raises:\n        ValueError: Raises a Valueerror if a feature based validation raises an exception.\n\n    Returns:\n        pd.Dataframe: Validated dataframe\n    \"\"\"\n    for feature in self:\n        if feature.key not in inputs:\n            raise ValueError(f\"no col for input feature `{feature.key}`\")\n        feature.validate_candidental(inputs[feature.key])  # type: ignore\n    return inputs\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput","title":"<code> NumericalInput            (InputFeature)         </code>  <code>pydantic-model</code>","text":"<p>Abstract base class for all numerical (ordinal) input features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class NumericalInput(InputFeature):\n\"\"\"Abstract base class for all numerical (ordinal) input features.\"\"\"\n\n    type: Literal[\"NumericalInput\"] = \"NumericalInput\"\n\n    @staticmethod\n    def from_dict(dict_: dict):\n        return parse_obj_as(AnyInputFeature, dict_)\n\n    def to_unit_range(\n        self, values: Union[pd.Series, np.ndarray], use_real_bounds: bool = False\n    ) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"Convert to the unit range between 0 and 1.\n\n        Args:\n            values (pd.Series): values to be transformed\n            use_real_bounds (bool, optional): if True, use the bounds from the actual values else the bounds from the feature.\n                Defaults to False.\n\n        Raises:\n            ValueError: If lower_bound == upper bound an error is raised\n\n        Returns:\n            pd.Series: transformed values.\n        \"\"\"\n        if use_real_bounds:\n            lower, upper = self.get_bounds(transform_type=None, values=values)\n            lower = lower[0]\n            upper = upper[0]\n        else:\n            lower, upper = self.lower_bound, self.upper_bound  # type: ignore\n        if lower == upper:\n            raise ValueError(\"Fixed feature cannot be transformed to unit range.\")\n        valrange = upper - lower\n        return (values - lower) / valrange\n\n    def from_unit_range(\n        self, values: Union[pd.Series, np.ndarray]\n    ) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"Convert from unit range.\n\n        Args:\n            values (pd.Series): values to transform from.\n\n        Raises:\n            ValueError: if the feature is fixed raise a value error.\n\n        Returns:\n            pd.Series: _description_\n        \"\"\"\n        if self.is_fixed():\n            raise ValueError(\"Fixed feature cannot be transformed from unit range.\")\n        valrange = self.upper_bound - self.lower_bound  # type: ignore\n        return (values * valrange) + self.lower_bound  # type: ignore\n\n    def is_fixed(self):\n\"\"\"Method to check if the feature is fixed\n\n        Returns:\n            Boolean: True when the feature is fixed, false otherwise.\n        \"\"\"\n        # TODO: the bounds are declared in the derived classes, hence the type checks fail here :(.\n        return self.lower_bound == self.upper_bound  # type: ignore\n\n    def fixed_value(\n        self, transform_type: Optional[TTransform] = None\n    ) -&gt; Union[None, List[float]]:\n\"\"\"Method to get the value to which the feature is fixed\n\n        Returns:\n            Float: Return the feature value or None if the feature is not fixed.\n        \"\"\"\n        assert transform_type is None\n        if self.is_fixed():\n            return [self.lower_bound]  # type: ignore\n        else:\n            return None\n\n    def validate_experimental(self, values: pd.Series, strict=False) -&gt; pd.Series:\n\"\"\"Method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not.\n                Defaults to False.\n\n        Raises:\n            ValueError: when a value is not numerical\n            ValueError: when there is no variation in a feature provided by the experimental data\n\n        Returns:\n            pd.Series: A dataFrame with experiments\n        \"\"\"\n        if not is_numeric(values):\n            raise ValueError(\n                f\"not all values of input feature `{self.key}` are numerical\"\n            )\n        if strict:\n            lower, upper = self.get_bounds(transform_type=None, values=values)\n            if lower == upper:\n                raise ValueError(\n                    f\"No variation present or planned for feature {self.key}. Remove it.\"\n                )\n        return values\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Validate the suggested candidates for the feature.\n\n        Args:\n            values (pd.Series): suggested candidates for the feature\n\n        Raises:\n            ValueError: Error is raised when one of the values is not numerical.\n\n        Returns:\n            pd.Series: the original provided candidates\n        \"\"\"\n        if not is_numeric(values):\n            raise ValueError(\n                f\"not all values of input feature `{self.key}` are numerical\"\n            )\n        return values\n\n    def get_bounds(\n        self,\n        transform_type: Optional[TTransform] = None,\n        values: Optional[pd.Series] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        assert transform_type is None\n        if values is None:\n            return [self.lower_bound], [self.upper_bound]  # type: ignore\n        lower = min(self.lower_bound, values.min())  # type: ignore\n        upper = max(self.upper_bound, values.max())  # type: ignore\n        return [lower], [upper]  # type: ignore\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput.fixed_value","title":"<code>fixed_value(self, transform_type=None)</code>","text":"<p>Method to get the value to which the feature is fixed</p> <p>Returns:</p> Type Description <code>Float</code> <p>Return the feature value or None if the feature is not fixed.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def fixed_value(\n    self, transform_type: Optional[TTransform] = None\n) -&gt; Union[None, List[float]]:\n\"\"\"Method to get the value to which the feature is fixed\n\n    Returns:\n        Float: Return the feature value or None if the feature is not fixed.\n    \"\"\"\n    assert transform_type is None\n    if self.is_fixed():\n        return [self.lower_bound]  # type: ignore\n    else:\n        return None\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput.from_unit_range","title":"<code>from_unit_range(self, values)</code>","text":"<p>Convert from unit range.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>values to transform from.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if the feature is fixed raise a value error.</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>description</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def from_unit_range(\n    self, values: Union[pd.Series, np.ndarray]\n) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"Convert from unit range.\n\n    Args:\n        values (pd.Series): values to transform from.\n\n    Raises:\n        ValueError: if the feature is fixed raise a value error.\n\n    Returns:\n        pd.Series: _description_\n    \"\"\"\n    if self.is_fixed():\n        raise ValueError(\"Fixed feature cannot be transformed from unit range.\")\n    valrange = self.upper_bound - self.lower_bound  # type: ignore\n    return (values * valrange) + self.lower_bound  # type: ignore\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput.get_bounds","title":"<code>get_bounds(self, transform_type=None, values=None)</code>","text":"<p>Returns the bounds of an input feature depending on the requested transform type.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>Optional[TTransform]</code> <p>The requested transform type. Defaults to None.</p> <code>None</code> <code>values</code> <code>Optional[pd.Series]</code> <p>If values are provided the bounds are returned taking the most extreme values for the feature into account. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>List of lower bound values, list of upper bound values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_bounds(\n    self,\n    transform_type: Optional[TTransform] = None,\n    values: Optional[pd.Series] = None,\n) -&gt; Tuple[List[float], List[float]]:\n    assert transform_type is None\n    if values is None:\n        return [self.lower_bound], [self.upper_bound]  # type: ignore\n    lower = min(self.lower_bound, values.min())  # type: ignore\n    upper = max(self.upper_bound, values.max())  # type: ignore\n    return [lower], [upper]  # type: ignore\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput.is_fixed","title":"<code>is_fixed(self)</code>","text":"<p>Method to check if the feature is fixed</p> <p>Returns:</p> Type Description <code>Boolean</code> <p>True when the feature is fixed, false otherwise.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def is_fixed(self):\n\"\"\"Method to check if the feature is fixed\n\n    Returns:\n        Boolean: True when the feature is fixed, false otherwise.\n    \"\"\"\n    # TODO: the bounds are declared in the derived classes, hence the type checks fail here :(.\n    return self.lower_bound == self.upper_bound  # type: ignore\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput.to_unit_range","title":"<code>to_unit_range(self, values, use_real_bounds=False)</code>","text":"<p>Convert to the unit range between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>values to be transformed</p> required <code>use_real_bounds</code> <code>bool</code> <p>if True, use the bounds from the actual values else the bounds from the feature. Defaults to False.</p> <code>False</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If lower_bound == upper bound an error is raised</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>transformed values.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def to_unit_range(\n    self, values: Union[pd.Series, np.ndarray], use_real_bounds: bool = False\n) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"Convert to the unit range between 0 and 1.\n\n    Args:\n        values (pd.Series): values to be transformed\n        use_real_bounds (bool, optional): if True, use the bounds from the actual values else the bounds from the feature.\n            Defaults to False.\n\n    Raises:\n        ValueError: If lower_bound == upper bound an error is raised\n\n    Returns:\n        pd.Series: transformed values.\n    \"\"\"\n    if use_real_bounds:\n        lower, upper = self.get_bounds(transform_type=None, values=values)\n        lower = lower[0]\n        upper = upper[0]\n    else:\n        lower, upper = self.lower_bound, self.upper_bound  # type: ignore\n    if lower == upper:\n        raise ValueError(\"Fixed feature cannot be transformed to unit range.\")\n    valrange = upper - lower\n    return (values - lower) / valrange\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput.validate_candidental","title":"<code>validate_candidental(self, values)</code>","text":"<p>Validate the suggested candidates for the feature.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>suggested candidates for the feature</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>Error is raised when one of the values is not numerical.</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>the original provided candidates</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n\"\"\"Validate the suggested candidates for the feature.\n\n    Args:\n        values (pd.Series): suggested candidates for the feature\n\n    Raises:\n        ValueError: Error is raised when one of the values is not numerical.\n\n    Returns:\n        pd.Series: the original provided candidates\n    \"\"\"\n    if not is_numeric(values):\n        raise ValueError(\n            f\"not all values of input feature `{self.key}` are numerical\"\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.NumericalInput.validate_experimental","title":"<code>validate_experimental(self, values, strict=False)</code>","text":"<p>Method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>pd.Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when a value is not numerical</p> <code>ValueError</code> <p>when there is no variation in a feature provided by the experimental data</p> <p>Returns:</p> Type Description <code>pd.Series</code> <p>A dataFrame with experiments</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def validate_experimental(self, values: pd.Series, strict=False) -&gt; pd.Series:\n\"\"\"Method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not.\n            Defaults to False.\n\n    Raises:\n        ValueError: when a value is not numerical\n        ValueError: when there is no variation in a feature provided by the experimental data\n\n    Returns:\n        pd.Series: A dataFrame with experiments\n    \"\"\"\n    if not is_numeric(values):\n        raise ValueError(\n            f\"not all values of input feature `{self.key}` are numerical\"\n        )\n    if strict:\n        lower, upper = self.get_bounds(transform_type=None, values=values)\n        if lower == upper:\n            raise ValueError(\n                f\"No variation present or planned for feature {self.key}. Remove it.\"\n            )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeature","title":"<code> OutputFeature            (Feature)         </code>  <code>pydantic-model</code>","text":"<p>Base class for all output features.</p> <p>Attributes:</p> Name Type Description <code>key(str)</code> <p>Key of the Feature.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class OutputFeature(Feature):\n\"\"\"Base class for all output features.\n\n    Attributes:\n        key(str): Key of the Feature.\n    \"\"\"\n\n    type: Literal[\"OutputFeature\"] = \"OutputFeature\"\n    objective: Optional[AnyObjective]\n\n    @staticmethod\n    def from_dict(dict_: dict):\n        return parse_obj_as(AnyOutputFeature, dict_)\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeatures","title":"<code> OutputFeatures            (Features)         </code>  <code>pydantic-model</code>","text":"<p>Container of output features, only output features are allowed.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>List(OutputFeatures</code> <p>list of the features.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>class OutputFeatures(Features):\n\"\"\"Container of output features, only output features are allowed.\n\n    Attributes:\n        features (List(OutputFeatures)): list of the features.\n    \"\"\"\n\n    features: Sequence[AnyOutputFeature] = Field(default_factory=lambda: [])\n\n    def get_by_objective(\n        self,\n        includes: Union[\n            List[Type[AnyAbstractObjective]],\n            Type[AnyAbstractObjective],\n            Type[Objective],\n        ] = Objective,\n        excludes: Union[\n            List[Type[AnyAbstractObjective]],\n            Type[AnyAbstractObjective],\n            None,\n        ] = None,\n        exact: bool = False,\n    ) -&gt; \"OutputFeatures\":\n\"\"\"Get output features filtered by the type of the attached objective.\n\n        Args:\n            includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n                to be returned. Defaults to Objective.\n            excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            List[AnyOutputFeature]: List of output features fitting to the passed requirements.\n        \"\"\"\n        if len(self.features) == 0:\n            return OutputFeatures(features=[])\n        else:\n            # TODO: why only continuous output?\n            return OutputFeatures(\n                features=sorted(\n                    filter_by_attribute(\n                        self.get(ContinuousOutput).features,\n                        lambda of: of.objective,\n                        includes,\n                        excludes,\n                        exact,\n                    )\n                )\n            )\n\n    def get_keys_by_objective(\n        self,\n        includes: Union[\n            List[Type[AnyAbstractObjective]],\n            Type[AnyAbstractObjective],\n            Type[Objective],\n        ] = Objective,\n        excludes: Union[\n            List[Type[AnyAbstractObjective]], Type[AnyAbstractObjective], None\n        ] = None,\n        exact: bool = False,\n    ) -&gt; List[str]:\n\"\"\"Get keys of output features filtered by the type of the attached objective.\n\n        Args:\n            includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n                to be returned. Defaults to Objective.\n            excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            List[str]: List of output feature keys fitting to the passed requirements.\n        \"\"\"\n        return [f.key for f in self.get_by_objective(includes, excludes, exact)]\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Evaluate the objective for every\n\n        Args:\n            experiments (pd.DataFrame): Experiments for which the objectives should be evaluated.\n\n        Returns:\n            pd.DataFrame: Objective values for the experiments of interest.\n        \"\"\"\n        return pd.concat(\n            [\n                feat.objective(experiments[[feat.key]])  # type: ignore\n                for feat in self.features\n                if feat.objective is not None\n            ],\n            axis=1,\n        )\n\n    def preprocess_experiments_one_valid_output(\n        self,\n        output_feature_key: str,\n        experiments: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n\"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n            output_feature_key (str): The feature based on which non-valid entries rows are removed\n\n        Returns:\n            pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included\n        \"\"\"\n        clean_exp = experiments.loc[\n            (experiments[\"valid_%s\" % output_feature_key] == 1)\n            &amp; (experiments[output_feature_key].notna())\n        ]\n\n        return clean_exp\n\n    def preprocess_experiments_all_valid_outputs(\n        self,\n        experiments: pd.DataFrame,\n        output_feature_keys: Optional[List] = None,\n    ) -&gt; pd.DataFrame:\n\"\"\"Method to get a dataframe where non-valid entries of all output feature are removed\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n            output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None.\n\n        Returns:\n            pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included\n        \"\"\"\n        if (output_feature_keys is None) or (len(output_feature_keys) == 0):\n            output_feature_keys = self.get_keys(OutputFeature)\n\n        clean_exp = experiments.query(\n            \" &amp; \".join([\"(`valid_%s` &gt; 0)\" % key for key in output_feature_keys])\n        )\n        clean_exp = clean_exp.dropna(subset=output_feature_keys)\n\n        return clean_exp\n\n    def preprocess_experiments_any_valid_output(\n        self, experiments: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n\"\"\"Method to get a dataframe where at least one output feature has a valid entry\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n\n        Returns:\n            pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry\n        \"\"\"\n\n        output_feature_keys = self.get_keys(OutputFeature)\n\n        # clean_exp = experiments.query(\" or \".join([\"(valid_%s &gt; 0)\" % key for key in output_feature_keys]))\n        # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys]))\n\n        assert experiments is not None\n        clean_exp = experiments.query(\n            \" or \".join(\n                [\n                    \"((`valid_%s` &gt;0) &amp; `%s`.notna())\" % (key, key)\n                    for key in output_feature_keys\n                ]\n            )\n        )\n\n        return clean_exp\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.__call__","title":"<code>__call__(self, experiments)</code>  <code>special</code>","text":"<p>Evaluate the objective for every</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Experiments for which the objectives should be evaluated.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Objective values for the experiments of interest.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Evaluate the objective for every\n\n    Args:\n        experiments (pd.DataFrame): Experiments for which the objectives should be evaluated.\n\n    Returns:\n        pd.DataFrame: Objective values for the experiments of interest.\n    \"\"\"\n    return pd.concat(\n        [\n            feat.objective(experiments[[feat.key]])  # type: ignore\n            for feat in self.features\n            if feat.objective is not None\n        ],\n        axis=1,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.get_by_objective","title":"<code>get_by_objective(self, includes=&lt;class 'bofire.domain.objectives.Objective'&gt;, excludes=None, exact=False)</code>","text":"<p>Get output features filtered by the type of the attached objective.</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[List[TObjective], TObjective]</code> <p>Objective class or list of objective classes to be returned. Defaults to Objective.</p> <code>&lt;class 'bofire.domain.objectives.Objective'&gt;</code> <code>excludes</code> <code>Union[List[TObjective], TObjective, None]</code> <p>Objective class or list of specific objective classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[AnyOutputFeature]</code> <p>List of output features fitting to the passed requirements.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_by_objective(\n    self,\n    includes: Union[\n        List[Type[AnyAbstractObjective]],\n        Type[AnyAbstractObjective],\n        Type[Objective],\n    ] = Objective,\n    excludes: Union[\n        List[Type[AnyAbstractObjective]],\n        Type[AnyAbstractObjective],\n        None,\n    ] = None,\n    exact: bool = False,\n) -&gt; \"OutputFeatures\":\n\"\"\"Get output features filtered by the type of the attached objective.\n\n    Args:\n        includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n            to be returned. Defaults to Objective.\n        excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        List[AnyOutputFeature]: List of output features fitting to the passed requirements.\n    \"\"\"\n    if len(self.features) == 0:\n        return OutputFeatures(features=[])\n    else:\n        # TODO: why only continuous output?\n        return OutputFeatures(\n            features=sorted(\n                filter_by_attribute(\n                    self.get(ContinuousOutput).features,\n                    lambda of: of.objective,\n                    includes,\n                    excludes,\n                    exact,\n                )\n            )\n        )\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.get_keys_by_objective","title":"<code>get_keys_by_objective(self, includes=&lt;class 'bofire.domain.objectives.Objective'&gt;, excludes=None, exact=False)</code>","text":"<p>Get keys of output features filtered by the type of the attached objective.</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[List[TObjective], TObjective]</code> <p>Objective class or list of objective classes to be returned. Defaults to Objective.</p> <code>&lt;class 'bofire.domain.objectives.Objective'&gt;</code> <code>excludes</code> <code>Union[List[TObjective], TObjective, None]</code> <p>Objective class or list of specific objective classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of output feature keys fitting to the passed requirements.</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def get_keys_by_objective(\n    self,\n    includes: Union[\n        List[Type[AnyAbstractObjective]],\n        Type[AnyAbstractObjective],\n        Type[Objective],\n    ] = Objective,\n    excludes: Union[\n        List[Type[AnyAbstractObjective]], Type[AnyAbstractObjective], None\n    ] = None,\n    exact: bool = False,\n) -&gt; List[str]:\n\"\"\"Get keys of output features filtered by the type of the attached objective.\n\n    Args:\n        includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n            to be returned. Defaults to Objective.\n        excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        List[str]: List of output feature keys fitting to the passed requirements.\n    \"\"\"\n    return [f.key for f in self.get_by_objective(includes, excludes, exact)]\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.preprocess_experiments_all_valid_outputs","title":"<code>preprocess_experiments_all_valid_outputs(self, experiments, output_feature_keys=None)</code>","text":"<p>Method to get a dataframe where non-valid entries of all output feature are removed</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe with experimental data</p> required <code>output_feature_keys</code> <code>Optional[List]</code> <p>List of output feature keys which should be considered for removal of invalid values. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dataframe with all experiments where only valid entries of the selected features are included</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def preprocess_experiments_all_valid_outputs(\n    self,\n    experiments: pd.DataFrame,\n    output_feature_keys: Optional[List] = None,\n) -&gt; pd.DataFrame:\n\"\"\"Method to get a dataframe where non-valid entries of all output feature are removed\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n        output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None.\n\n    Returns:\n        pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included\n    \"\"\"\n    if (output_feature_keys is None) or (len(output_feature_keys) == 0):\n        output_feature_keys = self.get_keys(OutputFeature)\n\n    clean_exp = experiments.query(\n        \" &amp; \".join([\"(`valid_%s` &gt; 0)\" % key for key in output_feature_keys])\n    )\n    clean_exp = clean_exp.dropna(subset=output_feature_keys)\n\n    return clean_exp\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.preprocess_experiments_any_valid_output","title":"<code>preprocess_experiments_any_valid_output(self, experiments)</code>","text":"<p>Method to get a dataframe where at least one output feature has a valid entry</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe with experimental data</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dataframe with all experiments where at least one output feature has a valid entry</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def preprocess_experiments_any_valid_output(\n    self, experiments: pd.DataFrame\n) -&gt; pd.DataFrame:\n\"\"\"Method to get a dataframe where at least one output feature has a valid entry\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n\n    Returns:\n        pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry\n    \"\"\"\n\n    output_feature_keys = self.get_keys(OutputFeature)\n\n    # clean_exp = experiments.query(\" or \".join([\"(valid_%s &gt; 0)\" % key for key in output_feature_keys]))\n    # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys]))\n\n    assert experiments is not None\n    clean_exp = experiments.query(\n        \" or \".join(\n            [\n                \"((`valid_%s` &gt;0) &amp; `%s`.notna())\" % (key, key)\n                for key in output_feature_keys\n            ]\n        )\n    )\n\n    return clean_exp\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.preprocess_experiments_one_valid_output","title":"<code>preprocess_experiments_one_valid_output(self, output_feature_key, experiments)</code>","text":"<p>Method to get a dataframe where non-valid entries of the provided output feature are removed</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>pd.DataFrame</code> <p>Dataframe with experimental data</p> required <code>output_feature_key</code> <code>str</code> <p>The feature based on which non-valid entries rows are removed</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Dataframe with all experiments where only valid entries of the specific feature are included</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def preprocess_experiments_one_valid_output(\n    self,\n    output_feature_key: str,\n    experiments: pd.DataFrame,\n) -&gt; pd.DataFrame:\n\"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n        output_feature_key (str): The feature based on which non-valid entries rows are removed\n\n    Returns:\n        pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included\n    \"\"\"\n    clean_exp = experiments.loc[\n        (experiments[\"valid_%s\" % output_feature_key] == 1)\n        &amp; (experiments[output_feature_key].notna())\n    ]\n\n    return clean_exp\n</code></pre>"},{"location":"ref-features/#bofire.domain.features.is_continuous","title":"<code>is_continuous(var)</code>","text":"<p>Checks if Feature is continous</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>Feature</code> <p>Feature to be checked</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if continuous, else False</p> Source code in <code>bofire/domain/features.py</code> <pre><code>def is_continuous(var: Feature) -&gt; bool:\n\"\"\"Checks if Feature is continous\n\n    Args:\n        var (Feature): Feature to be checked\n\n    Returns:\n        bool: True if continuous, else False\n    \"\"\"\n    # TODO: generalize query via attribute continuousFeature (not existing yet!)\n    if isinstance(var, ContinuousInput) or isinstance(var, ContinuousOutput):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"ref-mappers/","title":"Opti Mappers","text":""},{"location":"ref-objectives/","title":"Domain","text":""},{"location":"ref-objectives/#bofire.domain.objectives.AbstractTargetObjective","title":"<code> AbstractTargetObjective            (Objective)         </code>  <code>pydantic-model</code>","text":"Source code in <code>bofire/domain/objectives.py</code> <pre><code>class AbstractTargetObjective(Objective):\n    # TODO: add docstring to AbstractTargetObjective\n\n    type: Literal[\"AbstractTargetObjective\"] = \"AbstractTargetObjective\"\n    w: TWeight = 1\n    target_value: float\n    tolerance: TGe0\n\n    def plot_details(self, ax):\n\"\"\"Plot function highlighting the tolerance area of the objective\n\n        Args:\n            ax (matplotlib.axes.Axes): Matplotlib axes object\n\n        Returns:\n            matplotlib.axes.Axes: The object to be plotted\n        \"\"\"\n        ax.axvline(self.target_value, color=\"black\")\n        ax.axvspan(\n            self.target_value - self.tolerance,\n            self.target_value + self.tolerance,\n            color=\"gray\",\n            alpha=0.5,\n        )\n        return ax\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.AbstractTargetObjective.plot_details","title":"<code>plot_details(self, ax)</code>","text":"<p>Plot function highlighting the tolerance area of the objective</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>matplotlib.axes.Axes</code> <p>Matplotlib axes object</p> required <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The object to be plotted</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def plot_details(self, ax):\n\"\"\"Plot function highlighting the tolerance area of the objective\n\n    Args:\n        ax (matplotlib.axes.Axes): Matplotlib axes object\n\n    Returns:\n        matplotlib.axes.Axes: The object to be plotted\n    \"\"\"\n    ax.axvline(self.target_value, color=\"black\")\n    ax.axvspan(\n        self.target_value - self.tolerance,\n        self.target_value + self.tolerance,\n        color=\"gray\",\n        alpha=0.5,\n    )\n    return ax\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.BotorchConstrainedObjective","title":"<code> BotorchConstrainedObjective        </code>","text":"<p>This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class BotorchConstrainedObjective:\n\"\"\"This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints.\"\"\"\n\n    @abstractmethod\n    def to_constraints(\n        self, idx: int\n    ) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n        Args:\n            idx (int): Index of the constraint objective in the list of outputs.\n\n        Returns:\n            Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n                list of the corresponding botorch eta values.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.BotorchConstrainedObjective.to_constraints","title":"<code>to_constraints(self, idx)</code>","text":"<p>Create a callable that can be used by <code>botorch.utils.objective.apply_constraints</code> to setup ouput constrained optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the constraint objective in the list of outputs.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Callable[[Tensor], Tensor]], List[float]]</code> <p>List of callables that can be used by botorch for setting up the constrained objective, and     list of the corresponding botorch eta values.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>@abstractmethod\ndef to_constraints(\n    self, idx: int\n) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n    Args:\n        idx (int): Index of the constraint objective in the list of outputs.\n\n    Returns:\n        Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n            list of the corresponding botorch eta values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.ConstantObjective","title":"<code> ConstantObjective            (Objective)         </code>  <code>pydantic-model</code>","text":"<p>Constant objective to allow constrained output features which should not be optimized</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>value</code> <code>float</code> <p>constant return value</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class ConstantObjective(Objective):\n\"\"\"Constant objective to allow constrained output features which should not be optimized\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        value (float): constant return value\n    \"\"\"\n\n    type: Literal[\"ConstantObjective\"] = \"ConstantObjective\"\n    w: TWeight = 1\n    value: float\n\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning the fixed value as reward\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            np.ndarray: An array of passed constants with the shape of the passed x values array.\n        \"\"\"\n        return np.ones(x.shape) * self.value\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.ConstantObjective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>The call function returning the fixed value as reward</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An array of passed constants with the shape of the passed x values array.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning the fixed value as reward\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        np.ndarray: An array of passed constants with the shape of the passed x values array.\n    \"\"\"\n    return np.ones(x.shape) * self.value\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.DeltaObjective","title":"<code> DeltaObjective            (IdentityObjective)         </code>  <code>pydantic-model</code>","text":"<p>Class returning the difference between a reference value and identity as reward</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective</p> <code>ref_point</code> <code>float</code> <p>Reference value.</p> <code>scale</code> <code>float</code> <p>Scaling factor for the difference. Defaults to one.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class DeltaObjective(IdentityObjective):\n\"\"\"Class returning the difference between a reference value and identity as reward\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective\n        ref_point (float): Reference value.\n        scale (float, optional): Scaling factor for the difference. Defaults to one.\n    \"\"\"\n\n    type: Literal[\"DeltaObjective\"] = \"DeltaObjective\"\n    ref_point: float\n    scale: float = 1\n\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            np.ndarray: The difference between reference and the x value as reward, might be scaled with a passed scaling value\n        \"\"\"\n        return (self.ref_point - x) * self.scale\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.DeltaObjective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>The call function returning a reward for passed x values</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The difference between reference and the x value as reward, might be scaled with a passed scaling value</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        np.ndarray: The difference between reference and the x value as reward, might be scaled with a passed scaling value\n    \"\"\"\n    return (self.ref_point - x) * self.scale\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.IdentityObjective","title":"<code> IdentityObjective            (Objective)         </code>  <code>pydantic-model</code>","text":"<p>An objective returning the identity as reward. The return can be scaled, when a lower and upper bound are provided.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective</p> <code>lower_bound</code> <code>float</code> <p>Lower bound for normalizing the objective between zero and one. Defaults to zero.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound for normalizing the objective between zero and one. Defaults to one.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class IdentityObjective(Objective):\n\"\"\"An objective returning the identity as reward.\n    The return can be scaled, when a lower and upper bound are provided.\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective\n        lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero.\n        upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one.\n    \"\"\"\n\n    type: Literal[\"IdentityObjective\"] = \"IdentityObjective\"\n    w: TWeight = 1\n    lower_bound: float = 0\n    upper_bound: float = 1\n\n    @root_validator(pre=False, skip_on_failure=True)\n    def validate_lower_upper(cls, values):\n\"\"\"Validation function to ensure that lower bound is always greater the upper bound\n\n        Args:\n            values (Dict): The attributes of the class\n\n        Raises:\n            ValueError: when a lower bound higher than the upper bound is passed\n\n        Returns:\n            Dict: The attributes of the class\n        \"\"\"\n        if values[\"lower_bound\"] &gt; values[\"upper_bound\"]:\n            raise ValueError(\n                f'lower bound must be &lt;= upper bound, got {values[\"lower_bound\"]} &gt; {values[\"upper_bound\"]}'\n            )\n        return values\n\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds\n        \"\"\"\n        return (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.IdentityObjective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>The call function returning a reward for passed x values</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The identity as reward, might be normalized to the passed lower and upper bounds</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds\n    \"\"\"\n    return (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.IdentityObjective.validate_lower_upper","title":"<code>validate_lower_upper(values)</code>  <code>classmethod</code>","text":"<p>Validation function to ensure that lower bound is always greater the upper bound</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>The attributes of the class</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>when a lower bound higher than the upper bound is passed</p> <p>Returns:</p> Type Description <code>Dict</code> <p>The attributes of the class</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>@root_validator(pre=False, skip_on_failure=True)\ndef validate_lower_upper(cls, values):\n\"\"\"Validation function to ensure that lower bound is always greater the upper bound\n\n    Args:\n        values (Dict): The attributes of the class\n\n    Raises:\n        ValueError: when a lower bound higher than the upper bound is passed\n\n    Returns:\n        Dict: The attributes of the class\n    \"\"\"\n    if values[\"lower_bound\"] &gt; values[\"upper_bound\"]:\n        raise ValueError(\n            f'lower bound must be &lt;= upper bound, got {values[\"lower_bound\"]} &gt; {values[\"upper_bound\"]}'\n        )\n    return values\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeObjective","title":"<code> MaximizeObjective            (IdentityObjective)         </code>  <code>pydantic-model</code>","text":"<p>Child class from the identity function without modifications, since the parent class is already defined as maximization</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective</p> <code>lower_bound</code> <code>float</code> <p>Lower bound for normalizing the objective between zero and one. Defaults to zero.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound for normalizing the objective between zero and one. Defaults to one.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class MaximizeObjective(IdentityObjective):\n\"\"\"Child class from the identity function without modifications, since the parent class is already defined as maximization\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective\n        lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero.\n        upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one.\n    \"\"\"\n\n    type: Literal[\"MaximizeObjective\"] = \"MaximizeObjective\"\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeSigmoidObjective","title":"<code> MaximizeSigmoidObjective            (SigmoidObjective)         </code>  <code>pydantic-model</code>","text":"<p>Class for a maximizing sigmoid objective</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> <code>tp</code> <code>float</code> <p>Turning point of the sigmoid function.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class MaximizeSigmoidObjective(SigmoidObjective):\n\"\"\"Class for a maximizing sigmoid objective\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n        tp (float): Turning point of the sigmoid function.\n\n    \"\"\"\n\n    type: Literal[\"MaximizeSigmoidObjective\"] = \"MaximizeSigmoidObjective\"\n\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n        \"\"\"\n        return 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n\n    def to_constraints(\n        self, idx: int\n    ) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n        Args:\n            idx (int): Index of the constraint objective in the list of outputs.\n\n        Returns:\n            Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n                list of the corresponding botorch eta values.\n        \"\"\"\n        return [lambda Z: (Z[..., idx] - self.tp) * -1.0], [1.0 / self.steepness]\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeSigmoidObjective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>The call function returning a sigmoid shaped reward for passed x values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n    \"\"\"\n    return 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeSigmoidObjective.to_constraints","title":"<code>to_constraints(self, idx)</code>","text":"<p>Create a callable that can be used by <code>botorch.utils.objective.apply_constraints</code> to setup ouput constrained optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the constraint objective in the list of outputs.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Callable[[Tensor], Tensor]], List[float]]</code> <p>List of callables that can be used by botorch for setting up the constrained objective, and     list of the corresponding botorch eta values.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def to_constraints(\n    self, idx: int\n) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n    Args:\n        idx (int): Index of the constraint objective in the list of outputs.\n\n    Returns:\n        Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n            list of the corresponding botorch eta values.\n    \"\"\"\n    return [lambda Z: (Z[..., idx] - self.tp) * -1.0], [1.0 / self.steepness]\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeObjective","title":"<code> MinimizeObjective            (IdentityObjective)         </code>  <code>pydantic-model</code>","text":"<p>Class returning the negative identity as reward.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective</p> <code>lower_bound</code> <code>float</code> <p>Lower bound for normalizing the objective between zero and one. Defaults to zero.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound for normalizing the objective between zero and one. Defaults to one.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class MinimizeObjective(IdentityObjective):\n\"\"\"Class returning the negative identity as reward.\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective\n        lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero.\n        upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one.\n    \"\"\"\n\n    type: Literal[\"MinimizeObjective\"] = \"MinimizeObjective\"\n\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds\n        \"\"\"\n        return -1.0 * (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeObjective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>The call function returning a reward for passed x values</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The negative identity as reward, might be normalized to the passed lower and upper bounds</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds\n    \"\"\"\n    return -1.0 * (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeSigmoidObjective","title":"<code> MinimizeSigmoidObjective            (SigmoidObjective)         </code>  <code>pydantic-model</code>","text":"<p>Class for a minimizing a sigmoid objective</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> <code>tp</code> <code>float</code> <p>Turning point of the sigmoid function.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class MinimizeSigmoidObjective(SigmoidObjective):\n\"\"\"Class for a minimizing a sigmoid objective\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n        tp (float): Turning point of the sigmoid function.\n    \"\"\"\n\n    type: Literal[\"MinimizeSigmoidObjective\"] = \"MinimizeSigmoidObjective\"\n\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n        \"\"\"\n        return 1 - 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n\n    def to_constraints(\n        self, idx: int\n    ) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n        Args:\n            idx (int): Index of the constraint objective in the list of outputs.\n\n        Returns:\n            Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n                list of the corresponding botorch eta values.\n        \"\"\"\n        return [lambda Z: (Z[..., idx] - self.tp)], [1.0 / self.steepness]\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeSigmoidObjective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>The call function returning a sigmoid shaped reward for passed x values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n    \"\"\"\n    return 1 - 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeSigmoidObjective.to_constraints","title":"<code>to_constraints(self, idx)</code>","text":"<p>Create a callable that can be used by <code>botorch.utils.objective.apply_constraints</code> to setup ouput constrained optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the constraint objective in the list of outputs.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Callable[[Tensor], Tensor]], List[float]]</code> <p>List of callables that can be used by botorch for setting up the constrained objective, and     list of the corresponding botorch eta values.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def to_constraints(\n    self, idx: int\n) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n    Args:\n        idx (int): Index of the constraint objective in the list of outputs.\n\n    Returns:\n        Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n            list of the corresponding botorch eta values.\n    \"\"\"\n    return [lambda Z: (Z[..., idx] - self.tp)], [1.0 / self.steepness]\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.Objective","title":"<code> Objective            (PydanticBaseModel)         </code>  <code>pydantic-model</code>","text":"<p>The base class for all objectives</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class Objective(PydanticBaseModel):\n\"\"\"The base class for all objectives\"\"\"\n\n    type: str\n\n    @abstractmethod\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"Abstract method to define the call function for the class Objective\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            np.ndarray: The desirability of the passed x values\n        \"\"\"\n        pass\n\n    def plot_details(self, ax):\n\"\"\"\n        Args:\n            ax (matplotlib.axes.Axes): Matplotlib axes object\n\n        Returns:\n            matplotlib.axes.Axes: The object to be plotted\n        \"\"\"\n        return ax\n\n    @staticmethod\n    def from_dict(dict_: dict):\n        return parse_obj_as(AnyObjective, dict_)\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.Objective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>Abstract method to define the call function for the class Objective</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The desirability of the passed x values</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>@abstractmethod\ndef __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"Abstract method to define the call function for the class Objective\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        np.ndarray: The desirability of the passed x values\n    \"\"\"\n    pass\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.Objective.plot_details","title":"<code>plot_details(self, ax)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ax</code> <code>matplotlib.axes.Axes</code> <p>Matplotlib axes object</p> required <p>Returns:</p> Type Description <code>matplotlib.axes.Axes</code> <p>The object to be plotted</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def plot_details(self, ax):\n\"\"\"\n    Args:\n        ax (matplotlib.axes.Axes): Matplotlib axes object\n\n    Returns:\n        matplotlib.axes.Axes: The object to be plotted\n    \"\"\"\n    return ax\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.SigmoidObjective","title":"<code> SigmoidObjective            (Objective, BotorchConstrainedObjective)         </code>  <code>pydantic-model</code>","text":"<p>Base class for all sigmoid shaped objectives</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> <code>tp</code> <code>float</code> <p>Turning point of the sigmoid function.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class SigmoidObjective(Objective, BotorchConstrainedObjective):\n\"\"\"Base class for all sigmoid shaped objectives\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n        tp (float): Turning point of the sigmoid function.\n    \"\"\"\n\n    type: Literal[\"SigmoidObjective\"] = \"SigmoidObjective\"\n    steepness: TGt0\n    tp: float\n    w: TWeight = 1\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.TargetObjective","title":"<code> TargetObjective            (AbstractTargetObjective, BotorchConstrainedObjective)         </code>  <code>pydantic-model</code>","text":"<p>Class for objectives for optimizing towards a target value</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>target_value</code> <code>float</code> <p>target value that should be reached.</p> <code>tolerance</code> <code>float</code> <p>Tolerance for reaching the target. Has to be greater than zero.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>class TargetObjective(AbstractTargetObjective, BotorchConstrainedObjective):\n\"\"\"Class for objectives for optimizing towards a target value\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        target_value (float): target value that should be reached.\n        tolerance (float): Tolerance for reaching the target. Has to be greater than zero.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n\n    \"\"\"\n\n    type: Literal[\"TargetObjective\"] = \"TargetObjective\"\n    steepness: TGt0\n\n    def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values.\n\n        Args:\n            x (np.array): An array of x values\n\n        Returns:\n            np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value.\n        \"\"\"\n        return (\n            1\n            / (\n                1\n                + np.exp(\n                    -1 * self.steepness * (x - (self.target_value - self.tolerance))\n                )\n            )\n            * (\n                1\n                - 1\n                / (\n                    1.0\n                    + np.exp(\n                        -1 * self.steepness * (x - (self.target_value + self.tolerance))\n                    )\n                )\n            )\n        )\n\n    def to_constraints(\n        self, idx: int\n    ) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n        Args:\n            idx (int): Index of the constraint objective in the list of outputs.\n\n        Returns:\n            Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n                list of the corresponding botorch eta values.\n        \"\"\"\n        return [\n            lambda Z: (Z[..., idx] - (self.target_value - self.tolerance)) * -1.0,\n            lambda Z: (Z[..., idx] - (self.target_value + self.tolerance)),\n        ], [1.0 / self.steepness, 1.0 / self.steepness]\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.TargetObjective.__call__","title":"<code>__call__(self, x)</code>  <code>special</code>","text":"<p>The call function returning a reward for passed x values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.array</code> <p>An array of x values</p> required <p>Returns:</p> Type Description <code>np.array</code> <p>An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def __call__(self, x: Union[pd.Series, np.ndarray]) -&gt; Union[pd.Series, np.ndarray]:\n\"\"\"The call function returning a reward for passed x values.\n\n    Args:\n        x (np.array): An array of x values\n\n    Returns:\n        np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value.\n    \"\"\"\n    return (\n        1\n        / (\n            1\n            + np.exp(\n                -1 * self.steepness * (x - (self.target_value - self.tolerance))\n            )\n        )\n        * (\n            1\n            - 1\n            / (\n                1.0\n                + np.exp(\n                    -1 * self.steepness * (x - (self.target_value + self.tolerance))\n                )\n            )\n        )\n    )\n</code></pre>"},{"location":"ref-objectives/#bofire.domain.objectives.TargetObjective.to_constraints","title":"<code>to_constraints(self, idx)</code>","text":"<p>Create a callable that can be used by <code>botorch.utils.objective.apply_constraints</code> to setup ouput constrained optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the constraint objective in the list of outputs.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Callable[[Tensor], Tensor]], List[float]]</code> <p>List of callables that can be used by botorch for setting up the constrained objective, and     list of the corresponding botorch eta values.</p> Source code in <code>bofire/domain/objectives.py</code> <pre><code>def to_constraints(\n    self, idx: int\n) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations.\n\n    Args:\n        idx (int): Index of the constraint objective in the list of outputs.\n\n    Returns:\n        Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of callables that can be used by botorch for setting up the constrained objective, and\n            list of the corresponding botorch eta values.\n    \"\"\"\n    return [\n        lambda Z: (Z[..., idx] - (self.target_value - self.tolerance)) * -1.0,\n        lambda Z: (Z[..., idx] - (self.target_value + self.tolerance)),\n    ], [1.0 / self.steepness, 1.0 / self.steepness]\n</code></pre>"},{"location":"ref-utils/","title":"Utils","text":""},{"location":"ref-utils/#bofire.utils.enum","title":"<code>enum</code>","text":""},{"location":"ref-utils/#bofire.utils.enum.AcquisitionFunctionEnum","title":"<code> AcquisitionFunctionEnum            (Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>bofire/utils/enum.py</code> <pre><code>class AcquisitionFunctionEnum(Enum):\n    QNEI = \"QNEI\"\n    QUCB = \"QUCB\"\n    QEI = \"QEI\"\n    QPI = \"QPI\"\n    QSR = \"QSR\"\n</code></pre>"},{"location":"ref-utils/#bofire.utils.enum.CategoricalEncodingEnum","title":"<code> CategoricalEncodingEnum            (Enum)         </code>","text":"<p>Enumeration class of implemented categorical encodings Currently, one-hot and ordinal encoding are implemented.</p> Source code in <code>bofire/utils/enum.py</code> <pre><code>class CategoricalEncodingEnum(Enum):\n\"\"\"Enumeration class of implemented categorical encodings\n    Currently, one-hot and ordinal encoding are implemented.\n    \"\"\"\n\n    ONE_HOT = \"ONE_HOT\"\n    ORDINAL = \"ORDINAL\"\n    DUMMY = \"DUMMY\"\n    DESCRIPTOR = \"DESCRIPTOR\"  # only possible for categorical with descriptors\n</code></pre>"},{"location":"ref-utils/#bofire.utils.enum.CategoricalMethodEnum","title":"<code> CategoricalMethodEnum            (Enum)         </code>","text":"<p>Enumeration class of supported methods how to handle categorical features Currently, exhaustive search and free relaxation are implemented.</p> Source code in <code>bofire/utils/enum.py</code> <pre><code>class CategoricalMethodEnum(Enum):\n\"\"\"Enumeration class of supported methods how to handle categorical features\n    Currently, exhaustive search and free relaxation are implemented.\n    \"\"\"\n\n    EXHAUSTIVE = \"EXHAUSTIVE\"\n    FREE = \"FREE\"\n    # PR = \"PR\" available soon\n</code></pre>"},{"location":"ref-utils/#bofire.utils.enum.OutputFilteringEnum","title":"<code> OutputFilteringEnum            (Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>bofire/utils/enum.py</code> <pre><code>class OutputFilteringEnum(Enum):\n    ALL = \"ALL\"\n    ANY = \"ANY\"\n</code></pre>"},{"location":"ref-utils/#bofire.utils.enum.RegressionMetricsEnum","title":"<code> RegressionMetricsEnum            (Enum)         </code>","text":"<p>Enumeration class for regression metrics.</p> Source code in <code>bofire/utils/enum.py</code> <pre><code>class RegressionMetricsEnum(Enum):\n\"\"\"Enumeration class for regression metrics.\"\"\"\n\n    R2 = \"R2\"\n    MAE = \"MAE\"\n    MSD = \"MSD\"\n    MAPE = \"MAPE\"\n    PEARSON = \"PEARSON\"\n    SPEARMAN = \"SPEARMAN\"\n    FISHER = \"FISHER\"\n</code></pre>"},{"location":"ref-utils/#bofire.utils.enum.SamplingMethodEnum","title":"<code> SamplingMethodEnum            (Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>bofire/utils/enum.py</code> <pre><code>class SamplingMethodEnum(Enum):\n    UNIFORM = \"UNIFORM\"\n    SOBOL = \"SOBOL\"\n    LHS = \"LHS\"\n</code></pre>"},{"location":"ref-utils/#bofire.utils.enum.ScalerEnum","title":"<code> ScalerEnum            (Enum)         </code>","text":"<p>Enumeration class of supported scalers Currently, normalization and standardization are implemented.</p> Source code in <code>bofire/utils/enum.py</code> <pre><code>class ScalerEnum(Enum):\n\"\"\"Enumeration class of supported scalers\n    Currently, normalization and standardization are implemented.\n    \"\"\"\n\n    NORMALIZE = \"NORMALIZE\"\n    STANDARDIZE = \"STANDARDIZE\"\n</code></pre>"},{"location":"ref-utils/#bofire.utils.multiobjective","title":"<code>multiobjective</code>","text":""},{"location":"ref-utils/#bofire.utils.multiobjective.get_ref_point_mask","title":"<code>get_ref_point_mask(domain, output_feature_keys=None)</code>","text":"<p>Method to get a mask for the reference points taking into account if we want to maximize or minimize an objective. In case it is maximize the value in the mask is 1, in case we want to minimize it is -1.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain for which the mask should be generated.</p> required <code>output_feature_keys</code> <code>Optional[list]</code> <p>Name of output feature keys that should be considered in the mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>description</p> Source code in <code>bofire/utils/multiobjective.py</code> <pre><code>def get_ref_point_mask(\n    domain: Domain, output_feature_keys: Optional[list] = None\n) -&gt; np.ndarray:\n\"\"\"Method to get a mask for the reference points taking into account if we\n    want to maximize or minimize an objective. In case it is maximize the value\n    in the mask is 1, in case we want to minimize it is -1.\n\n    Args:\n        domain (Domain): Domain for which the mask should be generated.\n        output_feature_keys (Optional[list], optional): Name of output feature keys\n            that should be considered in the mask. Defaults to None.\n\n    Returns:\n        np.ndarray: _description_\n    \"\"\"\n    if output_feature_keys is None:\n        output_feature_keys = domain.outputs.get_keys_by_objective(\n            includes=[MaximizeObjective, MinimizeObjective]\n        )\n    if len(output_feature_keys) &lt; 2:\n        raise ValueError(\"At least two output features have to be provided.\")\n    mask = []\n    for key in output_feature_keys:\n        feat = domain.get_feature(key)\n        if isinstance(feat.objective, MaximizeObjective):  # type: ignore\n            mask.append(1.0)\n        elif isinstance(feat.objective, MinimizeObjective):  # type: ignore\n            mask.append(-1.0)\n        else:\n            raise ValueError(\n                \"Only `MaximizeObjective` and `MinimizeObjective` supported\"\n            )\n    return np.array(mask)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce","title":"<code>reduce</code>","text":""},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform","title":"<code> AffineTransform        </code>","text":"<p>Class to switch back and forth from the reduced to the original domain.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>class AffineTransform:\n\"\"\"Class to switch back and forth from the reduced to the original domain.\"\"\"\n\n    def __init__(self, equalities: List[Tuple[str, List[str], List[float]]]):\n\"\"\"Initializes a `AffineTransformation` object.\n\n        Args:\n            equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality\n                is defined as a tuple, in which the first entry is the key of the reduced feature, the second\n                one is a list of feature keys that can be used to compute the feature and the third list of floats\n                are the corresponding coefficients.\n        \"\"\"\n        self.equalities = equalities\n\n    def augment_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Restore the eliminated features in a dataframe\n\n        Args:\n            data (pd.DataFrame): Dataframe that should be restored.\n\n        Returns:\n            pd.DataFrame: Restored dataframe\n        \"\"\"\n        if len(self.equalities) == 0:\n            return data\n        data = data.copy()\n        for name_lhs, names_rhs, coeffs in self.equalities:\n            data[name_lhs] = coeffs[-1]\n            for i, name in enumerate(names_rhs):\n                data[name_lhs] += coeffs[i] * data[name]\n        return data\n\n    def drop_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Drop eliminated features from a dataframe.\n\n        Args:\n            data (pd.DataFrame): Dataframe with features to be dropped.\n\n        Returns:\n            pd.DataFrame: Reduced dataframe.\n        \"\"\"\n        if len(self.equalities) == 0:\n            return data\n        drop = []\n        for name_lhs, _, _ in self.equalities:\n            if name_lhs in data.columns:\n                drop.append(name_lhs)\n        return data.drop(columns=drop)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.__init__","title":"<code>__init__(self, equalities)</code>  <code>special</code>","text":"<p>Initializes a <code>AffineTransformation</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>equalities</code> <code>List[Tuple[str,List[str],List[float]]]</code> <p>List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients.</p> required Source code in <code>bofire/utils/reduce.py</code> <pre><code>def __init__(self, equalities: List[Tuple[str, List[str], List[float]]]):\n\"\"\"Initializes a `AffineTransformation` object.\n\n    Args:\n        equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality\n            is defined as a tuple, in which the first entry is the key of the reduced feature, the second\n            one is a list of feature keys that can be used to compute the feature and the third list of floats\n            are the corresponding coefficients.\n    \"\"\"\n    self.equalities = equalities\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.augment_data","title":"<code>augment_data(self, data)</code>","text":"<p>Restore the eliminated features in a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>Dataframe that should be restored.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Restored dataframe</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def augment_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Restore the eliminated features in a dataframe\n\n    Args:\n        data (pd.DataFrame): Dataframe that should be restored.\n\n    Returns:\n        pd.DataFrame: Restored dataframe\n    \"\"\"\n    if len(self.equalities) == 0:\n        return data\n    data = data.copy()\n    for name_lhs, names_rhs, coeffs in self.equalities:\n        data[name_lhs] = coeffs[-1]\n        for i, name in enumerate(names_rhs):\n            data[name_lhs] += coeffs[i] * data[name]\n    return data\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.drop_data","title":"<code>drop_data(self, data)</code>","text":"<p>Drop eliminated features from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>Dataframe with features to be dropped.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Reduced dataframe.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def drop_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Drop eliminated features from a dataframe.\n\n    Args:\n        data (pd.DataFrame): Dataframe with features to be dropped.\n\n    Returns:\n        pd.DataFrame: Reduced dataframe.\n    \"\"\"\n    if len(self.equalities) == 0:\n        return data\n    drop = []\n    for name_lhs, _, _ in self.equalities:\n        if name_lhs in data.columns:\n            drop.append(name_lhs)\n    return data.drop(columns=drop)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.adjust_boundary","title":"<code>adjust_boundary(feature, coef, rhs)</code>","text":"<p>Adjusts the boundaries of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>ContinuousInputFeature</code> <p>Feature to be adjusted.</p> required <code>coef</code> <code>float</code> <p>Coefficient.</p> required <code>rhs</code> <code>float</code> <p>Right-hand-side of the constraint.</p> required Source code in <code>bofire/utils/reduce.py</code> <pre><code>def adjust_boundary(feature: ContinuousInput, coef: float, rhs: float):\n\"\"\"Adjusts the boundaries of a feature.\n\n    Args:\n        feature (ContinuousInputFeature): Feature to be adjusted.\n        coef (float): Coefficient.\n        rhs (float): Right-hand-side of the constraint.\n    \"\"\"\n    boundary = rhs / coef\n    if coef &gt; 0:\n        if boundary &gt; feature.lower_bound:\n            feature.lower_bound = boundary\n    else:\n        if boundary &lt; feature.upper_bound:\n            feature.upper_bound = boundary\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.check_domain_for_reduction","title":"<code>check_domain_for_reduction(domain)</code>","text":"<p>Check if the reduction can be applied or if a trivial case is present.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain to be checked.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if reducable, else False.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def check_domain_for_reduction(domain: Domain) -&gt; bool:\n\"\"\"Check if the reduction can be applied or if a trivial case is present.\n\n    Args:\n        domain (Domain): Domain to be checked.\n    Returns:\n        bool: True if reducable, else False.\n    \"\"\"\n    # are there any constraints?\n    if len(domain.constraints) == 0:\n        return False\n\n    # are there any linear equality constraints?\n    linear_equalities = domain.cnstrs.get(LinearEqualityConstraint)\n    if len(linear_equalities) == 0:\n        return False\n\n    # are there no NChooseKConstraint constraints?\n    if len(domain.cnstrs.get([NChooseKConstraint])) &gt; 0:\n        return False\n\n    # are there continuous inputs\n    continuous_inputs = domain.get_features(ContinuousInput)\n    if len(continuous_inputs) == 0:\n        return False\n\n    # check that equality constraints only contain continuous inputs\n    for c in linear_equalities:\n        assert isinstance(c, LinearConstraint)\n        for feat in c.features:\n            if feat not in domain.get_feature_keys(ContinuousInput):\n                return False\n    return True\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.check_existence_of_solution","title":"<code>check_existence_of_solution(A_aug)</code>","text":"<p>Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def check_existence_of_solution(A_aug):\n\"\"\"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.\"\"\"\n    A = A_aug[:, :-1]\n    b = A_aug[:, -1]\n    len_inputs = np.shape(A)[1]\n\n    # catch special cases\n    rk_A_aug = np.linalg.matrix_rank(A_aug)\n    rk_A = np.linalg.matrix_rank(A)\n\n    if rk_A == rk_A_aug:\n        if rk_A &lt; len_inputs:\n            return  # all good\n        else:\n            x = np.linalg.solve(A, b)\n            raise Exception(\n                f\"There is a unique solution x for the linear equality constraints: x={x}\"\n            )\n    elif rk_A &lt; rk_A_aug:\n        raise Exception(\n            \"There is no solution fulfilling the linear equality constraints.\"\n        )\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.reduce_domain","title":"<code>reduce_domain(domain)</code>","text":"<p>Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain to be reduced.</p> required <p>Returns:</p> Type Description <code>Tuple[Domain, AffineTransform]</code> <p>reduced domain and the according transformation to switch between the     reduced and orginal domain.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def reduce_domain(domain: Domain) -&gt; Tuple[Domain, AffineTransform]:\n\"\"\"Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated.\n\n    Args:\n        domain (Domain): Domain to be reduced.\n\n    Returns:\n        Tuple[Domain, AffineTransform]: reduced domain and the according transformation to switch between the\n            reduced and orginal domain.\n    \"\"\"\n    # check if the domain can be reduced\n    if not check_domain_for_reduction(domain):\n        return domain, AffineTransform([])\n\n    # find linear equality constraints\n    linear_equalities = domain.cnstrs.get(LinearEqualityConstraint)\n    other_constraints = domain.cnstrs.get(\n        Constraint, excludes=[LinearEqualityConstraint]\n    )\n\n    # only consider continuous inputs\n    continuous_inputs = [\n        cast(ContinuousInput, f) for f in domain.get_features(ContinuousInput)\n    ]\n    other_inputs = domain.inputs.get(InputFeature, excludes=[ContinuousInput])\n\n    # assemble Matrix A from equality constraints\n    N = len(linear_equalities)\n    M = len(continuous_inputs) + 1\n    names = np.concatenate(([feat.key for feat in continuous_inputs], [\"rhs\"]))\n\n    A_aug = pd.DataFrame(data=np.zeros(shape=(N, M)), columns=names)\n\n    for i in range(len(linear_equalities)):\n        c = linear_equalities[i]\n        assert isinstance(c, LinearEqualityConstraint)\n        A_aug.loc[i, c.features] = c.coefficients  # type: ignore\n        A_aug.loc[i, \"rhs\"] = c.rhs\n    A_aug = A_aug.values\n\n    # catch special cases\n    check_existence_of_solution(A_aug)\n\n    # bring A_aug to reduced row-echelon form\n    A_aug_rref, pivots = rref(A_aug)\n    pivots = np.array(pivots)\n    A_aug_rref = np.array(A_aug_rref).astype(np.float64)\n\n    # formulate box bounds as linear inequality constraints in matrix form\n    B = np.zeros(shape=(2 * (M - 1), M))\n    B[: M - 1, : M - 1] = np.eye(M - 1)\n    B[M - 1 :, : M - 1] = -np.eye(M - 1)\n\n    B[: M - 1, -1] = np.array([feat.upper_bound for feat in continuous_inputs])\n    B[M - 1 :, -1] = -1.0 * np.array([feat.lower_bound for feat in continuous_inputs])\n\n    # eliminate columns with pivot element\n    for i in range(len(pivots)):\n        p = pivots[i]\n        B[p, :] -= A_aug_rref[i, :]\n        B[p + M - 1, :] += A_aug_rref[i, :]\n\n    # build up reduced domain\n    _domain = Domain.construct(\n        # _fields_set = {\"input_features\", \"output_features\", \"constraints\"}\n        input_features=deepcopy(other_inputs),\n        output_features=deepcopy(domain.output_features),\n        constraints=deepcopy(other_constraints),\n    )\n    new_inputs = [\n        deepcopy(feat) for i, feat in enumerate(continuous_inputs) if i not in pivots\n    ]\n    all_inputs = _domain.inputs + new_inputs\n    assert isinstance(all_inputs, InputFeatures)\n    _domain.input_features = all_inputs\n\n    constraints: List[AnyConstraint] = []\n    for i in pivots:\n        # reduce equation system of upper bounds\n        ind = np.where(B[i, :-1] != 0)[0]\n        if len(ind) &gt; 0 and B[i, -1] &lt; np.inf:\n            if len(list(names[ind])) &gt; 1:\n                c = LinearInequalityConstraint.from_greater_equal(\n                    features=list(names[ind]),\n                    coefficients=(-1.0 * B[i, ind]).tolist(),\n                    rhs=B[i, -1] * -1.0,\n                )\n                constraints.append(c)\n            else:\n                key = names[ind][0]\n                feat = cast(ContinuousInput, _domain.get_feature(key))\n                adjust_boundary(feat, (-1.0 * B[i, ind])[0], B[i, -1] * -1.0)\n        else:\n            if B[i, -1] &lt; -1e-16:\n                raise Exception(\"There is no solution that fulfills the constraints.\")\n\n        # reduce equation system of lower bounds\n        ind = np.where(B[i + M - 1, :-1] != 0)[0]\n        if len(ind) &gt; 0 and B[i + M - 1, -1] &lt; np.inf:\n            if len(list(names[ind])) &gt; 1:\n                c = LinearInequalityConstraint.from_greater_equal(\n                    features=list(names[ind]),\n                    coefficients=(-1.0 * B[i + M - 1, ind]).tolist(),\n                    rhs=B[i + M - 1, -1] * -1.0,\n                )\n                constraints.append(c)\n            else:\n                key = names[ind][0]\n                feat = cast(ContinuousInput, _domain.get_feature(key))\n                adjust_boundary(\n                    feat,\n                    (-1.0 * B[i + M - 1, ind])[0],\n                    B[i + M - 1, -1] * -1.0,\n                )\n        else:\n            if B[i + M - 1, -1] &lt; -1e-16:\n                raise Exception(\"There is no solution that fulfills the constraints.\")\n\n    if len(constraints) &gt; 0:\n        _domain._set_constraints_unvalidated(_domain.cnstrs + constraints)\n\n    # assemble equalities\n    _equalities = []\n    for i in range(len(pivots)):\n        name_lhs = names[pivots[i]]\n        names_rhs = []\n        coeffs = []\n\n        for j in range(len(names) - 1):\n            if A_aug_rref[i, j] != 0 and j != pivots[i]:\n                coeffs.append(-A_aug_rref[i, j])\n                names_rhs.append(names[j])\n\n        coeffs.append(A_aug_rref[i, -1])\n\n        _equalities.append((name_lhs, names_rhs, coeffs))\n\n    trafo = AffineTransform(_equalities)\n    # remove remaining dependencies of eliminated inputs from the problem\n    _domain = remove_eliminated_inputs(_domain, trafo)\n    return _domain, trafo\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.remove_eliminated_inputs","title":"<code>remove_eliminated_inputs(domain, transform)</code>","text":"<p>Eliminates remaining occurences of eliminated inputs in linear constraints.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain in which the linear constraints should be purged.</p> required <code>transform</code> <code>AffineTransform</code> <p>Affine transformation object that defines the obsolete features.</p> required <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If feature occurs in a constraint different from a linear one.</p> <p>Returns:</p> Type Description <code>Domain</code> <p>Purged domain.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def remove_eliminated_inputs(domain: Domain, transform: AffineTransform) -&gt; Domain:\n\"\"\"Eliminates remaining occurences of eliminated inputs in linear constraints.\n\n    Args:\n        domain (Domain): Domain in which the linear constraints should be purged.\n        transform (AffineTransform): Affine transformation object that defines the obsolete features.\n\n    Raises:\n        ValueError: If feature occurs in a constraint different from a linear one.\n\n    Returns:\n        Domain: Purged domain.\n    \"\"\"\n    inputs_names = domain.get_feature_keys()\n    M = len(inputs_names)\n\n    # write the equalities for the backtransformation into one matrix\n    inputs_dict = {inputs_names[i]: i for i in range(M)}\n\n    # build up dict from domain.equalities e.g. {\"xi1\": [coeff(xj1), ..., coeff(xjn)], ... \"xik\":...}\n    coeffs_dict = {}\n    for i, e in enumerate(transform.equalities):\n        coeffs = np.zeros(M + 1)\n        for j, name in enumerate(e[1]):\n            coeffs[inputs_dict[name]] = e[2][j]\n        coeffs[-1] = e[2][-1]\n        coeffs_dict[e[0]] = coeffs\n\n    constraints = []\n    for c in domain.cnstrs.get():\n        # Nonlinear constraints not supported\n        if not isinstance(c, LinearConstraint):\n            raise ValueError(\n                \"Elimination of variables is only supported for LinearEquality and LinearInequality constraints.\"\n            )\n\n        # no changes, if the constraint does not contain eliminated inputs\n        elif all(name in inputs_names for name in c.features):\n            constraints.append(c)\n\n        # remove inputs from the constraint that were eliminated from the inputs before\n        else:\n            totally_removed = False\n            _features = np.array(inputs_names)\n            _rhs = c.rhs\n\n            # create new lhs and rhs from the old one and knowledge from problem._equalities\n            _coefficients = np.zeros(M)\n            for j, name in enumerate(c.features):\n                if name in inputs_names:\n                    _coefficients[inputs_dict[name]] += c.coefficients[j]\n                else:\n                    _coefficients += c.coefficients[j] * coeffs_dict[name][:-1]\n                    _rhs -= c.coefficients[j] * coeffs_dict[name][-1]\n\n            _features = _features[np.abs(_coefficients) &gt; 1e-16]\n            _coefficients = _coefficients[np.abs(_coefficients) &gt; 1e-16]\n            _c = None\n            if isinstance(c, LinearEqualityConstraint):\n\n                if len(_features) &gt; 1:\n                    _c = LinearEqualityConstraint(\n                        features=_features.tolist(),\n                        coefficients=_coefficients.tolist(),\n                        rhs=_rhs,\n                    )\n                elif len(_features) == 0:\n                    totally_removed = True\n                else:\n                    feat: ContinuousInput = ContinuousInput(\n                        **domain.get_feature(_features[0]).dict()\n                    )\n                    feat.lower_bound = _coefficients[0]\n                    feat.upper_bound = _coefficients[0]\n                    totally_removed = True\n            else:\n                if len(_features) &gt; 1:\n                    _c = LinearInequalityConstraint(\n                        features=_features.tolist(),\n                        coefficients=_coefficients.tolist(),\n                        rhs=_rhs,\n                    )\n                elif len(_features) == 0:\n                    totally_removed = True\n                else:\n                    feat = cast(ContinuousInput, domain.get_feature(_features[0]))\n                    adjust_boundary(feat, _coefficients[0], _rhs)\n                    totally_removed = True\n\n            # check if constraint is always fulfilled/not fulfilled\n            if not totally_removed:\n                assert _c is not None\n                if len(_c.features) == 0 and _c.rhs &gt;= 0:\n                    pass\n                elif len(_c.features) == 0 and _c.rhs &lt; 0:\n                    raise Exception(\"Linear constraints cannot be fulfilled.\")\n                elif np.isinf(_c.rhs):\n                    pass\n                else:\n                    constraints.append(_c)\n    domain.constraints = Constraints(constraints=constraints)\n    return domain\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.rref","title":"<code>rref(A, tol=1e-08)</code>","text":"<p>Computes the reduced row echelon form of a Matrix</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>2d array representing a matrix.</p> required <code>tol</code> <code>float</code> <p>tolerance for rounding to 0. Defaults to 1e-8.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tuple[numpy.ndarray, List[int]]</code> <p>(A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def rref(A: np.ndarray, tol: float = 1e-8) -&gt; Tuple[np.ndarray, List[int]]:\n\"\"\"Computes the reduced row echelon form of a Matrix\n\n    Args:\n        A (ndarray): 2d array representing a matrix.\n        tol (float, optional): tolerance for rounding to 0. Defaults to 1e-8.\n\n    Returns:\n        (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots\n        is a numpy array containing the pivot columns of A_rref\n    \"\"\"\n    A = np.array(A, dtype=np.float64)\n    n, m = np.shape(A)\n\n    col = 0\n    row = 0\n    pivots = []\n\n    for col in range(m):\n        # does a pivot element exist?\n        if all(np.abs(A[row:, col]) &lt; tol):\n            pass\n        # if yes: start elimination\n        else:\n            pivots.append(col)\n            max_row = np.argmax(np.abs(A[row:, col])) + row\n            # switch to most stable row\n            A[[row, max_row], :] = A[[max_row, row], :]  # type: ignore\n            # normalize row\n            A[row, :] /= A[row, col]\n            # eliminate other elements from column\n            for r in range(n):\n                if r != row:\n                    A[r, :] -= A[r, col] / A[row, col] * A[row, :]\n            row += 1\n\n    prec = int(-np.log10(tol))\n    return np.round(A, prec), pivots\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools","title":"<code>torch_tools</code>","text":""},{"location":"ref-utils/#bofire.utils.torch_tools.OneHotToNumeric","title":"<code> OneHotToNumeric            (InputTransform, Module)         </code>","text":"<p>Transform categorical parameters from a one-hot to a numeric representation. This assumes that the categoricals are the trailing dimensions.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>class OneHotToNumeric(InputTransform, Module):\nr\"\"\"Transform categorical parameters from a one-hot to a numeric representation.\n    This assumes that the categoricals are the trailing dimensions.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        categorical_features: Optional[Dict[int, int]] = None,\n        transform_on_train: bool = True,\n        transform_on_eval: bool = True,\n        transform_on_fantasize: bool = True,\n    ) -&gt; None:\nr\"\"\"Initialize.\n        Args:\n            dim: The dimension of the one-hot-encoded input.\n            categorical_features: A dictionary mapping the starting index of each\n                categorical feature to its cardinality. This assumes that categoricals\n                are one-hot encoded.\n            transform_on_train: A boolean indicating whether to apply the\n                transforms in train() mode. Default: False.\n            transform_on_eval: A boolean indicating whether to apply the\n                transform in eval() mode. Default: True.\n            transform_on_fantasize: A boolean indicating whether to apply the\n                transform when called from within a `fantasize` call. Default: False.\n        Returns:\n            A `batch_shape x n x d'`-dim tensor of where the one-hot encoded\n            categoricals are transformed to integer representation.\n        \"\"\"\n        super().__init__()\n        self.transform_on_train = transform_on_train\n        self.transform_on_eval = transform_on_eval\n        self.transform_on_fantasize = transform_on_fantasize\n        categorical_features = categorical_features or {}\n        # sort by starting index\n        self.categorical_features = OrderedDict(\n            sorted(categorical_features.items(), key=lambda x: x[0])\n        )\n        if len(self.categorical_features) &gt; 0:\n            self.categorical_start_idx = min(self.categorical_features.keys())\n            # check that the trailing dimensions are categoricals\n            end = self.categorical_start_idx\n            err_msg = (\n                f\"{self.__class__.__name__} requires that the categorical \"\n                \"parameters are the rightmost elements.\"\n            )\n            for start, card in self.categorical_features.items():\n                # the end of one one-hot representation should be followed\n                # by the start of the next\n                if end != start:\n                    raise ValueError(err_msg)\n                end = start + card\n            if end != dim:\n                # check end\n                raise ValueError(err_msg)\n            # the numeric representation dimension is the total number of parameters\n            # (continuous, integer, and categorical)\n            self.numeric_dim = self.categorical_start_idx + len(categorical_features)\n\n    def transform(self, X: Tensor) -&gt; Tensor:\nr\"\"\"Transform the categorical inputs into integer representation.\n        Args:\n            X: A `batch_shape x n x d`-dim tensor of inputs.\n        Returns:\n            A `batch_shape x n x d'`-dim tensor of where the one-hot encoded\n            categoricals are transformed to integer representation.\n        \"\"\"\n        if len(self.categorical_features) &gt; 0:\n            X_numeric = X[..., : self.numeric_dim].clone()\n            idx = self.categorical_start_idx\n            for start, card in self.categorical_features.items():\n                X_numeric[..., idx] = X[..., start : start + card].argmax(dim=-1)\n                idx += 1\n            return X_numeric\n        return X\n\n    def untransform(self, X: Tensor) -&gt; Tensor:\nr\"\"\"Transform the categoricals from integer representation to one-hot.\n        Args:\n            X: A `batch_shape x n x d'`-dim tensor of transformed inputs, where\n                the categoricals are represented as integers.\n        Returns:\n            A `batch_shape x n x d`-dim tensor of inputs, where the categoricals\n            have been transformed to one-hot representation.\n        \"\"\"\n        if len(self.categorical_features) &gt; 0:\n            self.numeric_dim\n            one_hot_categoricals = [\n                # note that self.categorical_features is sorted by the starting index\n                # in one-hot representation\n                one_hot(\n                    X[..., idx - len(self.categorical_features)].long(),\n                    num_classes=cardinality,\n                )\n                for idx, cardinality in enumerate(self.categorical_features.values())\n            ]\n            X = torch.cat(\n                [\n                    X[..., : self.categorical_start_idx],\n                    *one_hot_categoricals,\n                ],\n                dim=-1,\n            )\n        return X\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.OneHotToNumeric.__init__","title":"<code>__init__(self, dim, categorical_features=None, transform_on_train=True, transform_on_eval=True, transform_on_fantasize=True)</code>  <code>special</code>","text":"<p>Initialize.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension of the one-hot-encoded input.</p> required <code>categorical_features</code> <code>Optional[Dict[int, int]]</code> <p>A dictionary mapping the starting index of each categorical feature to its cardinality. This assumes that categoricals are one-hot encoded.</p> <code>None</code> <code>transform_on_train</code> <code>bool</code> <p>A boolean indicating whether to apply the transforms in train() mode. Default: False.</p> <code>True</code> <code>transform_on_eval</code> <code>bool</code> <p>A boolean indicating whether to apply the transform in eval() mode. Default: True.</p> <code>True</code> <code>transform_on_fantasize</code> <code>bool</code> <p>A boolean indicating whether to apply the transform when called from within a <code>fantasize</code> call. Default: False.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>A <code>batch_shape x n x d'</code>-dim tensor of where the one-hot encoded categoricals are transformed to integer representation.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    categorical_features: Optional[Dict[int, int]] = None,\n    transform_on_train: bool = True,\n    transform_on_eval: bool = True,\n    transform_on_fantasize: bool = True,\n) -&gt; None:\nr\"\"\"Initialize.\n    Args:\n        dim: The dimension of the one-hot-encoded input.\n        categorical_features: A dictionary mapping the starting index of each\n            categorical feature to its cardinality. This assumes that categoricals\n            are one-hot encoded.\n        transform_on_train: A boolean indicating whether to apply the\n            transforms in train() mode. Default: False.\n        transform_on_eval: A boolean indicating whether to apply the\n            transform in eval() mode. Default: True.\n        transform_on_fantasize: A boolean indicating whether to apply the\n            transform when called from within a `fantasize` call. Default: False.\n    Returns:\n        A `batch_shape x n x d'`-dim tensor of where the one-hot encoded\n        categoricals are transformed to integer representation.\n    \"\"\"\n    super().__init__()\n    self.transform_on_train = transform_on_train\n    self.transform_on_eval = transform_on_eval\n    self.transform_on_fantasize = transform_on_fantasize\n    categorical_features = categorical_features or {}\n    # sort by starting index\n    self.categorical_features = OrderedDict(\n        sorted(categorical_features.items(), key=lambda x: x[0])\n    )\n    if len(self.categorical_features) &gt; 0:\n        self.categorical_start_idx = min(self.categorical_features.keys())\n        # check that the trailing dimensions are categoricals\n        end = self.categorical_start_idx\n        err_msg = (\n            f\"{self.__class__.__name__} requires that the categorical \"\n            \"parameters are the rightmost elements.\"\n        )\n        for start, card in self.categorical_features.items():\n            # the end of one one-hot representation should be followed\n            # by the start of the next\n            if end != start:\n                raise ValueError(err_msg)\n            end = start + card\n        if end != dim:\n            # check end\n            raise ValueError(err_msg)\n        # the numeric representation dimension is the total number of parameters\n        # (continuous, integer, and categorical)\n        self.numeric_dim = self.categorical_start_idx + len(categorical_features)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.OneHotToNumeric.transform","title":"<code>transform(self, X)</code>","text":"<p>Transform the categorical inputs into integer representation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>A <code>batch_shape x n x d</code>-dim tensor of inputs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A <code>batch_shape x n x d'</code>-dim tensor of where the one-hot encoded categoricals are transformed to integer representation.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def transform(self, X: Tensor) -&gt; Tensor:\nr\"\"\"Transform the categorical inputs into integer representation.\n    Args:\n        X: A `batch_shape x n x d`-dim tensor of inputs.\n    Returns:\n        A `batch_shape x n x d'`-dim tensor of where the one-hot encoded\n        categoricals are transformed to integer representation.\n    \"\"\"\n    if len(self.categorical_features) &gt; 0:\n        X_numeric = X[..., : self.numeric_dim].clone()\n        idx = self.categorical_start_idx\n        for start, card in self.categorical_features.items():\n            X_numeric[..., idx] = X[..., start : start + card].argmax(dim=-1)\n            idx += 1\n        return X_numeric\n    return X\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.OneHotToNumeric.untransform","title":"<code>untransform(self, X)</code>","text":"<p>Transform the categoricals from integer representation to one-hot.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>A <code>batch_shape x n x d'</code>-dim tensor of transformed inputs, where the categoricals are represented as integers.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A <code>batch_shape x n x d</code>-dim tensor of inputs, where the categoricals have been transformed to one-hot representation.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def untransform(self, X: Tensor) -&gt; Tensor:\nr\"\"\"Transform the categoricals from integer representation to one-hot.\n    Args:\n        X: A `batch_shape x n x d'`-dim tensor of transformed inputs, where\n            the categoricals are represented as integers.\n    Returns:\n        A `batch_shape x n x d`-dim tensor of inputs, where the categoricals\n        have been transformed to one-hot representation.\n    \"\"\"\n    if len(self.categorical_features) &gt; 0:\n        self.numeric_dim\n        one_hot_categoricals = [\n            # note that self.categorical_features is sorted by the starting index\n            # in one-hot representation\n            one_hot(\n                X[..., idx - len(self.categorical_features)].long(),\n                num_classes=cardinality,\n            )\n            for idx, cardinality in enumerate(self.categorical_features.values())\n        ]\n        X = torch.cat(\n            [\n                X[..., : self.categorical_start_idx],\n                *one_hot_categoricals,\n            ],\n            dim=-1,\n        )\n    return X\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_linear_constraints","title":"<code>get_linear_constraints(domain, constraint, unit_scaled=False)</code>","text":"<p>Converts linear constraints to the form required by BoTorch.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Optimization problem definition.</p> required <code>constraint</code> <code>Union[LinearEqualityConstraint, LinearInequalityConstraint]</code> <p>Type of constraint that should be converted.</p> required <code>unit_scaled</code> <code>bool</code> <p>If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tuple[Tensor, Tensor, float]]</code> <p>List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_linear_constraints(\n    domain: Domain,\n    constraint: Union[LinearEqualityConstraint, LinearInequalityConstraint],\n    unit_scaled: bool = False,\n) -&gt; List[Tuple[Tensor, Tensor, float]]:\n\"\"\"Converts linear constraints to the form required by BoTorch.\n\n    Args:\n        domain (Domain): Optimization problem definition.\n        constraint (Union[LinearEqualityConstraint, LinearInequalityConstraint]): Type of constraint that should be converted.\n        unit_scaled (bool, optional): If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False.\n\n    Returns:\n        List[Tuple[Tensor, Tensor, float]]: List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs.\n    \"\"\"\n    constraints = []\n    for c in domain.cnstrs.get(constraint):\n        indices = []\n        coefficients = []\n        lower = []\n        upper = []\n        rhs = 0.0\n        for i, featkey in enumerate(c.features):  # type: ignore\n            idx = domain.get_feature_keys(InputFeature).index(featkey)\n            feat = domain.get_feature(featkey)\n            if feat.is_fixed():  # type: ignore\n                rhs -= feat.fixed_value()[0] * c.coefficients[i]  # type: ignore\n            else:\n                lower.append(feat.lower_bound)  # type: ignore\n                upper.append(feat.upper_bound)  # type: ignore\n                indices.append(idx)\n                coefficients.append(\n                    c.coefficients[i]  # type: ignore\n                )  # if unit_scaled == False else c_scaled.coefficients[i])\n        if unit_scaled:\n            lower = np.array(lower)\n            upper = np.array(upper)\n            s = upper - lower\n            scaled_coefficients = s * np.array(coefficients)\n            constraints.append(\n                (\n                    torch.tensor(indices),\n                    -torch.tensor(scaled_coefficients).to(**tkwargs),\n                    -(rhs + c.rhs - np.sum(np.array(coefficients) * lower)),  # type: ignore\n                )\n            )\n        else:\n            constraints.append(\n                (\n                    torch.tensor(indices),\n                    -torch.tensor(coefficients).to(**tkwargs),\n                    -(rhs + c.rhs),  # type: ignore\n                )\n            )\n    return constraints\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_output_constraints","title":"<code>get_output_constraints(output_features)</code>","text":"<p>Method to translate output constraint objectives into a list of callables and list of etas for use in botorch.</p> <p>Parameters:</p> Name Type Description Default <code>output_features</code> <code>OutputFeatures</code> <p>Output feature object that should be processed.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Callable[[Tensor], Tensor]], List[float]]</code> <p>List of constraint callables,     list of associated etas.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_output_constraints(\n    output_features: OutputFeatures,\n) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n\"\"\"Method to translate output constraint objectives into a list of\n    callables and list of etas for use in botorch.\n\n    Args:\n        output_features (OutputFeatures): Output feature object that should\n            be processed.\n\n    Returns:\n        Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of constraint callables,\n            list of associated etas.\n    \"\"\"\n    constraints = []\n    etas = []\n    for idx, feat in enumerate(output_features.get()):\n        if isinstance(feat.objective, BotorchConstrainedObjective):  # type: ignore\n            iconstraints, ietas = feat.objective.to_constraints(idx=idx)  # type: ignore\n            constraints += iconstraints\n            etas += ietas\n    return constraints, etas\n</code></pre>"}]}