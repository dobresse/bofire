{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction BoFire is a framework to define and solve or black-box optimization problems. These problems can arise in a number of closely related fields including experimental design, multiobjective optimization and decision making, and Bayesian optimization. BoFire problem specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved. Experimental design In the context of experimental design BoFire allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values. Multiobjective optimization In the context of multiobjective optimization BoFire allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision. Bayesian optimization In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Introduction"},{"location":"#introduction","text":"BoFire is a framework to define and solve or black-box optimization problems. These problems can arise in a number of closely related fields including experimental design, multiobjective optimization and decision making, and Bayesian optimization. BoFire problem specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.","title":"Introduction"},{"location":"#experimental-design","text":"In the context of experimental design BoFire allows to define a design space \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] where the design parameters may take values depending on their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) and a set of equations define additional experimental constraints, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.","title":"Experimental design"},{"location":"#multiobjective-optimization","text":"In the context of multiobjective optimization BoFire allows to define a vector-valued optimization problem \\[ \\min_{x \\in \\mathbb{X}} s(y(x)) \\] where \\(x \\in \\mathbb{X}\\) is again the experimental design space \\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and \\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be minimized, e.g. \\(s_1\\) is the identity function if \\(y_1\\) is to be minimized. Since the objectives are in general conflicting, there is no point \\(x\\) that simulataneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises. A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.","title":"Multiobjective optimization"},{"location":"#bayesian-optimization","text":"In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, {data})\\) that estimates the distribution of posible outcomes for \\(y\\) . An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation \\[ \\min_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] and the minimizer \\(x_\\mathrm{opt}\\) of this acquisition function. determines the next experiment \\(y(x)\\) to run. When are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.","title":"Bayesian optimization"},{"location":"install/","text":"Installation For the latest stable release install pip install bofire To live at head you can use pip install git+https://github.com/experimental-design/bofire.git","title":"Install"},{"location":"install/#installation","text":"For the latest stable release install pip install bofire To live at head you can use pip install git+https://github.com/experimental-design/bofire.git","title":"Installation"},{"location":"ref-constraints/","text":"Domain Constraint ( BaseModel ) pydantic-model Abstract base class to define constraints on the optimization space. Source code in bofire/domain/constraints.py class Constraint ( BaseModel ): \"\"\"Abstract base class to define constraints on the optimization space.\"\"\" @abstractmethod def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Args: experiments (pd.DataFrame): Dataframe to check constraint fulfillment. Returns: bool: True if fulfilled else False \"\"\" pass @abstractmethod def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluates the constraint. Args: experiments (pd.DataFrame): Dataframe to evaluate the constraint on. Returns: pd.Series: Distance to reach constraint fulfillment. \"\"\" pass def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the constraint. Returns: Dict: Serialized version of the constraint as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } @staticmethod def from_config ( config : Dict ) -> \"Constraint\" : \"\"\"Generate constraint out of serialized version. Args: config (Dict): Serialized version of a constraint Returns: Constraint: Instaniated constraint of the type specified in the `config`. \"\"\" mapper = { \"LinearEqualityConstraint\" : LinearEqualityConstraint , \"LinearInequalityConstraint\" : LinearInequalityConstraint , \"NChooseKConstraint\" : NChooseKConstraint , \"NonlinearEqualityConstraint\" : NonlinearEqualityConstraint , \"NonlinearInqualityConstraint\" : NonlinearInqualityConstraint , } return mapper [ config [ \"type\" ]]( ** config ) __call__ ( self , experiments ) special Numerically evaluates the constraint. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to evaluate the constraint on. required Returns: Type Description pd.Series Distance to reach constraint fulfillment. Source code in bofire/domain/constraints.py @abstractmethod def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluates the constraint. Args: experiments (pd.DataFrame): Dataframe to evaluate the constraint on. Returns: pd.Series: Distance to reach constraint fulfillment. \"\"\" pass from_config ( config ) staticmethod Generate constraint out of serialized version. Parameters: Name Type Description Default config Dict Serialized version of a constraint required Returns: Type Description Constraint Instaniated constraint of the type specified in the config . Source code in bofire/domain/constraints.py @staticmethod def from_config ( config : Dict ) -> \"Constraint\" : \"\"\"Generate constraint out of serialized version. Args: config (Dict): Serialized version of a constraint Returns: Constraint: Instaniated constraint of the type specified in the `config`. \"\"\" mapper = { \"LinearEqualityConstraint\" : LinearEqualityConstraint , \"LinearInequalityConstraint\" : LinearInequalityConstraint , \"NChooseKConstraint\" : NChooseKConstraint , \"NonlinearEqualityConstraint\" : NonlinearEqualityConstraint , \"NonlinearInqualityConstraint\" : NonlinearInqualityConstraint , } return mapper [ config [ \"type\" ]]( ** config ) is_fulfilled ( self , experiments ) Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py @abstractmethod def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Args: experiments (pd.DataFrame): Dataframe to check constraint fulfillment. Returns: bool: True if fulfilled else False \"\"\" pass to_config ( self ) Generate serialized version of the constraint. Returns: Type Description Dict Serialized version of the constraint as dictionary. Source code in bofire/domain/constraints.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the constraint. Returns: Dict: Serialized version of the constraint as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } Constraints ( BaseModel ) pydantic-model Source code in bofire/domain/constraints.py class Constraints ( BaseModel ): constraints : Sequence [ Constraint ] = Field ( default_factory = lambda : []) def to_config ( self ) -> List : \"\"\"Serializes a `Constraints` object. Returns: List: Constraints objects as serialized list. \"\"\" return [ constraint . to_config () for constraint in self . constraints ] @classmethod def from_config ( cls , config : List ) -> \"Constraints\" : \"\"\"Instantiates a `Constraints` object based on the serialized list. Args: config (List): Serialized `Constraints` object as list. Returns: Constraints: Initialized `Constraints` object. \"\"\" return cls ( constraints = [ Constraint . from_config ( constraint ) for constraint in config ] ) def __iter__ ( self ): return iter ( self . constraints ) def __len__ ( self ): return len ( self . constraints ) def __getitem__ ( self , i ): return self . constraints [ i ] def __add__ ( self , other : Union [ Sequence [ Constraint ], \"Constraints\" ] ) -> \"Constraints\" : if isinstance ( other , collections . abc . Sequence ): other_constraints = other else : other_constraints = other . constraints constraints = list ( chain ( self . constraints , other_constraints )) return Constraints ( constraints = constraints ) def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints Args: experiments (pd.DataFrame): data to evaluate the constraint on Returns: pd.DataFrame: Constraint evaluation for each of the constraints \"\"\" return pd . concat ([ c ( experiments ) for c in self . constraints ], axis = 1 ) def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe Args: df_data (pd.DataFrame): Dataframe with data, the constraint validity should be tested on Returns: Boolean: True if all constraints are fulfilled for all rows, false if not \"\"\" if len ( self . constraints ) == 0 : return pd . Series ([ True ] * len ( experiments ), index = experiments . index ) return pd . concat ( [ c . is_fulfilled ( experiments ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) def get ( self , includes : Union [ Type , List [ Type ]] = Constraint , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> \"Constraints\" : \"\"\"get constraints of the domain Args: includes (Union[Constraint, List[Constraint]], optional): Constraint class or list of specific constraint classes to be returned. Defaults to Constraint. excludes (Union[Type, List[Type]], optional): Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[Constraint]: List of constraints in the domain fitting to the passed requirements. \"\"\" return Constraints ( constraints = filter_by_class ( self . constraints , includes = includes , excludes = excludes , exact = exact , ) ) __call__ ( self , experiments ) special Numerically evaluate all constraints Parameters: Name Type Description Default experiments pd.DataFrame data to evaluate the constraint on required Returns: Type Description pd.DataFrame Constraint evaluation for each of the constraints Source code in bofire/domain/constraints.py def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints Args: experiments (pd.DataFrame): data to evaluate the constraint on Returns: pd.DataFrame: Constraint evaluation for each of the constraints \"\"\" return pd . concat ([ c ( experiments ) for c in self . constraints ], axis = 1 ) from_config ( config ) classmethod Instantiates a Constraints object based on the serialized list. Parameters: Name Type Description Default config List Serialized Constraints object as list. required Returns: Type Description Constraints Initialized Constraints object. Source code in bofire/domain/constraints.py @classmethod def from_config ( cls , config : List ) -> \"Constraints\" : \"\"\"Instantiates a `Constraints` object based on the serialized list. Args: config (List): Serialized `Constraints` object as list. Returns: Constraints: Initialized `Constraints` object. \"\"\" return cls ( constraints = [ Constraint . from_config ( constraint ) for constraint in config ] ) get ( self , includes =< class ' bofire . domain . constraints . Constraint '>, excludes=None, exact=False) get constraints of the domain Parameters: Name Type Description Default includes Union[Constraint, List[Constraint]] Constraint class or list of specific constraint classes to be returned. Defaults to Constraint. <class 'bofire.domain.constraints.Constraint'> excludes Union[Type, List[Type]] Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[Constraint] List of constraints in the domain fitting to the passed requirements. Source code in bofire/domain/constraints.py def get ( self , includes : Union [ Type , List [ Type ]] = Constraint , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> \"Constraints\" : \"\"\"get constraints of the domain Args: includes (Union[Constraint, List[Constraint]], optional): Constraint class or list of specific constraint classes to be returned. Defaults to Constraint. excludes (Union[Type, List[Type]], optional): Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[Constraint]: List of constraints in the domain fitting to the passed requirements. \"\"\" return Constraints ( constraints = filter_by_class ( self . constraints , includes = includes , excludes = excludes , exact = exact , ) ) is_fulfilled ( self , experiments ) Check if all constraints are fulfilled on all rows of the provided dataframe Parameters: Name Type Description Default df_data pd.DataFrame Dataframe with data, the constraint validity should be tested on required Returns: Type Description Boolean True if all constraints are fulfilled for all rows, false if not Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe Args: df_data (pd.DataFrame): Dataframe with data, the constraint validity should be tested on Returns: Boolean: True if all constraints are fulfilled for all rows, false if not \"\"\" if len ( self . constraints ) == 0 : return pd . Series ([ True ] * len ( experiments ), index = experiments . index ) return pd . concat ( [ c . is_fulfilled ( experiments ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) to_config ( self ) Serializes a Constraints object. Returns: Type Description List Constraints objects as serialized list. Source code in bofire/domain/constraints.py def to_config ( self ) -> List : \"\"\"Serializes a `Constraints` object. Returns: List: Constraints objects as serialized list. \"\"\" return [ constraint . to_config () for constraint in self . constraints ] LinearConstraint ( Constraint ) pydantic-model Abstract base class for linear equality and inequality constraints. Attributes: Name Type Description features list list of feature keys (str) on which the constraint works on. coefficients list list of coefficients (float) of the constraint. rhs float Right-hand side of the constraint Source code in bofire/domain/constraints.py class LinearConstraint ( Constraint ): \"\"\"Abstract base class for linear equality and inequality constraints. Attributes: features (list): list of feature keys (str) on which the constraint works on. coefficients (list): list of coefficients (float) of the constraint. rhs (float): Right-hand side of the constraint \"\"\" features : TFeatureKeys coefficients : TCoefficients rhs : float @validator ( \"features\" ) def validate_features_unique ( cls , features ): \"\"\"Validate that feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\" if len ( values [ \"features\" ]) != len ( values [ \"coefficients\" ]): raise ValueError ( f 'must provide same number of features and coefficients, got { len ( values [ \"features\" ]) } != { len ( values [ \"coefficients\" ]) } ' ) return values def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : return ( experiments [ self . features ] @ self . coefficients - self . rhs ) / np . linalg . norm ( self . coefficients ) # def lhs(self, df_data: pd.DataFrame) -> float: # \"\"\"Evaluate the left-hand side of the constraint on each row of a dataframe # Args: # df_data (pd.DataFrame): Dataframe on which the left-hand side should be evaluated. # Returns: # np.array: 1-dim array with left-hand side of each row of the provided dataframe. # \"\"\" # cols = self.features # coefficients = self.coefficients # return np.sum(df_data[cols].values * np.array(coefficients), axis=1) def __str__ ( self ) -> str : \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return \" + \" . join ( [ f \" { self . coefficients [ i ] } * { feat } \" for i , feat in enumerate ( self . features )] ) __str__ ( self ) special Generate string representation of the constraint. Returns: Type Description str string representation of the constraint. Source code in bofire/domain/constraints.py def __str__ ( self ) -> str : \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return \" + \" . join ( [ f \" { self . coefficients [ i ] } * { feat } \" for i , feat in enumerate ( self . features )] ) validate_features_unique ( features ) classmethod Validate that feature keys are unique. Source code in bofire/domain/constraints.py @validator ( \"features\" ) def validate_features_unique ( cls , features ): \"\"\"Validate that feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features validate_list_lengths ( values ) classmethod Validate that length of the feature and coefficient lists have the same length. Source code in bofire/domain/constraints.py @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\" if len ( values [ \"features\" ]) != len ( values [ \"coefficients\" ]): raise ValueError ( f 'must provide same number of features and coefficients, got { len ( values [ \"features\" ]) } != { len ( values [ \"coefficients\" ]) } ' ) return values LinearEqualityConstraint ( LinearConstraint ) pydantic-model Linear equality constraint of the form coefficients * x = rhs . Attributes: Name Type Description features list list of feature keys (str) on which the constraint works on. coefficients list list of coefficients (float) of the constraint. rhs float Right-hand side of the constraint Source code in bofire/domain/constraints.py class LinearEqualityConstraint ( LinearConstraint ): \"\"\"Linear equality constraint of the form `coefficients * x = rhs`. Attributes: features (list): list of feature keys (str) on which the constraint works on. coefficients (list): list of coefficients (float) of the constraint. rhs (float): Right-hand side of the constraint \"\"\" # def is_fulfilled(self, experiments: pd.DataFrame, complete: bool) -> bool: # \"\"\"Check if the linear equality constraint is fulfilled for all the rows of the provided dataframe. # Args: # df_data (pd.DataFrame): Dataframe to evaluate constraint on. # Returns: # bool: True if fulfilled else False. # \"\"\" # fulfilled = np.isclose(self(experiments), 0) # if complete: # return fulfilled.all() # else: # pd.Series(fulfilled, index=experiments.index) def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index ) def __str__ ( self ) -> str : \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return super () . __str__ () + f \" = { self . rhs } \" is_fulfilled ( self , experiments ) Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index ) LinearInequalityConstraint ( LinearConstraint ) pydantic-model Linear inequality constraint of the form coefficients * x <= rhs . To instantiate a constraint of the form coefficients * x >= rhs multiply coefficients and rhs by -1, or use the classmethod from_greater_equal . Attributes: Name Type Description features list list of feature keys (str) on which the constraint works on. coefficients list list of coefficients (float) of the constraint. rhs float Right-hand side of the constraint Source code in bofire/domain/constraints.py class LinearInequalityConstraint ( LinearConstraint ): \"\"\"Linear inequality constraint of the form `coefficients * x <= rhs`. To instantiate a constraint of the form `coefficients * x >= rhs` multiply coefficients and rhs by -1, or use the classmethod `from_greater_equal`. Attributes: features (list): list of feature keys (str) on which the constraint works on. coefficients (list): list of coefficients (float) of the constraint. rhs (float): Right-hand side of the constraint \"\"\" # def is_fulfilled(self, df_data: pd.DataFrame) -> bool: # \"\"\"Check if the linear inequality constraint is fulfilled in each row of the provided dataframe. # Args: # df_data (pd.DataFrame): Dataframe to evaluate constraint on. # Returns: # bool: True if fulfilled else False. # \"\"\" # noise = 10e-10 # return (self.lhs(df_data) >= self.rhs - noise).all() def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : # noise = 10e-10 discuss with Behrang return self ( experiments ) <= 0 def as_smaller_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the smaller equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , self . coefficients , self . rhs def as_greater_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the greater equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , [ - 1.0 * c for c in self . coefficients ], - 1.0 * self . rhs @classmethod def from_greater_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x >= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = [ - 1.0 * c for c in coefficients ], rhs =- 1.0 * rhs , ) @classmethod def from_smaller_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x <= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = coefficients , rhs = rhs , ) def __str__ ( self ): \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return super () . __str__ () + f \" <= { self . rhs } \" as_greater_equal ( self ) Return attributes in the greater equal convention Returns: Type Description Tuple[List[str], List[float], float] features, coefficients, rhs Source code in bofire/domain/constraints.py def as_greater_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the greater equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , [ - 1.0 * c for c in self . coefficients ], - 1.0 * self . rhs as_smaller_equal ( self ) Return attributes in the smaller equal convention Returns: Type Description Tuple[List[str], List[float], float] features, coefficients, rhs Source code in bofire/domain/constraints.py def as_smaller_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the smaller equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , self . coefficients , self . rhs from_greater_equal ( features , coefficients , rhs ) classmethod Class method to construct linear inequality constraint of the form coefficients * x >= rhs . Parameters: Name Type Description Default features List[str] List of feature keys. required coefficients List[float] List of coefficients. required rhs float Right-hand side of the constraint. required Source code in bofire/domain/constraints.py @classmethod def from_greater_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x >= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = [ - 1.0 * c for c in coefficients ], rhs =- 1.0 * rhs , ) from_smaller_equal ( features , coefficients , rhs ) classmethod Class method to construct linear inequality constraint of the form coefficients * x <= rhs . Parameters: Name Type Description Default features List[str] List of feature keys. required coefficients List[float] List of coefficients. required rhs float Right-hand side of the constraint. required Source code in bofire/domain/constraints.py @classmethod def from_smaller_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x <= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = coefficients , rhs = rhs , ) is_fulfilled ( self , experiments ) Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : # noise = 10e-10 discuss with Behrang return self ( experiments ) <= 0 NChooseKConstraint ( Constraint ) pydantic-model NChooseK constraint that defines how many ingredients are allowed in a formulation. Attributes: Name Type Description features List[str] List of feature keys to which the constraint applies. min_count int Minimal number of non-zero/active feature values. max_count int Maximum number of non-zero/active feature values. none_also_valid bool In case that min_count > 0, this flag decides if zero active features are also allowed. Source code in bofire/domain/constraints.py class NChooseKConstraint ( Constraint ): \"\"\"NChooseK constraint that defines how many ingredients are allowed in a formulation. Attributes: features (List[str]): List of feature keys to which the constraint applies. min_count (int): Minimal number of non-zero/active feature values. max_count (int): Maximum number of non-zero/active feature values. none_also_valid (bool): In case that min_count > 0, this flag decides if zero active features are also allowed. \"\"\" features : TFeatureKeys min_count : int max_count : int none_also_valid : bool @validator ( \"features\" ) def validate_features_unique ( cls , features : List [ str ]): \"\"\"Validates that provided feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features @root_validator ( pre = False ) def validate_counts ( cls , values ): \"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\" features = values [ \"features\" ] min_count = values [ \"min_count\" ] max_count = values [ \"max_count\" ] if min_count > len ( features ): raise ValueError ( \"min_count must be <= # of features\" ) if max_count > len ( features ): raise ValueError ( \"max_count must be <= # of features\" ) if min_count > max_count : raise ValueError ( \"min_values must be <= max_values\" ) return values def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : raise NotImplementedError def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe. Args: df_data (pd.DataFrame): Dataframe to evaluate constraint on. Returns: bool: True if fulfilled else False. \"\"\" cols = self . features sums = ( experiments [ cols ] > 0 ) . sum ( axis = 1 ) lower = sums >= self . min_count upper = sums <= self . max_count if not self . none_also_valid : # return lower.all() and upper.all() return pd . Series ( np . logical_and ( lower , upper ), index = experiments . index ) else : none = sums == 0 return pd . Series ( np . logical_or ( none , np . logical_and ( lower , upper )), index = experiments . index , ) def __str__ ( self ): \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" res = ( \"of the features \" + \", \" . join ( self . features ) + f \" between { self . min_count } and { self . max_count } must be used\" ) if self . none_also_valid : res += \" (none is also ok)\" return res __str__ ( self ) special Generate string representation of the constraint. Returns: Type Description str string representation of the constraint. Source code in bofire/domain/constraints.py def __str__ ( self ): \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" res = ( \"of the features \" + \", \" . join ( self . features ) + f \" between { self . min_count } and { self . max_count } must be used\" ) if self . none_also_valid : res += \" (none is also ok)\" return res is_fulfilled ( self , experiments ) Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default df_data pd.DataFrame Dataframe to evaluate constraint on. required Returns: Type Description bool True if fulfilled else False. Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe. Args: df_data (pd.DataFrame): Dataframe to evaluate constraint on. Returns: bool: True if fulfilled else False. \"\"\" cols = self . features sums = ( experiments [ cols ] > 0 ) . sum ( axis = 1 ) lower = sums >= self . min_count upper = sums <= self . max_count if not self . none_also_valid : # return lower.all() and upper.all() return pd . Series ( np . logical_and ( lower , upper ), index = experiments . index ) else : none = sums == 0 return pd . Series ( np . logical_or ( none , np . logical_and ( lower , upper )), index = experiments . index , ) validate_counts ( values ) classmethod Validates if the minimum and maximum of allowed features are smaller than the overall number of features. Source code in bofire/domain/constraints.py @root_validator ( pre = False ) def validate_counts ( cls , values ): \"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\" features = values [ \"features\" ] min_count = values [ \"min_count\" ] max_count = values [ \"max_count\" ] if min_count > len ( features ): raise ValueError ( \"min_count must be <= # of features\" ) if max_count > len ( features ): raise ValueError ( \"max_count must be <= # of features\" ) if min_count > max_count : raise ValueError ( \"min_values must be <= max_values\" ) return values validate_features_unique ( features ) classmethod Validates that provided feature keys are unique. Source code in bofire/domain/constraints.py @validator ( \"features\" ) def validate_features_unique ( cls , features : List [ str ]): \"\"\"Validates that provided feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features NonlinearEqualityConstraint ( NonlinearConstraint ) pydantic-model Source code in bofire/domain/constraints.py class NonlinearEqualityConstraint ( NonlinearConstraint ): def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index ) def __str__ ( self ): return f \" { self . expression } ==0\" __str__ ( self ) special Return str(self). Source code in bofire/domain/constraints.py def __str__ ( self ): return f \" { self . expression } ==0\" is_fulfilled ( self , experiments ) Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index ) NonlinearInqualityConstraint ( NonlinearConstraint ) pydantic-model Source code in bofire/domain/constraints.py class NonlinearInqualityConstraint ( NonlinearConstraint ): def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return self ( experiments ) <= 0 def __str__ ( self ): return f \" { self . expression } <=0\" __str__ ( self ) special Return str(self). Source code in bofire/domain/constraints.py def __str__ ( self ): return f \" { self . expression } <=0\" is_fulfilled ( self , experiments ) Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return self ( experiments ) <= 0","title":"Constraints"},{"location":"ref-constraints/#domain","text":"","title":"Domain"},{"location":"ref-constraints/#bofire.domain.constraints.Constraint","text":"Abstract base class to define constraints on the optimization space. Source code in bofire/domain/constraints.py class Constraint ( BaseModel ): \"\"\"Abstract base class to define constraints on the optimization space.\"\"\" @abstractmethod def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Args: experiments (pd.DataFrame): Dataframe to check constraint fulfillment. Returns: bool: True if fulfilled else False \"\"\" pass @abstractmethod def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluates the constraint. Args: experiments (pd.DataFrame): Dataframe to evaluate the constraint on. Returns: pd.Series: Distance to reach constraint fulfillment. \"\"\" pass def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the constraint. Returns: Dict: Serialized version of the constraint as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } @staticmethod def from_config ( config : Dict ) -> \"Constraint\" : \"\"\"Generate constraint out of serialized version. Args: config (Dict): Serialized version of a constraint Returns: Constraint: Instaniated constraint of the type specified in the `config`. \"\"\" mapper = { \"LinearEqualityConstraint\" : LinearEqualityConstraint , \"LinearInequalityConstraint\" : LinearInequalityConstraint , \"NChooseKConstraint\" : NChooseKConstraint , \"NonlinearEqualityConstraint\" : NonlinearEqualityConstraint , \"NonlinearInqualityConstraint\" : NonlinearInqualityConstraint , } return mapper [ config [ \"type\" ]]( ** config )","title":"Constraint"},{"location":"ref-constraints/#bofire.domain.constraints.Constraint.__call__","text":"Numerically evaluates the constraint. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to evaluate the constraint on. required Returns: Type Description pd.Series Distance to reach constraint fulfillment. Source code in bofire/domain/constraints.py @abstractmethod def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Numerically evaluates the constraint. Args: experiments (pd.DataFrame): Dataframe to evaluate the constraint on. Returns: pd.Series: Distance to reach constraint fulfillment. \"\"\" pass","title":"__call__()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraint.from_config","text":"Generate constraint out of serialized version. Parameters: Name Type Description Default config Dict Serialized version of a constraint required Returns: Type Description Constraint Instaniated constraint of the type specified in the config . Source code in bofire/domain/constraints.py @staticmethod def from_config ( config : Dict ) -> \"Constraint\" : \"\"\"Generate constraint out of serialized version. Args: config (Dict): Serialized version of a constraint Returns: Constraint: Instaniated constraint of the type specified in the `config`. \"\"\" mapper = { \"LinearEqualityConstraint\" : LinearEqualityConstraint , \"LinearInequalityConstraint\" : LinearInequalityConstraint , \"NChooseKConstraint\" : NChooseKConstraint , \"NonlinearEqualityConstraint\" : NonlinearEqualityConstraint , \"NonlinearInqualityConstraint\" : NonlinearInqualityConstraint , } return mapper [ config [ \"type\" ]]( ** config )","title":"from_config()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraint.is_fulfilled","text":"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py @abstractmethod def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Args: experiments (pd.DataFrame): Dataframe to check constraint fulfillment. Returns: bool: True if fulfilled else False \"\"\" pass","title":"is_fulfilled()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraint.to_config","text":"Generate serialized version of the constraint. Returns: Type Description Dict Serialized version of the constraint as dictionary. Source code in bofire/domain/constraints.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the constraint. Returns: Dict: Serialized version of the constraint as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), }","title":"to_config()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints","text":"Source code in bofire/domain/constraints.py class Constraints ( BaseModel ): constraints : Sequence [ Constraint ] = Field ( default_factory = lambda : []) def to_config ( self ) -> List : \"\"\"Serializes a `Constraints` object. Returns: List: Constraints objects as serialized list. \"\"\" return [ constraint . to_config () for constraint in self . constraints ] @classmethod def from_config ( cls , config : List ) -> \"Constraints\" : \"\"\"Instantiates a `Constraints` object based on the serialized list. Args: config (List): Serialized `Constraints` object as list. Returns: Constraints: Initialized `Constraints` object. \"\"\" return cls ( constraints = [ Constraint . from_config ( constraint ) for constraint in config ] ) def __iter__ ( self ): return iter ( self . constraints ) def __len__ ( self ): return len ( self . constraints ) def __getitem__ ( self , i ): return self . constraints [ i ] def __add__ ( self , other : Union [ Sequence [ Constraint ], \"Constraints\" ] ) -> \"Constraints\" : if isinstance ( other , collections . abc . Sequence ): other_constraints = other else : other_constraints = other . constraints constraints = list ( chain ( self . constraints , other_constraints )) return Constraints ( constraints = constraints ) def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints Args: experiments (pd.DataFrame): data to evaluate the constraint on Returns: pd.DataFrame: Constraint evaluation for each of the constraints \"\"\" return pd . concat ([ c ( experiments ) for c in self . constraints ], axis = 1 ) def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe Args: df_data (pd.DataFrame): Dataframe with data, the constraint validity should be tested on Returns: Boolean: True if all constraints are fulfilled for all rows, false if not \"\"\" if len ( self . constraints ) == 0 : return pd . Series ([ True ] * len ( experiments ), index = experiments . index ) return pd . concat ( [ c . is_fulfilled ( experiments ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 ) def get ( self , includes : Union [ Type , List [ Type ]] = Constraint , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> \"Constraints\" : \"\"\"get constraints of the domain Args: includes (Union[Constraint, List[Constraint]], optional): Constraint class or list of specific constraint classes to be returned. Defaults to Constraint. excludes (Union[Type, List[Type]], optional): Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[Constraint]: List of constraints in the domain fitting to the passed requirements. \"\"\" return Constraints ( constraints = filter_by_class ( self . constraints , includes = includes , excludes = excludes , exact = exact , ) )","title":"Constraints"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.__call__","text":"Numerically evaluate all constraints Parameters: Name Type Description Default experiments pd.DataFrame data to evaluate the constraint on required Returns: Type Description pd.DataFrame Constraint evaluation for each of the constraints Source code in bofire/domain/constraints.py def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Numerically evaluate all constraints Args: experiments (pd.DataFrame): data to evaluate the constraint on Returns: pd.DataFrame: Constraint evaluation for each of the constraints \"\"\" return pd . concat ([ c ( experiments ) for c in self . constraints ], axis = 1 )","title":"__call__()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.from_config","text":"Instantiates a Constraints object based on the serialized list. Parameters: Name Type Description Default config List Serialized Constraints object as list. required Returns: Type Description Constraints Initialized Constraints object. Source code in bofire/domain/constraints.py @classmethod def from_config ( cls , config : List ) -> \"Constraints\" : \"\"\"Instantiates a `Constraints` object based on the serialized list. Args: config (List): Serialized `Constraints` object as list. Returns: Constraints: Initialized `Constraints` object. \"\"\" return cls ( constraints = [ Constraint . from_config ( constraint ) for constraint in config ] )","title":"from_config()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.get","text":"get constraints of the domain Parameters: Name Type Description Default includes Union[Constraint, List[Constraint]] Constraint class or list of specific constraint classes to be returned. Defaults to Constraint. <class 'bofire.domain.constraints.Constraint'> excludes Union[Type, List[Type]] Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[Constraint] List of constraints in the domain fitting to the passed requirements. Source code in bofire/domain/constraints.py def get ( self , includes : Union [ Type , List [ Type ]] = Constraint , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> \"Constraints\" : \"\"\"get constraints of the domain Args: includes (Union[Constraint, List[Constraint]], optional): Constraint class or list of specific constraint classes to be returned. Defaults to Constraint. excludes (Union[Type, List[Type]], optional): Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[Constraint]: List of constraints in the domain fitting to the passed requirements. \"\"\" return Constraints ( constraints = filter_by_class ( self . constraints , includes = includes , excludes = excludes , exact = exact , ) )","title":"get()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.is_fulfilled","text":"Check if all constraints are fulfilled on all rows of the provided dataframe Parameters: Name Type Description Default df_data pd.DataFrame Dataframe with data, the constraint validity should be tested on required Returns: Type Description Boolean True if all constraints are fulfilled for all rows, false if not Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe Args: df_data (pd.DataFrame): Dataframe with data, the constraint validity should be tested on Returns: Boolean: True if all constraints are fulfilled for all rows, false if not \"\"\" if len ( self . constraints ) == 0 : return pd . Series ([ True ] * len ( experiments ), index = experiments . index ) return pd . concat ( [ c . is_fulfilled ( experiments ) for c in self . constraints ], axis = 1 ) . all ( axis = 1 )","title":"is_fulfilled()"},{"location":"ref-constraints/#bofire.domain.constraints.Constraints.to_config","text":"Serializes a Constraints object. Returns: Type Description List Constraints objects as serialized list. Source code in bofire/domain/constraints.py def to_config ( self ) -> List : \"\"\"Serializes a `Constraints` object. Returns: List: Constraints objects as serialized list. \"\"\" return [ constraint . to_config () for constraint in self . constraints ]","title":"to_config()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint","text":"Abstract base class for linear equality and inequality constraints. Attributes: Name Type Description features list list of feature keys (str) on which the constraint works on. coefficients list list of coefficients (float) of the constraint. rhs float Right-hand side of the constraint Source code in bofire/domain/constraints.py class LinearConstraint ( Constraint ): \"\"\"Abstract base class for linear equality and inequality constraints. Attributes: features (list): list of feature keys (str) on which the constraint works on. coefficients (list): list of coefficients (float) of the constraint. rhs (float): Right-hand side of the constraint \"\"\" features : TFeatureKeys coefficients : TCoefficients rhs : float @validator ( \"features\" ) def validate_features_unique ( cls , features ): \"\"\"Validate that feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\" if len ( values [ \"features\" ]) != len ( values [ \"coefficients\" ]): raise ValueError ( f 'must provide same number of features and coefficients, got { len ( values [ \"features\" ]) } != { len ( values [ \"coefficients\" ]) } ' ) return values def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : return ( experiments [ self . features ] @ self . coefficients - self . rhs ) / np . linalg . norm ( self . coefficients ) # def lhs(self, df_data: pd.DataFrame) -> float: # \"\"\"Evaluate the left-hand side of the constraint on each row of a dataframe # Args: # df_data (pd.DataFrame): Dataframe on which the left-hand side should be evaluated. # Returns: # np.array: 1-dim array with left-hand side of each row of the provided dataframe. # \"\"\" # cols = self.features # coefficients = self.coefficients # return np.sum(df_data[cols].values * np.array(coefficients), axis=1) def __str__ ( self ) -> str : \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return \" + \" . join ( [ f \" { self . coefficients [ i ] } * { feat } \" for i , feat in enumerate ( self . features )] )","title":"LinearConstraint"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint.__str__","text":"Generate string representation of the constraint. Returns: Type Description str string representation of the constraint. Source code in bofire/domain/constraints.py def __str__ ( self ) -> str : \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return \" + \" . join ( [ f \" { self . coefficients [ i ] } * { feat } \" for i , feat in enumerate ( self . features )] )","title":"__str__()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint.validate_features_unique","text":"Validate that feature keys are unique. Source code in bofire/domain/constraints.py @validator ( \"features\" ) def validate_features_unique ( cls , features ): \"\"\"Validate that feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features","title":"validate_features_unique()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearConstraint.validate_list_lengths","text":"Validate that length of the feature and coefficient lists have the same length. Source code in bofire/domain/constraints.py @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\" if len ( values [ \"features\" ]) != len ( values [ \"coefficients\" ]): raise ValueError ( f 'must provide same number of features and coefficients, got { len ( values [ \"features\" ]) } != { len ( values [ \"coefficients\" ]) } ' ) return values","title":"validate_list_lengths()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearEqualityConstraint","text":"Linear equality constraint of the form coefficients * x = rhs . Attributes: Name Type Description features list list of feature keys (str) on which the constraint works on. coefficients list list of coefficients (float) of the constraint. rhs float Right-hand side of the constraint Source code in bofire/domain/constraints.py class LinearEqualityConstraint ( LinearConstraint ): \"\"\"Linear equality constraint of the form `coefficients * x = rhs`. Attributes: features (list): list of feature keys (str) on which the constraint works on. coefficients (list): list of coefficients (float) of the constraint. rhs (float): Right-hand side of the constraint \"\"\" # def is_fulfilled(self, experiments: pd.DataFrame, complete: bool) -> bool: # \"\"\"Check if the linear equality constraint is fulfilled for all the rows of the provided dataframe. # Args: # df_data (pd.DataFrame): Dataframe to evaluate constraint on. # Returns: # bool: True if fulfilled else False. # \"\"\" # fulfilled = np.isclose(self(experiments), 0) # if complete: # return fulfilled.all() # else: # pd.Series(fulfilled, index=experiments.index) def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index ) def __str__ ( self ) -> str : \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return super () . __str__ () + f \" = { self . rhs } \"","title":"LinearEqualityConstraint"},{"location":"ref-constraints/#bofire.domain.constraints.LinearEqualityConstraint.is_fulfilled","text":"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index )","title":"is_fulfilled()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint","text":"Linear inequality constraint of the form coefficients * x <= rhs . To instantiate a constraint of the form coefficients * x >= rhs multiply coefficients and rhs by -1, or use the classmethod from_greater_equal . Attributes: Name Type Description features list list of feature keys (str) on which the constraint works on. coefficients list list of coefficients (float) of the constraint. rhs float Right-hand side of the constraint Source code in bofire/domain/constraints.py class LinearInequalityConstraint ( LinearConstraint ): \"\"\"Linear inequality constraint of the form `coefficients * x <= rhs`. To instantiate a constraint of the form `coefficients * x >= rhs` multiply coefficients and rhs by -1, or use the classmethod `from_greater_equal`. Attributes: features (list): list of feature keys (str) on which the constraint works on. coefficients (list): list of coefficients (float) of the constraint. rhs (float): Right-hand side of the constraint \"\"\" # def is_fulfilled(self, df_data: pd.DataFrame) -> bool: # \"\"\"Check if the linear inequality constraint is fulfilled in each row of the provided dataframe. # Args: # df_data (pd.DataFrame): Dataframe to evaluate constraint on. # Returns: # bool: True if fulfilled else False. # \"\"\" # noise = 10e-10 # return (self.lhs(df_data) >= self.rhs - noise).all() def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : # noise = 10e-10 discuss with Behrang return self ( experiments ) <= 0 def as_smaller_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the smaller equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , self . coefficients , self . rhs def as_greater_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the greater equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , [ - 1.0 * c for c in self . coefficients ], - 1.0 * self . rhs @classmethod def from_greater_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x >= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = [ - 1.0 * c for c in coefficients ], rhs =- 1.0 * rhs , ) @classmethod def from_smaller_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x <= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = coefficients , rhs = rhs , ) def __str__ ( self ): \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" return super () . __str__ () + f \" <= { self . rhs } \"","title":"LinearInequalityConstraint"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.as_greater_equal","text":"Return attributes in the greater equal convention Returns: Type Description Tuple[List[str], List[float], float] features, coefficients, rhs Source code in bofire/domain/constraints.py def as_greater_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the greater equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , [ - 1.0 * c for c in self . coefficients ], - 1.0 * self . rhs","title":"as_greater_equal()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.as_smaller_equal","text":"Return attributes in the smaller equal convention Returns: Type Description Tuple[List[str], List[float], float] features, coefficients, rhs Source code in bofire/domain/constraints.py def as_smaller_equal ( self ) -> Tuple [ List [ str ], List [ float ], float ]: \"\"\"Return attributes in the smaller equal convention Returns: Tuple[List[str], List[float], float]: features, coefficients, rhs \"\"\" return self . features , self . coefficients , self . rhs","title":"as_smaller_equal()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.from_greater_equal","text":"Class method to construct linear inequality constraint of the form coefficients * x >= rhs . Parameters: Name Type Description Default features List[str] List of feature keys. required coefficients List[float] List of coefficients. required rhs float Right-hand side of the constraint. required Source code in bofire/domain/constraints.py @classmethod def from_greater_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x >= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = [ - 1.0 * c for c in coefficients ], rhs =- 1.0 * rhs , )","title":"from_greater_equal()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.from_smaller_equal","text":"Class method to construct linear inequality constraint of the form coefficients * x <= rhs . Parameters: Name Type Description Default features List[str] List of feature keys. required coefficients List[float] List of coefficients. required rhs float Right-hand side of the constraint. required Source code in bofire/domain/constraints.py @classmethod def from_smaller_equal ( cls , features : List [ str ], coefficients : List [ float ], rhs : float ): \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x <= rhs`. Args: features (List[str]): List of feature keys. coefficients (List[float]): List of coefficients. rhs (float): Right-hand side of the constraint. \"\"\" return cls ( features = features , coefficients = coefficients , rhs = rhs , )","title":"from_smaller_equal()"},{"location":"ref-constraints/#bofire.domain.constraints.LinearInequalityConstraint.is_fulfilled","text":"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : # noise = 10e-10 discuss with Behrang return self ( experiments ) <= 0","title":"is_fulfilled()"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint","text":"NChooseK constraint that defines how many ingredients are allowed in a formulation. Attributes: Name Type Description features List[str] List of feature keys to which the constraint applies. min_count int Minimal number of non-zero/active feature values. max_count int Maximum number of non-zero/active feature values. none_also_valid bool In case that min_count > 0, this flag decides if zero active features are also allowed. Source code in bofire/domain/constraints.py class NChooseKConstraint ( Constraint ): \"\"\"NChooseK constraint that defines how many ingredients are allowed in a formulation. Attributes: features (List[str]): List of feature keys to which the constraint applies. min_count (int): Minimal number of non-zero/active feature values. max_count (int): Maximum number of non-zero/active feature values. none_also_valid (bool): In case that min_count > 0, this flag decides if zero active features are also allowed. \"\"\" features : TFeatureKeys min_count : int max_count : int none_also_valid : bool @validator ( \"features\" ) def validate_features_unique ( cls , features : List [ str ]): \"\"\"Validates that provided feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features @root_validator ( pre = False ) def validate_counts ( cls , values ): \"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\" features = values [ \"features\" ] min_count = values [ \"min_count\" ] max_count = values [ \"max_count\" ] if min_count > len ( features ): raise ValueError ( \"min_count must be <= # of features\" ) if max_count > len ( features ): raise ValueError ( \"max_count must be <= # of features\" ) if min_count > max_count : raise ValueError ( \"min_values must be <= max_values\" ) return values def __call__ ( self , experiments : pd . DataFrame ) -> pd . Series : raise NotImplementedError def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe. Args: df_data (pd.DataFrame): Dataframe to evaluate constraint on. Returns: bool: True if fulfilled else False. \"\"\" cols = self . features sums = ( experiments [ cols ] > 0 ) . sum ( axis = 1 ) lower = sums >= self . min_count upper = sums <= self . max_count if not self . none_also_valid : # return lower.all() and upper.all() return pd . Series ( np . logical_and ( lower , upper ), index = experiments . index ) else : none = sums == 0 return pd . Series ( np . logical_or ( none , np . logical_and ( lower , upper )), index = experiments . index , ) def __str__ ( self ): \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" res = ( \"of the features \" + \", \" . join ( self . features ) + f \" between { self . min_count } and { self . max_count } must be used\" ) if self . none_also_valid : res += \" (none is also ok)\" return res","title":"NChooseKConstraint"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.__str__","text":"Generate string representation of the constraint. Returns: Type Description str string representation of the constraint. Source code in bofire/domain/constraints.py def __str__ ( self ): \"\"\"Generate string representation of the constraint. Returns: str: string representation of the constraint. \"\"\" res = ( \"of the features \" + \", \" . join ( self . features ) + f \" between { self . min_count } and { self . max_count } must be used\" ) if self . none_also_valid : res += \" (none is also ok)\" return res","title":"__str__()"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.is_fulfilled","text":"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default df_data pd.DataFrame Dataframe to evaluate constraint on. required Returns: Type Description bool True if fulfilled else False. Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : \"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe. Args: df_data (pd.DataFrame): Dataframe to evaluate constraint on. Returns: bool: True if fulfilled else False. \"\"\" cols = self . features sums = ( experiments [ cols ] > 0 ) . sum ( axis = 1 ) lower = sums >= self . min_count upper = sums <= self . max_count if not self . none_also_valid : # return lower.all() and upper.all() return pd . Series ( np . logical_and ( lower , upper ), index = experiments . index ) else : none = sums == 0 return pd . Series ( np . logical_or ( none , np . logical_and ( lower , upper )), index = experiments . index , )","title":"is_fulfilled()"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.validate_counts","text":"Validates if the minimum and maximum of allowed features are smaller than the overall number of features. Source code in bofire/domain/constraints.py @root_validator ( pre = False ) def validate_counts ( cls , values ): \"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\" features = values [ \"features\" ] min_count = values [ \"min_count\" ] max_count = values [ \"max_count\" ] if min_count > len ( features ): raise ValueError ( \"min_count must be <= # of features\" ) if max_count > len ( features ): raise ValueError ( \"max_count must be <= # of features\" ) if min_count > max_count : raise ValueError ( \"min_values must be <= max_values\" ) return values","title":"validate_counts()"},{"location":"ref-constraints/#bofire.domain.constraints.NChooseKConstraint.validate_features_unique","text":"Validates that provided feature keys are unique. Source code in bofire/domain/constraints.py @validator ( \"features\" ) def validate_features_unique ( cls , features : List [ str ]): \"\"\"Validates that provided feature keys are unique.\"\"\" if len ( features ) != len ( set ( features )): raise ValueError ( \"features must be unique\" ) return features","title":"validate_features_unique()"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearEqualityConstraint","text":"Source code in bofire/domain/constraints.py class NonlinearEqualityConstraint ( NonlinearConstraint ): def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index ) def __str__ ( self ): return f \" { self . expression } ==0\"","title":"NonlinearEqualityConstraint"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearEqualityConstraint.__str__","text":"Return str(self). Source code in bofire/domain/constraints.py def __str__ ( self ): return f \" { self . expression } ==0\"","title":"__str__()"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearEqualityConstraint.is_fulfilled","text":"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return pd . Series ( np . isclose ( self ( experiments ), 0 ), index = experiments . index )","title":"is_fulfilled()"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearInqualityConstraint","text":"Source code in bofire/domain/constraints.py class NonlinearInqualityConstraint ( NonlinearConstraint ): def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return self ( experiments ) <= 0 def __str__ ( self ): return f \" { self . expression } <=0\"","title":"NonlinearInqualityConstraint"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearInqualityConstraint.__str__","text":"Return str(self). Source code in bofire/domain/constraints.py def __str__ ( self ): return f \" { self . expression } <=0\"","title":"__str__()"},{"location":"ref-constraints/#bofire.domain.constraints.NonlinearInqualityConstraint.is_fulfilled","text":"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe to check constraint fulfillment. required Returns: Type Description bool True if fulfilled else False Source code in bofire/domain/constraints.py def is_fulfilled ( self , experiments : pd . DataFrame ) -> pd . Series : return self ( experiments ) <= 0","title":"is_fulfilled()"},{"location":"ref-domain-util/","text":"Domain filter_by_attribute ( data , attribute_getter , includes = None , excludes = None , exact = False ) Returns those data elements where the attribute is of one of the include types. Parameters: Name Type Description Default data Sequence to be filtered required attribute_getter Callable[[Type], Any] expects an item of the data list and returns the attribute to filter by required includes Union[Type, Sequence[Type]] attribute types that should be kept, sub-type are included by default, see exact None excludes Union[Type, Sequence[Type]] attribute types that will be excluded even if they are sub-types of or include types. None exact bool true for not including subtypes False Returns: Type Description List list of data point with attributes as filtered for Source code in bofire/domain/util.py def filter_by_attribute ( data : Sequence , attribute_getter : Callable [[ Type ], Any ], includes : Union [ Type , Sequence [ Type ]] = None , excludes : Union [ Type , Sequence [ Type ]] = None , exact : bool = False , ) -> List : \"\"\"Returns those data elements where the attribute is of one of the include types. Args: data: to be filtered attribute_getter: expects an item of the data list and returns the attribute to filter by includes: attribute types that should be kept, sub-type are included by default, see exact excludes: attribute types that will be excluded even if they are sub-types of or include types. exact: true for not including subtypes Returns: list of data point with attributes as filtered for \"\"\" data_with_attr = [] for d in data : try : attribute_getter ( d ) data_with_attr . append ( d ) except AttributeError : pass filtered = filter_by_class ( data_with_attr , includes = includes , excludes = excludes , exact = exact , key = attribute_getter , ) return filtered filter_by_class ( data , includes = None , excludes = None , exact = False , key =< function < lambda > at 0x7f66febbd3a0 > ) Returns those data elements where are one of the include types. Parameters: Name Type Description Default data Sequence to be filtered required includes Union[Type, Sequence[Type]] attribute types that should be kept, sub-type are included by default, see exact None excludes Union[Type, Sequence[Type]] attribute types that will be excluded even if they are sub-types of or include types. None exact bool true for not including subtypes False key Callable[[Type], Any] maps a data list item to something that is used for filtering, identity by default <function <lambda> at 0x7f66febbd3a0> Returns: Type Description List filtered list of data points Source code in bofire/domain/util.py def filter_by_class ( data : Sequence , includes : Union [ Type , Sequence [ Type ]] = None , excludes : Union [ Type , Sequence [ Type ]] = None , exact : bool = False , key : Callable [[ Type ], Any ] = lambda x : x , ) -> List : \"\"\"Returns those data elements where are one of the include types. Args: data: to be filtered includes: attribute types that should be kept, sub-type are included by default, see exact excludes: attribute types that will be excluded even if they are sub-types of or include types. exact: true for not including subtypes key: maps a data list item to something that is used for filtering, identity by default Returns: filtered list of data points \"\"\" if includes is None : includes = [] if not isinstance ( includes , collections . Sequence ): includes = [ includes ] if excludes is None : excludes = [] if not isinstance ( excludes , collections . Sequence ): excludes = [ excludes ] if len ( includes ) == len ( excludes ) == 0 : raise ValueError ( \"no filter provided\" ) if len ( includes ) == 0 : includes = [ object ] if len ([ x for x in includes if x in excludes ]) > 0 : raise ValueError ( \"includes and excludes overlap\" ) if exact : return [ d for d in data if type ( key ( d )) in includes and type ( key ( d )) not in excludes ] return [ d for d in data if isinstance ( key ( d ), tuple ( includes )) and not isinstance ( key ( d ), tuple ( excludes )) ]","title":"Domain Util"},{"location":"ref-domain-util/#domain","text":"","title":"Domain"},{"location":"ref-domain-util/#bofire.domain.util.filter_by_attribute","text":"Returns those data elements where the attribute is of one of the include types. Parameters: Name Type Description Default data Sequence to be filtered required attribute_getter Callable[[Type], Any] expects an item of the data list and returns the attribute to filter by required includes Union[Type, Sequence[Type]] attribute types that should be kept, sub-type are included by default, see exact None excludes Union[Type, Sequence[Type]] attribute types that will be excluded even if they are sub-types of or include types. None exact bool true for not including subtypes False Returns: Type Description List list of data point with attributes as filtered for Source code in bofire/domain/util.py def filter_by_attribute ( data : Sequence , attribute_getter : Callable [[ Type ], Any ], includes : Union [ Type , Sequence [ Type ]] = None , excludes : Union [ Type , Sequence [ Type ]] = None , exact : bool = False , ) -> List : \"\"\"Returns those data elements where the attribute is of one of the include types. Args: data: to be filtered attribute_getter: expects an item of the data list and returns the attribute to filter by includes: attribute types that should be kept, sub-type are included by default, see exact excludes: attribute types that will be excluded even if they are sub-types of or include types. exact: true for not including subtypes Returns: list of data point with attributes as filtered for \"\"\" data_with_attr = [] for d in data : try : attribute_getter ( d ) data_with_attr . append ( d ) except AttributeError : pass filtered = filter_by_class ( data_with_attr , includes = includes , excludes = excludes , exact = exact , key = attribute_getter , ) return filtered","title":"filter_by_attribute()"},{"location":"ref-domain-util/#bofire.domain.util.filter_by_class","text":"Returns those data elements where are one of the include types. Parameters: Name Type Description Default data Sequence to be filtered required includes Union[Type, Sequence[Type]] attribute types that should be kept, sub-type are included by default, see exact None excludes Union[Type, Sequence[Type]] attribute types that will be excluded even if they are sub-types of or include types. None exact bool true for not including subtypes False key Callable[[Type], Any] maps a data list item to something that is used for filtering, identity by default <function <lambda> at 0x7f66febbd3a0> Returns: Type Description List filtered list of data points Source code in bofire/domain/util.py def filter_by_class ( data : Sequence , includes : Union [ Type , Sequence [ Type ]] = None , excludes : Union [ Type , Sequence [ Type ]] = None , exact : bool = False , key : Callable [[ Type ], Any ] = lambda x : x , ) -> List : \"\"\"Returns those data elements where are one of the include types. Args: data: to be filtered includes: attribute types that should be kept, sub-type are included by default, see exact excludes: attribute types that will be excluded even if they are sub-types of or include types. exact: true for not including subtypes key: maps a data list item to something that is used for filtering, identity by default Returns: filtered list of data points \"\"\" if includes is None : includes = [] if not isinstance ( includes , collections . Sequence ): includes = [ includes ] if excludes is None : excludes = [] if not isinstance ( excludes , collections . Sequence ): excludes = [ excludes ] if len ( includes ) == len ( excludes ) == 0 : raise ValueError ( \"no filter provided\" ) if len ( includes ) == 0 : includes = [ object ] if len ([ x for x in includes if x in excludes ]) > 0 : raise ValueError ( \"includes and excludes overlap\" ) if exact : return [ d for d in data if type ( key ( d )) in includes and type ( key ( d )) not in excludes ] return [ d for d in data if isinstance ( key ( d ), tuple ( includes )) and not isinstance ( key ( d ), tuple ( excludes )) ]","title":"filter_by_class()"},{"location":"ref-domain/","text":"Domain Domain ( BaseModel ) pydantic-model Source code in bofire/domain/domain.py class Domain ( BaseModel ): # The types describe what we expect to be passed as arguments. # They will be converted to InputFeatures and OutputFeatures, respectively. input_features : Union [ Sequence [ InputFeature ], InputFeatures ] = Field ( default_factory = lambda : InputFeatures () ) output_features : Union [ Sequence [ OutputFeature ], OutputFeatures ] = Field ( default_factory = lambda : OutputFeatures () ) constraints : Union [ Sequence [ Constraint ], Constraints ] = Field ( default_factory = lambda : Constraints () ) experiments : Optional [ pd . DataFrame ] = None candidates : Optional [ pd . DataFrame ] = None \"\"\"Representation of the optimization problem/domain Attributes: input_features (List[InputFeature], optional): List of input features. Defaults to []. output_features (List[OutputFeature], optional): List of output features. Defaults to []. constraints (List[Constraint], optional): List of constraints. Defaults to []. \"\"\" @property def outputs ( self ) -> OutputFeatures : \"\"\"Returns output features as OutputFeatures\"\"\" return cast ( OutputFeatures , self . output_features ) @property def inputs ( self ) -> InputFeatures : \"\"\"Returns input features as InputFeatures\"\"\" return cast ( InputFeatures , self . input_features ) @property def cnstrs ( self ) -> Constraints : return cast ( Constraints , self . constraints ) @validator ( \"input_features\" , always = True , pre = True ) def validate_input_features_list ( cls , v , values ): if isinstance ( v , collections . abc . Sequence ): v = InputFeatures ( features = v ) return v if isinstance ( v , InputFeature ): return InputFeatures ( features = [ v ]) else : return v @validator ( \"output_features\" , always = True , pre = True ) def validate_output_features_list ( cls , v , values ): if isinstance ( v , collections . abc . Sequence ): return OutputFeatures ( features = v ) if isinstance ( v , OutputFeature ): return OutputFeatures ( features = [ v ]) else : return v @validator ( \"constraints\" , always = True , pre = True ) def validate_constraints_list ( cls , v , values ): if isinstance ( v , list ): return Constraints ( constraints = v ) if isinstance ( v , Constraint ): return Constraints ( constraints = [ v ]) else : return v @validator ( \"output_features\" , always = True ) def validate_unique_feature_keys ( cls , v : OutputFeatures , values ) -> OutputFeatures : \"\"\"Validates if provided input and output feature keys are unique Args: v (OutputFeatures): List of all output features of the domain. value (Dict[str, InputFeatures]): Dict containing a list of input features as single entry. Raises: ValueError: Feature keys are not unique. Returns: OutputFeatures: Keeps output features as given. \"\"\" if \"input_features\" not in values : return v features = v + values [ \"input_features\" ] keys = [ f . key for f in features ] if len ( set ( keys )) != len ( keys ): raise ValueError ( \"feature keys are not unique\" ) return v @validator ( \"constraints\" , always = True ) def validate_constraints ( cls , v , values ): \"\"\"Validate if all features included in the constraints are also defined as features for the domain. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: Feature key in constraint is unknown. Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v keys = [ f . key for f in values [ \"input_features\" ]] for c in v : if isinstance ( c , LinearConstraint ) or isinstance ( c , NChooseKConstraint ): for f in c . features : if f not in keys : raise ValueError ( f \"feature { f } in constraint unknown ( { keys } )\" ) return v @validator ( \"constraints\" , always = True ) def validate_linear_constraints ( cls , v , values ): \"\"\"Validate if all features included in linear constraints are continuous ones. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if non continuous input features appear in linear constraints for c in v : if isinstance ( c , LinearConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" return v @validator ( \"constraints\" , always = True ) def validate_lower_bounds_in_nchoosek_constraints ( cls , v , values ): \"\"\"Validate the lower bound as well if the chosen number of allowed features is continuous. Args: v (List[Constraint]): List of all constraints defined for the domain values (List[InputFeature]): _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if unfixed continuous features appearing in NChooseK constraints have lower bound of 0 for c in v : if isinstance ( c , NChooseKConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" assert ( continuous_input_features_dict [ f ] . lower_bound == 0 ), f \"lower bound of { f } must be 0 for NChooseK constraint.\" return v def to_config ( self ) -> Dict : \"\"\"Serializables itself to a dictionary. Returns: Dict: Serialized version of the domain as dictionary. \"\"\" assert isinstance ( self . input_features , InputFeatures ) assert isinstance ( self . output_features , OutputFeatures ) assert isinstance ( self . constraints , Constraints ) config : Dict [ str , Any ] = { \"input_features\" : self . input_features . to_config (), \"output_features\" : self . output_features . to_config (), \"constraints\" : self . constraints . to_config (), } if self . experiments is not None and self . num_experiments > 0 : config [ \"experiments\" ] = self . experiments . to_dict () if self . candidates is not None and self . num_candidates > 0 : config [ \"candidates\" ] = self . candidates . to_dict () return config @classmethod def from_config ( cls , config : Dict ): \"\"\"Instantiates a `Domain` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized version of a domain as dictionary. \"\"\" d = cls ( input_features = typing . cast ( InputFeatures , InputFeatures . from_config ( config [ \"input_features\" ]) ), output_features = typing . cast ( OutputFeatures , OutputFeatures . from_config ( config [ \"output_features\" ]) ), constraints = Constraints . from_config ( config [ \"constraints\" ]), ) if \"experiments\" in config . keys (): d . set_experiments ( experiments = config [ \"experiments\" ]) if \"candidates\" in config . keys (): d . set_candidates ( candidates = config [ \"candidates\" ]) return d def get_feature_reps_df ( self ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe describing the features contained in the optimization domain.\"\"\" df = pd . DataFrame ( index = self . get_feature_keys ( Feature ), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . get_features ( Feature ) ], \"Description\" : [ feat . __str__ () for feat in self . get_features ( Feature )], }, ) return df def get_constraint_reps_df ( self ): \"\"\"Provides a tabular overwiev of all constraints within the domain Returns: pd.DataFrame: DataFrame listing all constraints of the domain with a description \"\"\" df = pd . DataFrame ( index = range ( len ( self . constraints )), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . constraints ], \"Description\" : [ constraint . __str__ () for constraint in self . constraints ], }, ) return df def get_features ( self , includes : Union [ Type [ Feature ], List [ Type [ Feature ]]] = Feature , excludes : Union [ Type [ Feature ], List [ Type [ Feature ]], None ] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" assert isinstance ( self . input_features , InputFeatures ) features = self . input_features + self . output_features return features . get ( includes , excludes , exact ) def get_feature_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_features ( includes = includes , excludes = excludes , exact = exact , ) ] def get_feature ( self , key : str ): \"\"\"get a specific feature by its key Args: key (str): Feature key Returns: Feature: The feature with the passed key \"\"\" assert isinstance ( self . input_features , InputFeatures ) return { f . key : f for f in self . input_features + self . output_features }[ key ] # getting list of fixed values def get_nchoosek_combinations ( self ): \"\"\"get all possible NChooseK combinations Returns: Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination. \"\"\" if len ( self . cnstrs . get ( NChooseKConstraint )) == 0 : used_continuous_features = self . get_feature_keys ( ContinuousInput ) return used_continuous_features , [] used_features_list_all = [] # loops through each NChooseK constraint for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) used_features_list = [] for n in range ( con . min_count , con . max_count + 1 ): used_features_list . extend ( itertools . combinations ( con . features , n )) if con . none_also_valid : used_features_list . append ( tuple ([])) used_features_list_all . append ( used_features_list ) used_features_list_all = list ( itertools . product ( * used_features_list_all ) ) # product between NChooseK constraints # format into a list of used features used_features_list_formatted = [] for used_features_list in used_features_list_all : used_features_list_flattened = [ item for sublist in used_features_list for item in sublist ] used_features_list_formatted . append ( list ( set ( used_features_list_flattened ))) # sort lists used_features_list_sorted = [] for used_features in used_features_list_formatted : used_features_list_sorted . append ( sorted ( used_features )) # drop duplicates used_features_list_no_dup = [] for used_features in used_features_list_sorted : if used_features not in used_features_list_no_dup : used_features_list_no_dup . append ( used_features ) # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\") # remove combinations not fulfilling constraints used_features_list_final = [] for combo in used_features_list_no_dup : fulfil_constraints = ( [] ) # list of bools tracking if constraints are fulfilled for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) count = 0 # count of features in combo that are in con.features for f in combo : if f in con . features : count += 1 if count >= con . min_count and count <= con . max_count : fulfil_constraints . append ( True ) elif count == 0 and con . none_also_valid : fulfil_constraints . append ( True ) else : fulfil_constraints . append ( False ) if np . all ( fulfil_constraints ): used_features_list_final . append ( combo ) # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\") # features unused features_in_cc = [] for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) features_in_cc . extend ( con . features ) features_in_cc = list ( set ( features_in_cc )) features_in_cc . sort () unused_features_list = [] for used_features in used_features_list_final : unused_features_list . append ( [ f_key for f_key in features_in_cc if f_key not in used_features ] ) # postprocess # used_features_list_final2 = [] # unused_features_list2 = [] # for used, unused in zip(used_features_list_final,unused_features_list): # if len(used) == 3: # used_features_list_final2.append(used), unused_features_list2.append(unused) return used_features_list_final , unused_features_list def preprocess_experiments_one_valid_output ( self , output_feature_key : str , experiments : Optional [ pd . DataFrame ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_key (str): The feature based on which non-valid entries rows are removed Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) assert experiments is not None clean_exp = experiments . loc [ ( experiments [ \"valid_ %s \" % output_feature_key ] == 1 ) & ( experiments [ output_feature_key ] . notna ()) ] # clean_exp = clean_exp.dropna() return clean_exp def preprocess_experiments_all_valid_outputs ( self , experiments : Optional [ pd . DataFrame ] = None , output_feature_keys : Optional [ List ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of all output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None. Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) if ( output_feature_keys is None ) or ( len ( output_feature_keys ) == 0 ): output_feature_keys = self . get_feature_keys ( OutputFeature ) else : for key in output_feature_keys : feat = self . get_feature ( key ) assert isinstance ( feat , OutputFeature ), f \"feat { key } is not an OutputFeature\" assert experiments is not None clean_exp = experiments . query ( \" & \" . join ([ \"(`valid_ %s ` > 0)\" % key for key in output_feature_keys ]) ) clean_exp = clean_exp . dropna ( subset = output_feature_keys ) return clean_exp def preprocess_experiments_any_valid_output ( self , experiments : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\"Method to get a dataframe where at least one output feature has a valid entry Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) output_feature_keys = self . get_feature_keys ( OutputFeature ) # clean_exp = experiments.query(\" or \".join([\"(valid_%s > 0)\" % key for key in output_feature_keys])) # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys])) assert experiments is not None clean_exp = experiments . query ( \" or \" . join ( [ \"((`valid_ %s ` >0) & ` %s `.notna())\" % ( key , key ) for key in output_feature_keys ] ) ) return clean_exp def coerce_invalids ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Coerces all invalid output measurements to np.nan Args: experiments (pd.DataFrame): Dataframe containing experimental data Returns: pd.DataFrame: coerced dataframe \"\"\" # coerce invalid to nan for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ f \"valid_ { feat } \" ] == 0 , feat ] = np . nan return experiments def aggregate_by_duplicates ( self , experiments : pd . DataFrame , prec : int , delimiter : str = \"-\" ) -> Tuple [ pd . DataFrame , list ]: \"\"\"Aggregate the dataframe by duplicate experiments Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features. Args: experiments (pd.DataFrame): Dataframe containing experimental data prec (int): Precision of the rounding of the continuous input features delimiter (str, optional): Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\". Returns: Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates \"\"\" # prepare the parent frame preprocessed = self . preprocess_experiments_any_valid_output ( experiments ) assert preprocessed is not None experiments = preprocessed . copy () if \"labcode\" not in experiments . columns : experiments [ \"labcode\" ] = [ str ( i + 1 ) . zfill ( int ( np . ceil ( np . log10 ( experiments . shape [ 0 ])))) for i in range ( experiments . shape [ 0 ]) ] # round it experiments [ self . get_feature_keys ( ContinuousInput )] = experiments [ self . get_feature_keys ( ContinuousInput ) ] . round ( prec ) # coerce invalid to nan experiments = self . coerce_invalids ( experiments ) # group and aggregate agg : Dict [ str , Any ] = { feat : \"mean\" for feat in self . get_feature_keys ( ContinuousOutput ) } agg [ \"labcode\" ] = lambda x : delimiter . join ( sorted ( x . tolist ())) for feat in self . get_feature_keys ( OutputFeature ): agg [ f \"valid_ { feat } \" ] = lambda x : 1 grouped = experiments . groupby ( self . get_feature_keys ( InputFeature )) duplicated_labcodes = [ sorted ( group . labcode . to_numpy () . tolist ()) for _ , group in grouped if group . shape [ 0 ] > 1 ] experiments = grouped . aggregate ( agg ) . reset_index ( drop = False ) for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ feat ] . isna (), f \"valid_ { feat } \" ] = 0 experiments = experiments . sort_values ( by = \"labcode\" ) experiments = experiments . reset_index ( drop = True ) return experiments , sorted ( duplicated_labcodes ) def validate_experiments ( self , experiments : pd . DataFrame , strict : bool = False , ) -> pd . DataFrame : \"\"\"checks the experimental data on validity Args: experiments (pd.DataFrame): Dataframe with experimental data Raises: ValueError: empty dataframe ValueError: the column for a specific feature is missing the provided data ValueError: there are labcodes with null value ValueError: there are labcodes with nan value ValueError: labcodes are not unique ValueError: the provided columns do no match to the defined domain ValueError: the provided columns do no match to the defined domain ValueError: inputFeature with null values ValueError: inputFeature with nan values Returns: pd.DataFrame: The provided dataframe with experimental data \"\"\" if len ( experiments ) == 0 : raise ValueError ( \"no experiments provided (empty dataframe)\" ) # check that each feature is a col feature_keys = self . get_feature_keys () for feature_key in feature_keys : if feature_key not in experiments : raise ValueError ( f \"no col in experiments for feature { feature_key } \" ) # add valid_{key} cols if missing valid_keys = [ f \"valid_ { output_feature_key } \" for output_feature_key in self . get_feature_keys ( OutputFeature ) ] for valid_key in valid_keys : if valid_key not in experiments : experiments [ valid_key ] = True # check all cols expected = feature_keys + valid_keys cols = list ( experiments . columns ) # we allow here for a column named labcode used to identify experiments if \"labcode\" in cols : # test that labcodes are not na if experiments . labcode . isnull () . to_numpy () . any (): raise ValueError ( \"there are labcodes with null value\" ) if experiments . labcode . isna () . to_numpy () . any (): raise ValueError ( \"there are labcodes with nan value\" ) # test that labcodes are distinct if ( len ( set ( experiments . labcode . to_numpy () . tolist ())) != experiments . shape [ 0 ] ): raise ValueError ( \"labcodes are not unique\" ) # we remove the labcode from the cols list to proceed as before cols . remove ( \"labcode\" ) if len ( expected ) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) if len ( set ( expected + cols )) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) # check values of continuous input features if experiments [ self . get_feature_keys ( InputFeature )] . isnull () . to_numpy () . any (): raise ValueError ( \"there are null values\" ) if experiments [ self . get_feature_keys ( InputFeature )] . isna () . to_numpy () . any (): raise ValueError ( \"there are na values\" ) # run the individual validators for feat in self . get_features ( InputFeature ): assert isinstance ( feat , InputFeature ) feat . validate_experimental ( experiments [ feat . key ], strict = strict ) return experiments def describe_experiments ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature \"\"\" data = {} for feat in self . get_feature_keys ( OutputFeature ): data [ feat ] = [ experiments . loc [ experiments [ feat ] . notna ()] . shape [ 0 ], experiments . loc [ experiments [ feat ] . notna (), \"valid_ %s \" % feat ] . sum (), ] preprocessed = self . preprocess_experiments_all_valid_outputs ( experiments ) assert preprocessed is not None data [ \"all\" ] = [ experiments . shape [ 0 ], preprocessed . shape [ 0 ], ] return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = [ \"measured\" , \"valid\" ] ) def validate_candidates ( self , candidates : pd . DataFrame , only_inputs : bool = False ) -> pd . DataFrame : \"\"\"Method to check the validty of porposed candidates Args: candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates) only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False. Raises: ValueError: when a column is missing for a defined input feature ValueError: when a column is missing for a defined output feature ValueError: when a non-numerical value is proposed ValueError: when the constraints are not fulfilled ValueError: when an additional column is found Returns: pd.DataFrame: dataframe with suggested experiments (candidates) \"\"\" # check that each input feature has a col and is valid in itself assert isinstance ( self . input_features , InputFeatures ) self . input_features . validate_inputs ( candidates ) # check if all constraints are fulfilled if not self . cnstrs . is_fulfilled ( candidates ) . all (): raise ValueError ( \"Constraints not fulfilled.\" ) # for each continuous output feature with an attached objective object if not only_inputs : assert isinstance ( self . output_features , OutputFeatures ) for key in self . output_features . get_keys_by_objective ( Objective ): # check that pred, sd, and des cols are specified and numerical for col in [ f \" { key } _pred\" , f \" { key } _sd\" , f \" { key } _des\" ]: if col not in candidates : raise ValueError ( \"missing column {col} \" ) if ( not is_numeric ( candidates [ col ])) and ( not candidates [ col ] . isnull () . to_numpy () . all () ): raise ValueError ( f \"not all values of output feature ` { key } ` are numerical\" ) # validate no additional cols exist if_count = len ( self . get_features ( InputFeature )) of_count = len ( self . output_features . get_keys_by_objective ( Objective )) # input features, prediction, standard deviation and reward for each output feature, 3 additional usefull infos: reward, aquisition function, strategy if len ( candidates . columns ) != if_count + 3 * of_count : raise ValueError ( \"additional columns found\" ) return candidates @property def experiment_column_names ( self ): \"\"\"the columns in the experimental dataframe Returns: List[str]: List of columns in the experiment dataframe (output feature keys + valid_output feature keys) \"\"\" return self . get_feature_keys () + [ f \"valid_ { output_feature_key } \" for output_feature_key in self . get_feature_keys ( OutputFeature ) ] @property def candidate_column_names ( self ): \"\"\"the columns in the candidate dataframe Returns: List[str]: List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des) \"\"\" assert isinstance ( self . output_features , OutputFeatures ) return ( self . get_feature_keys ( InputFeature ) + [ f \" { output_feature_key } _pred\" for output_feature_key in self . output_features . get_keys_by_objective ( Objective ) ] + [ f \" { output_feature_key } _sd\" for output_feature_key in self . output_features . get_keys_by_objective ( Objective ) ] + [ f \" { output_feature_key } _des\" for output_feature_key in self . output_features . get_keys_by_objective ( Objective ) ] ) def set_candidates ( self , candidates : pd . DataFrame ): candidates = self . validate_candidates ( candidates ) self . candidates = candidates def add_candidates ( self , candidates : pd . DataFrame ): candidates = self . validate_candidates ( candidates ) if candidates is None : self . candidates = candidates else : self . _candidates = pd . concat ( ( self . _candidates , candidates ), ignore_index = True ) @property def num_candidates ( self ) -> int : if self . candidates is None : return 0 return len ( self . candidates ) def set_experiments ( self , experiments : pd . DataFrame ): experiments = self . validate_experiments ( experiments ) self . experiments = experiments def add_experiments ( self , experiments : pd . DataFrame ): experiments = self . validate_experiments ( experiments ) if experiments is None : self . experiments = None elif self . experiments is None : self . experiments = experiments else : self . experiments = pd . concat ( ( self . experiments , experiments ), ignore_index = True ) def _set_constraints_unvalidated ( self , constraints : Union [ Sequence [ Constraint ], Constraints ] ): \"\"\"Hack for reduce_domain\"\"\" self . constraints = Constraints ( constraints = []) if isinstance ( constraints , Constraints ): constraints = constraints . constraints self . constraints . constraints = constraints @property def num_experiments ( self ) -> int : if self . experiments is None : return 0 return len ( self . experiments ) candidate_column_names property readonly the columns in the candidate dataframe Returns: Type Description List[str] List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des) experiment_column_names property readonly the columns in the experimental dataframe Returns: Type Description List[str] List of columns in the experiment dataframe (output feature keys + valid_output feature keys) inputs : InputFeatures property readonly Returns input features as InputFeatures outputs : OutputFeatures property readonly Returns output features as OutputFeatures aggregate_by_duplicates ( self , experiments , prec , delimiter = '-' ) Aggregate the dataframe by duplicate experiments Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe containing experimental data required prec int Precision of the rounding of the continuous input features required delimiter str Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\". '-' Returns: Type Description Tuple[pd.DataFrame, list] Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates Source code in bofire/domain/domain.py def aggregate_by_duplicates ( self , experiments : pd . DataFrame , prec : int , delimiter : str = \"-\" ) -> Tuple [ pd . DataFrame , list ]: \"\"\"Aggregate the dataframe by duplicate experiments Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features. Args: experiments (pd.DataFrame): Dataframe containing experimental data prec (int): Precision of the rounding of the continuous input features delimiter (str, optional): Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\". Returns: Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates \"\"\" # prepare the parent frame preprocessed = self . preprocess_experiments_any_valid_output ( experiments ) assert preprocessed is not None experiments = preprocessed . copy () if \"labcode\" not in experiments . columns : experiments [ \"labcode\" ] = [ str ( i + 1 ) . zfill ( int ( np . ceil ( np . log10 ( experiments . shape [ 0 ])))) for i in range ( experiments . shape [ 0 ]) ] # round it experiments [ self . get_feature_keys ( ContinuousInput )] = experiments [ self . get_feature_keys ( ContinuousInput ) ] . round ( prec ) # coerce invalid to nan experiments = self . coerce_invalids ( experiments ) # group and aggregate agg : Dict [ str , Any ] = { feat : \"mean\" for feat in self . get_feature_keys ( ContinuousOutput ) } agg [ \"labcode\" ] = lambda x : delimiter . join ( sorted ( x . tolist ())) for feat in self . get_feature_keys ( OutputFeature ): agg [ f \"valid_ { feat } \" ] = lambda x : 1 grouped = experiments . groupby ( self . get_feature_keys ( InputFeature )) duplicated_labcodes = [ sorted ( group . labcode . to_numpy () . tolist ()) for _ , group in grouped if group . shape [ 0 ] > 1 ] experiments = grouped . aggregate ( agg ) . reset_index ( drop = False ) for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ feat ] . isna (), f \"valid_ { feat } \" ] = 0 experiments = experiments . sort_values ( by = \"labcode\" ) experiments = experiments . reset_index ( drop = True ) return experiments , sorted ( duplicated_labcodes ) coerce_invalids ( self , experiments ) Coerces all invalid output measurements to np.nan Parameters: Name Type Description Default experiments pd.DataFrame Dataframe containing experimental data required Returns: Type Description pd.DataFrame coerced dataframe Source code in bofire/domain/domain.py def coerce_invalids ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Coerces all invalid output measurements to np.nan Args: experiments (pd.DataFrame): Dataframe containing experimental data Returns: pd.DataFrame: coerced dataframe \"\"\" # coerce invalid to nan for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ f \"valid_ { feat } \" ] == 0 , feat ] = np . nan return experiments describe_experiments ( self , experiments ) Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data required Returns: Type Description pd.DataFrame Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature Source code in bofire/domain/domain.py def describe_experiments ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature \"\"\" data = {} for feat in self . get_feature_keys ( OutputFeature ): data [ feat ] = [ experiments . loc [ experiments [ feat ] . notna ()] . shape [ 0 ], experiments . loc [ experiments [ feat ] . notna (), \"valid_ %s \" % feat ] . sum (), ] preprocessed = self . preprocess_experiments_all_valid_outputs ( experiments ) assert preprocessed is not None data [ \"all\" ] = [ experiments . shape [ 0 ], preprocessed . shape [ 0 ], ] return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = [ \"measured\" , \"valid\" ] ) from_config ( config ) classmethod Instantiates a Domain object from a dictionary created by the to_config method. Parameters: Name Type Description Default config Dict Serialized version of a domain as dictionary. required Source code in bofire/domain/domain.py @classmethod def from_config ( cls , config : Dict ): \"\"\"Instantiates a `Domain` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized version of a domain as dictionary. \"\"\" d = cls ( input_features = typing . cast ( InputFeatures , InputFeatures . from_config ( config [ \"input_features\" ]) ), output_features = typing . cast ( OutputFeatures , OutputFeatures . from_config ( config [ \"output_features\" ]) ), constraints = Constraints . from_config ( config [ \"constraints\" ]), ) if \"experiments\" in config . keys (): d . set_experiments ( experiments = config [ \"experiments\" ]) if \"candidates\" in config . keys (): d . set_candidates ( candidates = config [ \"candidates\" ]) return d get_constraint_reps_df ( self ) Provides a tabular overwiev of all constraints within the domain Returns: Type Description pd.DataFrame DataFrame listing all constraints of the domain with a description Source code in bofire/domain/domain.py def get_constraint_reps_df ( self ): \"\"\"Provides a tabular overwiev of all constraints within the domain Returns: pd.DataFrame: DataFrame listing all constraints of the domain with a description \"\"\" df = pd . DataFrame ( index = range ( len ( self . constraints )), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . constraints ], \"Description\" : [ constraint . __str__ () for constraint in self . constraints ], }, ) return df get_feature ( self , key ) get a specific feature by its key Parameters: Name Type Description Default key str Feature key required Returns: Type Description Feature The feature with the passed key Source code in bofire/domain/domain.py def get_feature ( self , key : str ): \"\"\"get a specific feature by its key Args: key (str): Feature key Returns: Feature: The feature with the passed key \"\"\" assert isinstance ( self . input_features , InputFeatures ) return { f . key : f for f in self . input_features + self . output_features }[ key ] get_feature_keys ( self , includes =< class ' bofire . domain . features . Feature '>, excludes=None, exact=False) Method to get feature keys of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[str] List of feature keys fitting to the passed requirements. Source code in bofire/domain/domain.py def get_feature_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_features ( includes = includes , excludes = excludes , exact = exact , ) ] get_feature_reps_df ( self ) Returns a pandas dataframe describing the features contained in the optimization domain. Source code in bofire/domain/domain.py def get_feature_reps_df ( self ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe describing the features contained in the optimization domain.\"\"\" df = pd . DataFrame ( index = self . get_feature_keys ( Feature ), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . get_features ( Feature ) ], \"Description\" : [ feat . __str__ () for feat in self . get_features ( Feature )], }, ) return df get_features ( self , includes =< class ' bofire . domain . features . Feature '>, excludes=None, exact=False) get features of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False by_attribute str If set it is filtered by the attribute specified in by by_attribute . Defaults to None. required Returns: Type Description List[Feature] List of features in the domain fitting to the passed requirements. Source code in bofire/domain/domain.py def get_features ( self , includes : Union [ Type [ Feature ], List [ Type [ Feature ]]] = Feature , excludes : Union [ Type [ Feature ], List [ Type [ Feature ]], None ] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" assert isinstance ( self . input_features , InputFeatures ) features = self . input_features + self . output_features return features . get ( includes , excludes , exact ) get_nchoosek_combinations ( self ) get all possible NChooseK combinations Returns: Type Description Tuple(used_features_list, unused_features_list) used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination. Source code in bofire/domain/domain.py def get_nchoosek_combinations ( self ): \"\"\"get all possible NChooseK combinations Returns: Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination. \"\"\" if len ( self . cnstrs . get ( NChooseKConstraint )) == 0 : used_continuous_features = self . get_feature_keys ( ContinuousInput ) return used_continuous_features , [] used_features_list_all = [] # loops through each NChooseK constraint for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) used_features_list = [] for n in range ( con . min_count , con . max_count + 1 ): used_features_list . extend ( itertools . combinations ( con . features , n )) if con . none_also_valid : used_features_list . append ( tuple ([])) used_features_list_all . append ( used_features_list ) used_features_list_all = list ( itertools . product ( * used_features_list_all ) ) # product between NChooseK constraints # format into a list of used features used_features_list_formatted = [] for used_features_list in used_features_list_all : used_features_list_flattened = [ item for sublist in used_features_list for item in sublist ] used_features_list_formatted . append ( list ( set ( used_features_list_flattened ))) # sort lists used_features_list_sorted = [] for used_features in used_features_list_formatted : used_features_list_sorted . append ( sorted ( used_features )) # drop duplicates used_features_list_no_dup = [] for used_features in used_features_list_sorted : if used_features not in used_features_list_no_dup : used_features_list_no_dup . append ( used_features ) # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\") # remove combinations not fulfilling constraints used_features_list_final = [] for combo in used_features_list_no_dup : fulfil_constraints = ( [] ) # list of bools tracking if constraints are fulfilled for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) count = 0 # count of features in combo that are in con.features for f in combo : if f in con . features : count += 1 if count >= con . min_count and count <= con . max_count : fulfil_constraints . append ( True ) elif count == 0 and con . none_also_valid : fulfil_constraints . append ( True ) else : fulfil_constraints . append ( False ) if np . all ( fulfil_constraints ): used_features_list_final . append ( combo ) # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\") # features unused features_in_cc = [] for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) features_in_cc . extend ( con . features ) features_in_cc = list ( set ( features_in_cc )) features_in_cc . sort () unused_features_list = [] for used_features in used_features_list_final : unused_features_list . append ( [ f_key for f_key in features_in_cc if f_key not in used_features ] ) # postprocess # used_features_list_final2 = [] # unused_features_list2 = [] # for used, unused in zip(used_features_list_final,unused_features_list): # if len(used) == 3: # used_features_list_final2.append(used), unused_features_list2.append(unused) return used_features_list_final , unused_features_list preprocess_experiments_all_valid_outputs ( self , experiments = None , output_feature_keys = None ) Method to get a dataframe where non-valid entries of all output feature are removed Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data None output_feature_keys Optional[List] List of output feature keys which should be considered for removal of invalid values. Defaults to None. None Returns: Type Description pd.DataFrame Dataframe with all experiments where only valid entries of the selected features are included Source code in bofire/domain/domain.py def preprocess_experiments_all_valid_outputs ( self , experiments : Optional [ pd . DataFrame ] = None , output_feature_keys : Optional [ List ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of all output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None. Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) if ( output_feature_keys is None ) or ( len ( output_feature_keys ) == 0 ): output_feature_keys = self . get_feature_keys ( OutputFeature ) else : for key in output_feature_keys : feat = self . get_feature ( key ) assert isinstance ( feat , OutputFeature ), f \"feat { key } is not an OutputFeature\" assert experiments is not None clean_exp = experiments . query ( \" & \" . join ([ \"(`valid_ %s ` > 0)\" % key for key in output_feature_keys ]) ) clean_exp = clean_exp . dropna ( subset = output_feature_keys ) return clean_exp preprocess_experiments_any_valid_output ( self , experiments = None ) Method to get a dataframe where at least one output feature has a valid entry Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data None Returns: Type Description pd.DataFrame Dataframe with all experiments where at least one output feature has a valid entry Source code in bofire/domain/domain.py def preprocess_experiments_any_valid_output ( self , experiments : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\"Method to get a dataframe where at least one output feature has a valid entry Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) output_feature_keys = self . get_feature_keys ( OutputFeature ) # clean_exp = experiments.query(\" or \".join([\"(valid_%s > 0)\" % key for key in output_feature_keys])) # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys])) assert experiments is not None clean_exp = experiments . query ( \" or \" . join ( [ \"((`valid_ %s ` >0) & ` %s `.notna())\" % ( key , key ) for key in output_feature_keys ] ) ) return clean_exp preprocess_experiments_one_valid_output ( self , output_feature_key , experiments = None ) Method to get a dataframe where non-valid entries of the provided output feature are removed Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data None output_feature_key str The feature based on which non-valid entries rows are removed required Returns: Type Description pd.DataFrame Dataframe with all experiments where only valid entries of the specific feature are included Source code in bofire/domain/domain.py def preprocess_experiments_one_valid_output ( self , output_feature_key : str , experiments : Optional [ pd . DataFrame ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_key (str): The feature based on which non-valid entries rows are removed Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) assert experiments is not None clean_exp = experiments . loc [ ( experiments [ \"valid_ %s \" % output_feature_key ] == 1 ) & ( experiments [ output_feature_key ] . notna ()) ] # clean_exp = clean_exp.dropna() return clean_exp to_config ( self ) Serializables itself to a dictionary. Returns: Type Description Dict Serialized version of the domain as dictionary. Source code in bofire/domain/domain.py def to_config ( self ) -> Dict : \"\"\"Serializables itself to a dictionary. Returns: Dict: Serialized version of the domain as dictionary. \"\"\" assert isinstance ( self . input_features , InputFeatures ) assert isinstance ( self . output_features , OutputFeatures ) assert isinstance ( self . constraints , Constraints ) config : Dict [ str , Any ] = { \"input_features\" : self . input_features . to_config (), \"output_features\" : self . output_features . to_config (), \"constraints\" : self . constraints . to_config (), } if self . experiments is not None and self . num_experiments > 0 : config [ \"experiments\" ] = self . experiments . to_dict () if self . candidates is not None and self . num_candidates > 0 : config [ \"candidates\" ] = self . candidates . to_dict () return config validate_candidates ( self , candidates , only_inputs = False ) Method to check the validty of porposed candidates Parameters: Name Type Description Default candidates pd.DataFrame Dataframe with suggested new experiments (candidates) required only_inputs bool,optional If True, only the input columns are validated. Defaults to False. False Exceptions: Type Description ValueError when a column is missing for a defined input feature ValueError when a column is missing for a defined output feature ValueError when a non-numerical value is proposed ValueError when the constraints are not fulfilled ValueError when an additional column is found Returns: Type Description pd.DataFrame dataframe with suggested experiments (candidates) Source code in bofire/domain/domain.py def validate_candidates ( self , candidates : pd . DataFrame , only_inputs : bool = False ) -> pd . DataFrame : \"\"\"Method to check the validty of porposed candidates Args: candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates) only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False. Raises: ValueError: when a column is missing for a defined input feature ValueError: when a column is missing for a defined output feature ValueError: when a non-numerical value is proposed ValueError: when the constraints are not fulfilled ValueError: when an additional column is found Returns: pd.DataFrame: dataframe with suggested experiments (candidates) \"\"\" # check that each input feature has a col and is valid in itself assert isinstance ( self . input_features , InputFeatures ) self . input_features . validate_inputs ( candidates ) # check if all constraints are fulfilled if not self . cnstrs . is_fulfilled ( candidates ) . all (): raise ValueError ( \"Constraints not fulfilled.\" ) # for each continuous output feature with an attached objective object if not only_inputs : assert isinstance ( self . output_features , OutputFeatures ) for key in self . output_features . get_keys_by_objective ( Objective ): # check that pred, sd, and des cols are specified and numerical for col in [ f \" { key } _pred\" , f \" { key } _sd\" , f \" { key } _des\" ]: if col not in candidates : raise ValueError ( \"missing column {col} \" ) if ( not is_numeric ( candidates [ col ])) and ( not candidates [ col ] . isnull () . to_numpy () . all () ): raise ValueError ( f \"not all values of output feature ` { key } ` are numerical\" ) # validate no additional cols exist if_count = len ( self . get_features ( InputFeature )) of_count = len ( self . output_features . get_keys_by_objective ( Objective )) # input features, prediction, standard deviation and reward for each output feature, 3 additional usefull infos: reward, aquisition function, strategy if len ( candidates . columns ) != if_count + 3 * of_count : raise ValueError ( \"additional columns found\" ) return candidates validate_constraints ( v , values ) classmethod Validate if all features included in the constraints are also defined as features for the domain. Parameters: Name Type Description Default v List[Constraint] List of constraints or empty if no constraints are defined required values List[InputFeature] List of input features of the domain required Exceptions: Type Description ValueError Feature key in constraint is unknown. Returns: Type Description List[Constraint] List of constraints defined for the domain Source code in bofire/domain/domain.py @validator ( \"constraints\" , always = True ) def validate_constraints ( cls , v , values ): \"\"\"Validate if all features included in the constraints are also defined as features for the domain. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: Feature key in constraint is unknown. Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v keys = [ f . key for f in values [ \"input_features\" ]] for c in v : if isinstance ( c , LinearConstraint ) or isinstance ( c , NChooseKConstraint ): for f in c . features : if f not in keys : raise ValueError ( f \"feature { f } in constraint unknown ( { keys } )\" ) return v validate_experiments ( self , experiments , strict = False ) checks the experimental data on validity Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data required Exceptions: Type Description ValueError empty dataframe ValueError the column for a specific feature is missing the provided data ValueError there are labcodes with null value ValueError there are labcodes with nan value ValueError labcodes are not unique ValueError the provided columns do no match to the defined domain ValueError the provided columns do no match to the defined domain ValueError inputFeature with null values ValueError inputFeature with nan values Returns: Type Description pd.DataFrame The provided dataframe with experimental data Source code in bofire/domain/domain.py def validate_experiments ( self , experiments : pd . DataFrame , strict : bool = False , ) -> pd . DataFrame : \"\"\"checks the experimental data on validity Args: experiments (pd.DataFrame): Dataframe with experimental data Raises: ValueError: empty dataframe ValueError: the column for a specific feature is missing the provided data ValueError: there are labcodes with null value ValueError: there are labcodes with nan value ValueError: labcodes are not unique ValueError: the provided columns do no match to the defined domain ValueError: the provided columns do no match to the defined domain ValueError: inputFeature with null values ValueError: inputFeature with nan values Returns: pd.DataFrame: The provided dataframe with experimental data \"\"\" if len ( experiments ) == 0 : raise ValueError ( \"no experiments provided (empty dataframe)\" ) # check that each feature is a col feature_keys = self . get_feature_keys () for feature_key in feature_keys : if feature_key not in experiments : raise ValueError ( f \"no col in experiments for feature { feature_key } \" ) # add valid_{key} cols if missing valid_keys = [ f \"valid_ { output_feature_key } \" for output_feature_key in self . get_feature_keys ( OutputFeature ) ] for valid_key in valid_keys : if valid_key not in experiments : experiments [ valid_key ] = True # check all cols expected = feature_keys + valid_keys cols = list ( experiments . columns ) # we allow here for a column named labcode used to identify experiments if \"labcode\" in cols : # test that labcodes are not na if experiments . labcode . isnull () . to_numpy () . any (): raise ValueError ( \"there are labcodes with null value\" ) if experiments . labcode . isna () . to_numpy () . any (): raise ValueError ( \"there are labcodes with nan value\" ) # test that labcodes are distinct if ( len ( set ( experiments . labcode . to_numpy () . tolist ())) != experiments . shape [ 0 ] ): raise ValueError ( \"labcodes are not unique\" ) # we remove the labcode from the cols list to proceed as before cols . remove ( \"labcode\" ) if len ( expected ) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) if len ( set ( expected + cols )) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) # check values of continuous input features if experiments [ self . get_feature_keys ( InputFeature )] . isnull () . to_numpy () . any (): raise ValueError ( \"there are null values\" ) if experiments [ self . get_feature_keys ( InputFeature )] . isna () . to_numpy () . any (): raise ValueError ( \"there are na values\" ) # run the individual validators for feat in self . get_features ( InputFeature ): assert isinstance ( feat , InputFeature ) feat . validate_experimental ( experiments [ feat . key ], strict = strict ) return experiments validate_linear_constraints ( v , values ) classmethod Validate if all features included in linear constraints are continuous ones. Parameters: Name Type Description Default v List[Constraint] List of constraints or empty if no constraints are defined required values List[InputFeature] List of input features of the domain required Exceptions: Type Description ValueError description Returns: Type Description List[Constraint] List of constraints defined for the domain Source code in bofire/domain/domain.py @validator ( \"constraints\" , always = True ) def validate_linear_constraints ( cls , v , values ): \"\"\"Validate if all features included in linear constraints are continuous ones. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if non continuous input features appear in linear constraints for c in v : if isinstance ( c , LinearConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" return v validate_lower_bounds_in_nchoosek_constraints ( v , values ) classmethod Validate the lower bound as well if the chosen number of allowed features is continuous. Parameters: Name Type Description Default v List[Constraint] List of all constraints defined for the domain required values List[InputFeature] description required Returns: Type Description List[Constraint] List of constraints defined for the domain Source code in bofire/domain/domain.py @validator ( \"constraints\" , always = True ) def validate_lower_bounds_in_nchoosek_constraints ( cls , v , values ): \"\"\"Validate the lower bound as well if the chosen number of allowed features is continuous. Args: v (List[Constraint]): List of all constraints defined for the domain values (List[InputFeature]): _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if unfixed continuous features appearing in NChooseK constraints have lower bound of 0 for c in v : if isinstance ( c , NChooseKConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" assert ( continuous_input_features_dict [ f ] . lower_bound == 0 ), f \"lower bound of { f } must be 0 for NChooseK constraint.\" return v validate_unique_feature_keys ( v , values ) classmethod Validates if provided input and output feature keys are unique Parameters: Name Type Description Default v OutputFeatures List of all output features of the domain. required value Dict[str, InputFeatures] Dict containing a list of input features as single entry. required Exceptions: Type Description ValueError Feature keys are not unique. Returns: Type Description OutputFeatures Keeps output features as given. Source code in bofire/domain/domain.py @validator ( \"output_features\" , always = True ) def validate_unique_feature_keys ( cls , v : OutputFeatures , values ) -> OutputFeatures : \"\"\"Validates if provided input and output feature keys are unique Args: v (OutputFeatures): List of all output features of the domain. value (Dict[str, InputFeatures]): Dict containing a list of input features as single entry. Raises: ValueError: Feature keys are not unique. Returns: OutputFeatures: Keeps output features as given. \"\"\" if \"input_features\" not in values : return v features = v + values [ \"input_features\" ] keys = [ f . key for f in features ] if len ( set ( keys )) != len ( keys ): raise ValueError ( \"feature keys are not unique\" ) return v DomainError ( Exception ) A class defining a specific domain error Source code in bofire/domain/domain.py class DomainError ( Exception ): \"\"\"A class defining a specific domain error\"\"\" pass get_subdomain ( domain , feature_keys ) removes all features not defined as argument creating a subdomain of the provided domain Parameters: Name Type Description Default domain Domain the original domain wherefrom a subdomain should be created required feature_keys List List of features that shall be included in the subdomain required Exceptions: Type Description Assert when in total less than 2 features are provided ValueError when a provided feature key is not present in the provided domain Assert when no output feature is provided Assert when no input feature is provided ValueError description Returns: Type Description Domain A new domain containing only parts of the original domain Source code in bofire/domain/domain.py def get_subdomain ( domain : Domain , feature_keys : List , ) -> Domain : \"\"\"removes all features not defined as argument creating a subdomain of the provided domain Args: domain (Domain): the original domain wherefrom a subdomain should be created feature_keys (List): List of features that shall be included in the subdomain Raises: Assert: when in total less than 2 features are provided ValueError: when a provided feature key is not present in the provided domain Assert: when no output feature is provided Assert: when no input feature is provided ValueError: _description_ Returns: Domain: A new domain containing only parts of the original domain \"\"\" assert len ( feature_keys ) >= 2 , \"At least two features have to be provided.\" output_features = [] input_features = [] for key in feature_keys : try : feat = domain . get_feature ( key ) except KeyError : raise ValueError ( f \"Feature { key } not present in domain.\" ) if isinstance ( feat , InputFeature ): input_features . append ( feat ) else : output_features . append ( feat ) assert len ( output_features ) > 0 , \"At least one output feature has to be provided.\" assert len ( input_features ) > 0 , \"At least one input feature has to be provided.\" input_features = InputFeatures ( features = input_features ) # loop over constraints and make sure that all features used in constraints are in the input_feature_keys for c in domain . constraints : # TODO: fix type hint for key in c . features : # type: ignore if key not in input_features . get_keys (): raise ValueError ( f \"Removed input feature { key } is used in a constraint.\" ) subdomain = deepcopy ( domain ) subdomain . input_features = input_features subdomain . output_features = output_features return subdomain","title":"Domain"},{"location":"ref-domain/#domain","text":"","title":"Domain"},{"location":"ref-domain/#bofire.domain.domain.Domain","text":"Source code in bofire/domain/domain.py class Domain ( BaseModel ): # The types describe what we expect to be passed as arguments. # They will be converted to InputFeatures and OutputFeatures, respectively. input_features : Union [ Sequence [ InputFeature ], InputFeatures ] = Field ( default_factory = lambda : InputFeatures () ) output_features : Union [ Sequence [ OutputFeature ], OutputFeatures ] = Field ( default_factory = lambda : OutputFeatures () ) constraints : Union [ Sequence [ Constraint ], Constraints ] = Field ( default_factory = lambda : Constraints () ) experiments : Optional [ pd . DataFrame ] = None candidates : Optional [ pd . DataFrame ] = None \"\"\"Representation of the optimization problem/domain Attributes: input_features (List[InputFeature], optional): List of input features. Defaults to []. output_features (List[OutputFeature], optional): List of output features. Defaults to []. constraints (List[Constraint], optional): List of constraints. Defaults to []. \"\"\" @property def outputs ( self ) -> OutputFeatures : \"\"\"Returns output features as OutputFeatures\"\"\" return cast ( OutputFeatures , self . output_features ) @property def inputs ( self ) -> InputFeatures : \"\"\"Returns input features as InputFeatures\"\"\" return cast ( InputFeatures , self . input_features ) @property def cnstrs ( self ) -> Constraints : return cast ( Constraints , self . constraints ) @validator ( \"input_features\" , always = True , pre = True ) def validate_input_features_list ( cls , v , values ): if isinstance ( v , collections . abc . Sequence ): v = InputFeatures ( features = v ) return v if isinstance ( v , InputFeature ): return InputFeatures ( features = [ v ]) else : return v @validator ( \"output_features\" , always = True , pre = True ) def validate_output_features_list ( cls , v , values ): if isinstance ( v , collections . abc . Sequence ): return OutputFeatures ( features = v ) if isinstance ( v , OutputFeature ): return OutputFeatures ( features = [ v ]) else : return v @validator ( \"constraints\" , always = True , pre = True ) def validate_constraints_list ( cls , v , values ): if isinstance ( v , list ): return Constraints ( constraints = v ) if isinstance ( v , Constraint ): return Constraints ( constraints = [ v ]) else : return v @validator ( \"output_features\" , always = True ) def validate_unique_feature_keys ( cls , v : OutputFeatures , values ) -> OutputFeatures : \"\"\"Validates if provided input and output feature keys are unique Args: v (OutputFeatures): List of all output features of the domain. value (Dict[str, InputFeatures]): Dict containing a list of input features as single entry. Raises: ValueError: Feature keys are not unique. Returns: OutputFeatures: Keeps output features as given. \"\"\" if \"input_features\" not in values : return v features = v + values [ \"input_features\" ] keys = [ f . key for f in features ] if len ( set ( keys )) != len ( keys ): raise ValueError ( \"feature keys are not unique\" ) return v @validator ( \"constraints\" , always = True ) def validate_constraints ( cls , v , values ): \"\"\"Validate if all features included in the constraints are also defined as features for the domain. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: Feature key in constraint is unknown. Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v keys = [ f . key for f in values [ \"input_features\" ]] for c in v : if isinstance ( c , LinearConstraint ) or isinstance ( c , NChooseKConstraint ): for f in c . features : if f not in keys : raise ValueError ( f \"feature { f } in constraint unknown ( { keys } )\" ) return v @validator ( \"constraints\" , always = True ) def validate_linear_constraints ( cls , v , values ): \"\"\"Validate if all features included in linear constraints are continuous ones. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if non continuous input features appear in linear constraints for c in v : if isinstance ( c , LinearConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" return v @validator ( \"constraints\" , always = True ) def validate_lower_bounds_in_nchoosek_constraints ( cls , v , values ): \"\"\"Validate the lower bound as well if the chosen number of allowed features is continuous. Args: v (List[Constraint]): List of all constraints defined for the domain values (List[InputFeature]): _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if unfixed continuous features appearing in NChooseK constraints have lower bound of 0 for c in v : if isinstance ( c , NChooseKConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" assert ( continuous_input_features_dict [ f ] . lower_bound == 0 ), f \"lower bound of { f } must be 0 for NChooseK constraint.\" return v def to_config ( self ) -> Dict : \"\"\"Serializables itself to a dictionary. Returns: Dict: Serialized version of the domain as dictionary. \"\"\" assert isinstance ( self . input_features , InputFeatures ) assert isinstance ( self . output_features , OutputFeatures ) assert isinstance ( self . constraints , Constraints ) config : Dict [ str , Any ] = { \"input_features\" : self . input_features . to_config (), \"output_features\" : self . output_features . to_config (), \"constraints\" : self . constraints . to_config (), } if self . experiments is not None and self . num_experiments > 0 : config [ \"experiments\" ] = self . experiments . to_dict () if self . candidates is not None and self . num_candidates > 0 : config [ \"candidates\" ] = self . candidates . to_dict () return config @classmethod def from_config ( cls , config : Dict ): \"\"\"Instantiates a `Domain` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized version of a domain as dictionary. \"\"\" d = cls ( input_features = typing . cast ( InputFeatures , InputFeatures . from_config ( config [ \"input_features\" ]) ), output_features = typing . cast ( OutputFeatures , OutputFeatures . from_config ( config [ \"output_features\" ]) ), constraints = Constraints . from_config ( config [ \"constraints\" ]), ) if \"experiments\" in config . keys (): d . set_experiments ( experiments = config [ \"experiments\" ]) if \"candidates\" in config . keys (): d . set_candidates ( candidates = config [ \"candidates\" ]) return d def get_feature_reps_df ( self ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe describing the features contained in the optimization domain.\"\"\" df = pd . DataFrame ( index = self . get_feature_keys ( Feature ), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . get_features ( Feature ) ], \"Description\" : [ feat . __str__ () for feat in self . get_features ( Feature )], }, ) return df def get_constraint_reps_df ( self ): \"\"\"Provides a tabular overwiev of all constraints within the domain Returns: pd.DataFrame: DataFrame listing all constraints of the domain with a description \"\"\" df = pd . DataFrame ( index = range ( len ( self . constraints )), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . constraints ], \"Description\" : [ constraint . __str__ () for constraint in self . constraints ], }, ) return df def get_features ( self , includes : Union [ Type [ Feature ], List [ Type [ Feature ]]] = Feature , excludes : Union [ Type [ Feature ], List [ Type [ Feature ]], None ] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" assert isinstance ( self . input_features , InputFeatures ) features = self . input_features + self . output_features return features . get ( includes , excludes , exact ) def get_feature_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_features ( includes = includes , excludes = excludes , exact = exact , ) ] def get_feature ( self , key : str ): \"\"\"get a specific feature by its key Args: key (str): Feature key Returns: Feature: The feature with the passed key \"\"\" assert isinstance ( self . input_features , InputFeatures ) return { f . key : f for f in self . input_features + self . output_features }[ key ] # getting list of fixed values def get_nchoosek_combinations ( self ): \"\"\"get all possible NChooseK combinations Returns: Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination. \"\"\" if len ( self . cnstrs . get ( NChooseKConstraint )) == 0 : used_continuous_features = self . get_feature_keys ( ContinuousInput ) return used_continuous_features , [] used_features_list_all = [] # loops through each NChooseK constraint for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) used_features_list = [] for n in range ( con . min_count , con . max_count + 1 ): used_features_list . extend ( itertools . combinations ( con . features , n )) if con . none_also_valid : used_features_list . append ( tuple ([])) used_features_list_all . append ( used_features_list ) used_features_list_all = list ( itertools . product ( * used_features_list_all ) ) # product between NChooseK constraints # format into a list of used features used_features_list_formatted = [] for used_features_list in used_features_list_all : used_features_list_flattened = [ item for sublist in used_features_list for item in sublist ] used_features_list_formatted . append ( list ( set ( used_features_list_flattened ))) # sort lists used_features_list_sorted = [] for used_features in used_features_list_formatted : used_features_list_sorted . append ( sorted ( used_features )) # drop duplicates used_features_list_no_dup = [] for used_features in used_features_list_sorted : if used_features not in used_features_list_no_dup : used_features_list_no_dup . append ( used_features ) # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\") # remove combinations not fulfilling constraints used_features_list_final = [] for combo in used_features_list_no_dup : fulfil_constraints = ( [] ) # list of bools tracking if constraints are fulfilled for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) count = 0 # count of features in combo that are in con.features for f in combo : if f in con . features : count += 1 if count >= con . min_count and count <= con . max_count : fulfil_constraints . append ( True ) elif count == 0 and con . none_also_valid : fulfil_constraints . append ( True ) else : fulfil_constraints . append ( False ) if np . all ( fulfil_constraints ): used_features_list_final . append ( combo ) # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\") # features unused features_in_cc = [] for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) features_in_cc . extend ( con . features ) features_in_cc = list ( set ( features_in_cc )) features_in_cc . sort () unused_features_list = [] for used_features in used_features_list_final : unused_features_list . append ( [ f_key for f_key in features_in_cc if f_key not in used_features ] ) # postprocess # used_features_list_final2 = [] # unused_features_list2 = [] # for used, unused in zip(used_features_list_final,unused_features_list): # if len(used) == 3: # used_features_list_final2.append(used), unused_features_list2.append(unused) return used_features_list_final , unused_features_list def preprocess_experiments_one_valid_output ( self , output_feature_key : str , experiments : Optional [ pd . DataFrame ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_key (str): The feature based on which non-valid entries rows are removed Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) assert experiments is not None clean_exp = experiments . loc [ ( experiments [ \"valid_ %s \" % output_feature_key ] == 1 ) & ( experiments [ output_feature_key ] . notna ()) ] # clean_exp = clean_exp.dropna() return clean_exp def preprocess_experiments_all_valid_outputs ( self , experiments : Optional [ pd . DataFrame ] = None , output_feature_keys : Optional [ List ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of all output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None. Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) if ( output_feature_keys is None ) or ( len ( output_feature_keys ) == 0 ): output_feature_keys = self . get_feature_keys ( OutputFeature ) else : for key in output_feature_keys : feat = self . get_feature ( key ) assert isinstance ( feat , OutputFeature ), f \"feat { key } is not an OutputFeature\" assert experiments is not None clean_exp = experiments . query ( \" & \" . join ([ \"(`valid_ %s ` > 0)\" % key for key in output_feature_keys ]) ) clean_exp = clean_exp . dropna ( subset = output_feature_keys ) return clean_exp def preprocess_experiments_any_valid_output ( self , experiments : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\"Method to get a dataframe where at least one output feature has a valid entry Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) output_feature_keys = self . get_feature_keys ( OutputFeature ) # clean_exp = experiments.query(\" or \".join([\"(valid_%s > 0)\" % key for key in output_feature_keys])) # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys])) assert experiments is not None clean_exp = experiments . query ( \" or \" . join ( [ \"((`valid_ %s ` >0) & ` %s `.notna())\" % ( key , key ) for key in output_feature_keys ] ) ) return clean_exp def coerce_invalids ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Coerces all invalid output measurements to np.nan Args: experiments (pd.DataFrame): Dataframe containing experimental data Returns: pd.DataFrame: coerced dataframe \"\"\" # coerce invalid to nan for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ f \"valid_ { feat } \" ] == 0 , feat ] = np . nan return experiments def aggregate_by_duplicates ( self , experiments : pd . DataFrame , prec : int , delimiter : str = \"-\" ) -> Tuple [ pd . DataFrame , list ]: \"\"\"Aggregate the dataframe by duplicate experiments Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features. Args: experiments (pd.DataFrame): Dataframe containing experimental data prec (int): Precision of the rounding of the continuous input features delimiter (str, optional): Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\". Returns: Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates \"\"\" # prepare the parent frame preprocessed = self . preprocess_experiments_any_valid_output ( experiments ) assert preprocessed is not None experiments = preprocessed . copy () if \"labcode\" not in experiments . columns : experiments [ \"labcode\" ] = [ str ( i + 1 ) . zfill ( int ( np . ceil ( np . log10 ( experiments . shape [ 0 ])))) for i in range ( experiments . shape [ 0 ]) ] # round it experiments [ self . get_feature_keys ( ContinuousInput )] = experiments [ self . get_feature_keys ( ContinuousInput ) ] . round ( prec ) # coerce invalid to nan experiments = self . coerce_invalids ( experiments ) # group and aggregate agg : Dict [ str , Any ] = { feat : \"mean\" for feat in self . get_feature_keys ( ContinuousOutput ) } agg [ \"labcode\" ] = lambda x : delimiter . join ( sorted ( x . tolist ())) for feat in self . get_feature_keys ( OutputFeature ): agg [ f \"valid_ { feat } \" ] = lambda x : 1 grouped = experiments . groupby ( self . get_feature_keys ( InputFeature )) duplicated_labcodes = [ sorted ( group . labcode . to_numpy () . tolist ()) for _ , group in grouped if group . shape [ 0 ] > 1 ] experiments = grouped . aggregate ( agg ) . reset_index ( drop = False ) for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ feat ] . isna (), f \"valid_ { feat } \" ] = 0 experiments = experiments . sort_values ( by = \"labcode\" ) experiments = experiments . reset_index ( drop = True ) return experiments , sorted ( duplicated_labcodes ) def validate_experiments ( self , experiments : pd . DataFrame , strict : bool = False , ) -> pd . DataFrame : \"\"\"checks the experimental data on validity Args: experiments (pd.DataFrame): Dataframe with experimental data Raises: ValueError: empty dataframe ValueError: the column for a specific feature is missing the provided data ValueError: there are labcodes with null value ValueError: there are labcodes with nan value ValueError: labcodes are not unique ValueError: the provided columns do no match to the defined domain ValueError: the provided columns do no match to the defined domain ValueError: inputFeature with null values ValueError: inputFeature with nan values Returns: pd.DataFrame: The provided dataframe with experimental data \"\"\" if len ( experiments ) == 0 : raise ValueError ( \"no experiments provided (empty dataframe)\" ) # check that each feature is a col feature_keys = self . get_feature_keys () for feature_key in feature_keys : if feature_key not in experiments : raise ValueError ( f \"no col in experiments for feature { feature_key } \" ) # add valid_{key} cols if missing valid_keys = [ f \"valid_ { output_feature_key } \" for output_feature_key in self . get_feature_keys ( OutputFeature ) ] for valid_key in valid_keys : if valid_key not in experiments : experiments [ valid_key ] = True # check all cols expected = feature_keys + valid_keys cols = list ( experiments . columns ) # we allow here for a column named labcode used to identify experiments if \"labcode\" in cols : # test that labcodes are not na if experiments . labcode . isnull () . to_numpy () . any (): raise ValueError ( \"there are labcodes with null value\" ) if experiments . labcode . isna () . to_numpy () . any (): raise ValueError ( \"there are labcodes with nan value\" ) # test that labcodes are distinct if ( len ( set ( experiments . labcode . to_numpy () . tolist ())) != experiments . shape [ 0 ] ): raise ValueError ( \"labcodes are not unique\" ) # we remove the labcode from the cols list to proceed as before cols . remove ( \"labcode\" ) if len ( expected ) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) if len ( set ( expected + cols )) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) # check values of continuous input features if experiments [ self . get_feature_keys ( InputFeature )] . isnull () . to_numpy () . any (): raise ValueError ( \"there are null values\" ) if experiments [ self . get_feature_keys ( InputFeature )] . isna () . to_numpy () . any (): raise ValueError ( \"there are na values\" ) # run the individual validators for feat in self . get_features ( InputFeature ): assert isinstance ( feat , InputFeature ) feat . validate_experimental ( experiments [ feat . key ], strict = strict ) return experiments def describe_experiments ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature \"\"\" data = {} for feat in self . get_feature_keys ( OutputFeature ): data [ feat ] = [ experiments . loc [ experiments [ feat ] . notna ()] . shape [ 0 ], experiments . loc [ experiments [ feat ] . notna (), \"valid_ %s \" % feat ] . sum (), ] preprocessed = self . preprocess_experiments_all_valid_outputs ( experiments ) assert preprocessed is not None data [ \"all\" ] = [ experiments . shape [ 0 ], preprocessed . shape [ 0 ], ] return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = [ \"measured\" , \"valid\" ] ) def validate_candidates ( self , candidates : pd . DataFrame , only_inputs : bool = False ) -> pd . DataFrame : \"\"\"Method to check the validty of porposed candidates Args: candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates) only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False. Raises: ValueError: when a column is missing for a defined input feature ValueError: when a column is missing for a defined output feature ValueError: when a non-numerical value is proposed ValueError: when the constraints are not fulfilled ValueError: when an additional column is found Returns: pd.DataFrame: dataframe with suggested experiments (candidates) \"\"\" # check that each input feature has a col and is valid in itself assert isinstance ( self . input_features , InputFeatures ) self . input_features . validate_inputs ( candidates ) # check if all constraints are fulfilled if not self . cnstrs . is_fulfilled ( candidates ) . all (): raise ValueError ( \"Constraints not fulfilled.\" ) # for each continuous output feature with an attached objective object if not only_inputs : assert isinstance ( self . output_features , OutputFeatures ) for key in self . output_features . get_keys_by_objective ( Objective ): # check that pred, sd, and des cols are specified and numerical for col in [ f \" { key } _pred\" , f \" { key } _sd\" , f \" { key } _des\" ]: if col not in candidates : raise ValueError ( \"missing column {col} \" ) if ( not is_numeric ( candidates [ col ])) and ( not candidates [ col ] . isnull () . to_numpy () . all () ): raise ValueError ( f \"not all values of output feature ` { key } ` are numerical\" ) # validate no additional cols exist if_count = len ( self . get_features ( InputFeature )) of_count = len ( self . output_features . get_keys_by_objective ( Objective )) # input features, prediction, standard deviation and reward for each output feature, 3 additional usefull infos: reward, aquisition function, strategy if len ( candidates . columns ) != if_count + 3 * of_count : raise ValueError ( \"additional columns found\" ) return candidates @property def experiment_column_names ( self ): \"\"\"the columns in the experimental dataframe Returns: List[str]: List of columns in the experiment dataframe (output feature keys + valid_output feature keys) \"\"\" return self . get_feature_keys () + [ f \"valid_ { output_feature_key } \" for output_feature_key in self . get_feature_keys ( OutputFeature ) ] @property def candidate_column_names ( self ): \"\"\"the columns in the candidate dataframe Returns: List[str]: List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des) \"\"\" assert isinstance ( self . output_features , OutputFeatures ) return ( self . get_feature_keys ( InputFeature ) + [ f \" { output_feature_key } _pred\" for output_feature_key in self . output_features . get_keys_by_objective ( Objective ) ] + [ f \" { output_feature_key } _sd\" for output_feature_key in self . output_features . get_keys_by_objective ( Objective ) ] + [ f \" { output_feature_key } _des\" for output_feature_key in self . output_features . get_keys_by_objective ( Objective ) ] ) def set_candidates ( self , candidates : pd . DataFrame ): candidates = self . validate_candidates ( candidates ) self . candidates = candidates def add_candidates ( self , candidates : pd . DataFrame ): candidates = self . validate_candidates ( candidates ) if candidates is None : self . candidates = candidates else : self . _candidates = pd . concat ( ( self . _candidates , candidates ), ignore_index = True ) @property def num_candidates ( self ) -> int : if self . candidates is None : return 0 return len ( self . candidates ) def set_experiments ( self , experiments : pd . DataFrame ): experiments = self . validate_experiments ( experiments ) self . experiments = experiments def add_experiments ( self , experiments : pd . DataFrame ): experiments = self . validate_experiments ( experiments ) if experiments is None : self . experiments = None elif self . experiments is None : self . experiments = experiments else : self . experiments = pd . concat ( ( self . experiments , experiments ), ignore_index = True ) def _set_constraints_unvalidated ( self , constraints : Union [ Sequence [ Constraint ], Constraints ] ): \"\"\"Hack for reduce_domain\"\"\" self . constraints = Constraints ( constraints = []) if isinstance ( constraints , Constraints ): constraints = constraints . constraints self . constraints . constraints = constraints @property def num_experiments ( self ) -> int : if self . experiments is None : return 0 return len ( self . experiments )","title":"Domain"},{"location":"ref-domain/#bofire.domain.domain.Domain.candidate_column_names","text":"the columns in the candidate dataframe Returns: Type Description List[str] List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des)","title":"candidate_column_names"},{"location":"ref-domain/#bofire.domain.domain.Domain.experiment_column_names","text":"the columns in the experimental dataframe Returns: Type Description List[str] List of columns in the experiment dataframe (output feature keys + valid_output feature keys)","title":"experiment_column_names"},{"location":"ref-domain/#bofire.domain.domain.Domain.inputs","text":"Returns input features as InputFeatures","title":"inputs"},{"location":"ref-domain/#bofire.domain.domain.Domain.outputs","text":"Returns output features as OutputFeatures","title":"outputs"},{"location":"ref-domain/#bofire.domain.domain.Domain.aggregate_by_duplicates","text":"Aggregate the dataframe by duplicate experiments Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features. Parameters: Name Type Description Default experiments pd.DataFrame Dataframe containing experimental data required prec int Precision of the rounding of the continuous input features required delimiter str Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\". '-' Returns: Type Description Tuple[pd.DataFrame, list] Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates Source code in bofire/domain/domain.py def aggregate_by_duplicates ( self , experiments : pd . DataFrame , prec : int , delimiter : str = \"-\" ) -> Tuple [ pd . DataFrame , list ]: \"\"\"Aggregate the dataframe by duplicate experiments Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features. Args: experiments (pd.DataFrame): Dataframe containing experimental data prec (int): Precision of the rounding of the continuous input features delimiter (str, optional): Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\". Returns: Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates \"\"\" # prepare the parent frame preprocessed = self . preprocess_experiments_any_valid_output ( experiments ) assert preprocessed is not None experiments = preprocessed . copy () if \"labcode\" not in experiments . columns : experiments [ \"labcode\" ] = [ str ( i + 1 ) . zfill ( int ( np . ceil ( np . log10 ( experiments . shape [ 0 ])))) for i in range ( experiments . shape [ 0 ]) ] # round it experiments [ self . get_feature_keys ( ContinuousInput )] = experiments [ self . get_feature_keys ( ContinuousInput ) ] . round ( prec ) # coerce invalid to nan experiments = self . coerce_invalids ( experiments ) # group and aggregate agg : Dict [ str , Any ] = { feat : \"mean\" for feat in self . get_feature_keys ( ContinuousOutput ) } agg [ \"labcode\" ] = lambda x : delimiter . join ( sorted ( x . tolist ())) for feat in self . get_feature_keys ( OutputFeature ): agg [ f \"valid_ { feat } \" ] = lambda x : 1 grouped = experiments . groupby ( self . get_feature_keys ( InputFeature )) duplicated_labcodes = [ sorted ( group . labcode . to_numpy () . tolist ()) for _ , group in grouped if group . shape [ 0 ] > 1 ] experiments = grouped . aggregate ( agg ) . reset_index ( drop = False ) for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ feat ] . isna (), f \"valid_ { feat } \" ] = 0 experiments = experiments . sort_values ( by = \"labcode\" ) experiments = experiments . reset_index ( drop = True ) return experiments , sorted ( duplicated_labcodes )","title":"aggregate_by_duplicates()"},{"location":"ref-domain/#bofire.domain.domain.Domain.coerce_invalids","text":"Coerces all invalid output measurements to np.nan Parameters: Name Type Description Default experiments pd.DataFrame Dataframe containing experimental data required Returns: Type Description pd.DataFrame coerced dataframe Source code in bofire/domain/domain.py def coerce_invalids ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Coerces all invalid output measurements to np.nan Args: experiments (pd.DataFrame): Dataframe containing experimental data Returns: pd.DataFrame: coerced dataframe \"\"\" # coerce invalid to nan for feat in self . get_feature_keys ( OutputFeature ): experiments . loc [ experiments [ f \"valid_ { feat } \" ] == 0 , feat ] = np . nan return experiments","title":"coerce_invalids()"},{"location":"ref-domain/#bofire.domain.domain.Domain.describe_experiments","text":"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data required Returns: Type Description pd.DataFrame Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature Source code in bofire/domain/domain.py def describe_experiments ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature \"\"\" data = {} for feat in self . get_feature_keys ( OutputFeature ): data [ feat ] = [ experiments . loc [ experiments [ feat ] . notna ()] . shape [ 0 ], experiments . loc [ experiments [ feat ] . notna (), \"valid_ %s \" % feat ] . sum (), ] preprocessed = self . preprocess_experiments_all_valid_outputs ( experiments ) assert preprocessed is not None data [ \"all\" ] = [ experiments . shape [ 0 ], preprocessed . shape [ 0 ], ] return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = [ \"measured\" , \"valid\" ] )","title":"describe_experiments()"},{"location":"ref-domain/#bofire.domain.domain.Domain.from_config","text":"Instantiates a Domain object from a dictionary created by the to_config method. Parameters: Name Type Description Default config Dict Serialized version of a domain as dictionary. required Source code in bofire/domain/domain.py @classmethod def from_config ( cls , config : Dict ): \"\"\"Instantiates a `Domain` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized version of a domain as dictionary. \"\"\" d = cls ( input_features = typing . cast ( InputFeatures , InputFeatures . from_config ( config [ \"input_features\" ]) ), output_features = typing . cast ( OutputFeatures , OutputFeatures . from_config ( config [ \"output_features\" ]) ), constraints = Constraints . from_config ( config [ \"constraints\" ]), ) if \"experiments\" in config . keys (): d . set_experiments ( experiments = config [ \"experiments\" ]) if \"candidates\" in config . keys (): d . set_candidates ( candidates = config [ \"candidates\" ]) return d","title":"from_config()"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_constraint_reps_df","text":"Provides a tabular overwiev of all constraints within the domain Returns: Type Description pd.DataFrame DataFrame listing all constraints of the domain with a description Source code in bofire/domain/domain.py def get_constraint_reps_df ( self ): \"\"\"Provides a tabular overwiev of all constraints within the domain Returns: pd.DataFrame: DataFrame listing all constraints of the domain with a description \"\"\" df = pd . DataFrame ( index = range ( len ( self . constraints )), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . constraints ], \"Description\" : [ constraint . __str__ () for constraint in self . constraints ], }, ) return df","title":"get_constraint_reps_df()"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_feature","text":"get a specific feature by its key Parameters: Name Type Description Default key str Feature key required Returns: Type Description Feature The feature with the passed key Source code in bofire/domain/domain.py def get_feature ( self , key : str ): \"\"\"get a specific feature by its key Args: key (str): Feature key Returns: Feature: The feature with the passed key \"\"\" assert isinstance ( self . input_features , InputFeatures ) return { f . key : f for f in self . input_features + self . output_features }[ key ]","title":"get_feature()"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_feature_keys","text":"Method to get feature keys of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[str] List of feature keys fitting to the passed requirements. Source code in bofire/domain/domain.py def get_feature_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_features ( includes = includes , excludes = excludes , exact = exact , ) ]","title":"get_feature_keys()"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_feature_reps_df","text":"Returns a pandas dataframe describing the features contained in the optimization domain. Source code in bofire/domain/domain.py def get_feature_reps_df ( self ) -> pd . DataFrame : \"\"\"Returns a pandas dataframe describing the features contained in the optimization domain.\"\"\" df = pd . DataFrame ( index = self . get_feature_keys ( Feature ), columns = [ \"Type\" , \"Description\" ], data = { \"Type\" : [ feat . __class__ . __name__ for feat in self . get_features ( Feature ) ], \"Description\" : [ feat . __str__ () for feat in self . get_features ( Feature )], }, ) return df","title":"get_feature_reps_df()"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_features","text":"get features of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False by_attribute str If set it is filtered by the attribute specified in by by_attribute . Defaults to None. required Returns: Type Description List[Feature] List of features in the domain fitting to the passed requirements. Source code in bofire/domain/domain.py def get_features ( self , includes : Union [ Type [ Feature ], List [ Type [ Feature ]]] = Feature , excludes : Union [ Type [ Feature ], List [ Type [ Feature ]], None ] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" assert isinstance ( self . input_features , InputFeatures ) features = self . input_features + self . output_features return features . get ( includes , excludes , exact )","title":"get_features()"},{"location":"ref-domain/#bofire.domain.domain.Domain.get_nchoosek_combinations","text":"get all possible NChooseK combinations Returns: Type Description Tuple(used_features_list, unused_features_list) used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination. Source code in bofire/domain/domain.py def get_nchoosek_combinations ( self ): \"\"\"get all possible NChooseK combinations Returns: Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination. \"\"\" if len ( self . cnstrs . get ( NChooseKConstraint )) == 0 : used_continuous_features = self . get_feature_keys ( ContinuousInput ) return used_continuous_features , [] used_features_list_all = [] # loops through each NChooseK constraint for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) used_features_list = [] for n in range ( con . min_count , con . max_count + 1 ): used_features_list . extend ( itertools . combinations ( con . features , n )) if con . none_also_valid : used_features_list . append ( tuple ([])) used_features_list_all . append ( used_features_list ) used_features_list_all = list ( itertools . product ( * used_features_list_all ) ) # product between NChooseK constraints # format into a list of used features used_features_list_formatted = [] for used_features_list in used_features_list_all : used_features_list_flattened = [ item for sublist in used_features_list for item in sublist ] used_features_list_formatted . append ( list ( set ( used_features_list_flattened ))) # sort lists used_features_list_sorted = [] for used_features in used_features_list_formatted : used_features_list_sorted . append ( sorted ( used_features )) # drop duplicates used_features_list_no_dup = [] for used_features in used_features_list_sorted : if used_features not in used_features_list_no_dup : used_features_list_no_dup . append ( used_features ) # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\") # remove combinations not fulfilling constraints used_features_list_final = [] for combo in used_features_list_no_dup : fulfil_constraints = ( [] ) # list of bools tracking if constraints are fulfilled for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) count = 0 # count of features in combo that are in con.features for f in combo : if f in con . features : count += 1 if count >= con . min_count and count <= con . max_count : fulfil_constraints . append ( True ) elif count == 0 and con . none_also_valid : fulfil_constraints . append ( True ) else : fulfil_constraints . append ( False ) if np . all ( fulfil_constraints ): used_features_list_final . append ( combo ) # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\") # features unused features_in_cc = [] for con in self . cnstrs . get ( NChooseKConstraint ): assert isinstance ( con , NChooseKConstraint ) features_in_cc . extend ( con . features ) features_in_cc = list ( set ( features_in_cc )) features_in_cc . sort () unused_features_list = [] for used_features in used_features_list_final : unused_features_list . append ( [ f_key for f_key in features_in_cc if f_key not in used_features ] ) # postprocess # used_features_list_final2 = [] # unused_features_list2 = [] # for used, unused in zip(used_features_list_final,unused_features_list): # if len(used) == 3: # used_features_list_final2.append(used), unused_features_list2.append(unused) return used_features_list_final , unused_features_list","title":"get_nchoosek_combinations()"},{"location":"ref-domain/#bofire.domain.domain.Domain.preprocess_experiments_all_valid_outputs","text":"Method to get a dataframe where non-valid entries of all output feature are removed Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data None output_feature_keys Optional[List] List of output feature keys which should be considered for removal of invalid values. Defaults to None. None Returns: Type Description pd.DataFrame Dataframe with all experiments where only valid entries of the selected features are included Source code in bofire/domain/domain.py def preprocess_experiments_all_valid_outputs ( self , experiments : Optional [ pd . DataFrame ] = None , output_feature_keys : Optional [ List ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of all output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None. Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) if ( output_feature_keys is None ) or ( len ( output_feature_keys ) == 0 ): output_feature_keys = self . get_feature_keys ( OutputFeature ) else : for key in output_feature_keys : feat = self . get_feature ( key ) assert isinstance ( feat , OutputFeature ), f \"feat { key } is not an OutputFeature\" assert experiments is not None clean_exp = experiments . query ( \" & \" . join ([ \"(`valid_ %s ` > 0)\" % key for key in output_feature_keys ]) ) clean_exp = clean_exp . dropna ( subset = output_feature_keys ) return clean_exp","title":"preprocess_experiments_all_valid_outputs()"},{"location":"ref-domain/#bofire.domain.domain.Domain.preprocess_experiments_any_valid_output","text":"Method to get a dataframe where at least one output feature has a valid entry Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data None Returns: Type Description pd.DataFrame Dataframe with all experiments where at least one output feature has a valid entry Source code in bofire/domain/domain.py def preprocess_experiments_any_valid_output ( self , experiments : Optional [ pd . DataFrame ] = None ) -> pd . DataFrame : \"\"\"Method to get a dataframe where at least one output feature has a valid entry Args: experiments (pd.DataFrame): Dataframe with experimental data Returns: pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) output_feature_keys = self . get_feature_keys ( OutputFeature ) # clean_exp = experiments.query(\" or \".join([\"(valid_%s > 0)\" % key for key in output_feature_keys])) # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys])) assert experiments is not None clean_exp = experiments . query ( \" or \" . join ( [ \"((`valid_ %s ` >0) & ` %s `.notna())\" % ( key , key ) for key in output_feature_keys ] ) ) return clean_exp","title":"preprocess_experiments_any_valid_output()"},{"location":"ref-domain/#bofire.domain.domain.Domain.preprocess_experiments_one_valid_output","text":"Method to get a dataframe where non-valid entries of the provided output feature are removed Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data None output_feature_key str The feature based on which non-valid entries rows are removed required Returns: Type Description pd.DataFrame Dataframe with all experiments where only valid entries of the specific feature are included Source code in bofire/domain/domain.py def preprocess_experiments_one_valid_output ( self , output_feature_key : str , experiments : Optional [ pd . DataFrame ] = None , ) -> pd . DataFrame : \"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed Args: experiments (pd.DataFrame): Dataframe with experimental data output_feature_key (str): The feature based on which non-valid entries rows are removed Returns: pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included \"\"\" if experiments is None : if self . experiments is not None : experiments = self . experiments else : raise ValueError ( \"No experiments available for preprocessing.\" ) assert experiments is not None clean_exp = experiments . loc [ ( experiments [ \"valid_ %s \" % output_feature_key ] == 1 ) & ( experiments [ output_feature_key ] . notna ()) ] # clean_exp = clean_exp.dropna() return clean_exp","title":"preprocess_experiments_one_valid_output()"},{"location":"ref-domain/#bofire.domain.domain.Domain.to_config","text":"Serializables itself to a dictionary. Returns: Type Description Dict Serialized version of the domain as dictionary. Source code in bofire/domain/domain.py def to_config ( self ) -> Dict : \"\"\"Serializables itself to a dictionary. Returns: Dict: Serialized version of the domain as dictionary. \"\"\" assert isinstance ( self . input_features , InputFeatures ) assert isinstance ( self . output_features , OutputFeatures ) assert isinstance ( self . constraints , Constraints ) config : Dict [ str , Any ] = { \"input_features\" : self . input_features . to_config (), \"output_features\" : self . output_features . to_config (), \"constraints\" : self . constraints . to_config (), } if self . experiments is not None and self . num_experiments > 0 : config [ \"experiments\" ] = self . experiments . to_dict () if self . candidates is not None and self . num_candidates > 0 : config [ \"candidates\" ] = self . candidates . to_dict () return config","title":"to_config()"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_candidates","text":"Method to check the validty of porposed candidates Parameters: Name Type Description Default candidates pd.DataFrame Dataframe with suggested new experiments (candidates) required only_inputs bool,optional If True, only the input columns are validated. Defaults to False. False Exceptions: Type Description ValueError when a column is missing for a defined input feature ValueError when a column is missing for a defined output feature ValueError when a non-numerical value is proposed ValueError when the constraints are not fulfilled ValueError when an additional column is found Returns: Type Description pd.DataFrame dataframe with suggested experiments (candidates) Source code in bofire/domain/domain.py def validate_candidates ( self , candidates : pd . DataFrame , only_inputs : bool = False ) -> pd . DataFrame : \"\"\"Method to check the validty of porposed candidates Args: candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates) only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False. Raises: ValueError: when a column is missing for a defined input feature ValueError: when a column is missing for a defined output feature ValueError: when a non-numerical value is proposed ValueError: when the constraints are not fulfilled ValueError: when an additional column is found Returns: pd.DataFrame: dataframe with suggested experiments (candidates) \"\"\" # check that each input feature has a col and is valid in itself assert isinstance ( self . input_features , InputFeatures ) self . input_features . validate_inputs ( candidates ) # check if all constraints are fulfilled if not self . cnstrs . is_fulfilled ( candidates ) . all (): raise ValueError ( \"Constraints not fulfilled.\" ) # for each continuous output feature with an attached objective object if not only_inputs : assert isinstance ( self . output_features , OutputFeatures ) for key in self . output_features . get_keys_by_objective ( Objective ): # check that pred, sd, and des cols are specified and numerical for col in [ f \" { key } _pred\" , f \" { key } _sd\" , f \" { key } _des\" ]: if col not in candidates : raise ValueError ( \"missing column {col} \" ) if ( not is_numeric ( candidates [ col ])) and ( not candidates [ col ] . isnull () . to_numpy () . all () ): raise ValueError ( f \"not all values of output feature ` { key } ` are numerical\" ) # validate no additional cols exist if_count = len ( self . get_features ( InputFeature )) of_count = len ( self . output_features . get_keys_by_objective ( Objective )) # input features, prediction, standard deviation and reward for each output feature, 3 additional usefull infos: reward, aquisition function, strategy if len ( candidates . columns ) != if_count + 3 * of_count : raise ValueError ( \"additional columns found\" ) return candidates","title":"validate_candidates()"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_constraints","text":"Validate if all features included in the constraints are also defined as features for the domain. Parameters: Name Type Description Default v List[Constraint] List of constraints or empty if no constraints are defined required values List[InputFeature] List of input features of the domain required Exceptions: Type Description ValueError Feature key in constraint is unknown. Returns: Type Description List[Constraint] List of constraints defined for the domain Source code in bofire/domain/domain.py @validator ( \"constraints\" , always = True ) def validate_constraints ( cls , v , values ): \"\"\"Validate if all features included in the constraints are also defined as features for the domain. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: Feature key in constraint is unknown. Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v keys = [ f . key for f in values [ \"input_features\" ]] for c in v : if isinstance ( c , LinearConstraint ) or isinstance ( c , NChooseKConstraint ): for f in c . features : if f not in keys : raise ValueError ( f \"feature { f } in constraint unknown ( { keys } )\" ) return v","title":"validate_constraints()"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_experiments","text":"checks the experimental data on validity Parameters: Name Type Description Default experiments pd.DataFrame Dataframe with experimental data required Exceptions: Type Description ValueError empty dataframe ValueError the column for a specific feature is missing the provided data ValueError there are labcodes with null value ValueError there are labcodes with nan value ValueError labcodes are not unique ValueError the provided columns do no match to the defined domain ValueError the provided columns do no match to the defined domain ValueError inputFeature with null values ValueError inputFeature with nan values Returns: Type Description pd.DataFrame The provided dataframe with experimental data Source code in bofire/domain/domain.py def validate_experiments ( self , experiments : pd . DataFrame , strict : bool = False , ) -> pd . DataFrame : \"\"\"checks the experimental data on validity Args: experiments (pd.DataFrame): Dataframe with experimental data Raises: ValueError: empty dataframe ValueError: the column for a specific feature is missing the provided data ValueError: there are labcodes with null value ValueError: there are labcodes with nan value ValueError: labcodes are not unique ValueError: the provided columns do no match to the defined domain ValueError: the provided columns do no match to the defined domain ValueError: inputFeature with null values ValueError: inputFeature with nan values Returns: pd.DataFrame: The provided dataframe with experimental data \"\"\" if len ( experiments ) == 0 : raise ValueError ( \"no experiments provided (empty dataframe)\" ) # check that each feature is a col feature_keys = self . get_feature_keys () for feature_key in feature_keys : if feature_key not in experiments : raise ValueError ( f \"no col in experiments for feature { feature_key } \" ) # add valid_{key} cols if missing valid_keys = [ f \"valid_ { output_feature_key } \" for output_feature_key in self . get_feature_keys ( OutputFeature ) ] for valid_key in valid_keys : if valid_key not in experiments : experiments [ valid_key ] = True # check all cols expected = feature_keys + valid_keys cols = list ( experiments . columns ) # we allow here for a column named labcode used to identify experiments if \"labcode\" in cols : # test that labcodes are not na if experiments . labcode . isnull () . to_numpy () . any (): raise ValueError ( \"there are labcodes with null value\" ) if experiments . labcode . isna () . to_numpy () . any (): raise ValueError ( \"there are labcodes with nan value\" ) # test that labcodes are distinct if ( len ( set ( experiments . labcode . to_numpy () . tolist ())) != experiments . shape [ 0 ] ): raise ValueError ( \"labcodes are not unique\" ) # we remove the labcode from the cols list to proceed as before cols . remove ( \"labcode\" ) if len ( expected ) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) if len ( set ( expected + cols )) != len ( cols ): raise ValueError ( f \"expected the following cols: ` { expected } `, got ` { cols } `\" ) # check values of continuous input features if experiments [ self . get_feature_keys ( InputFeature )] . isnull () . to_numpy () . any (): raise ValueError ( \"there are null values\" ) if experiments [ self . get_feature_keys ( InputFeature )] . isna () . to_numpy () . any (): raise ValueError ( \"there are na values\" ) # run the individual validators for feat in self . get_features ( InputFeature ): assert isinstance ( feat , InputFeature ) feat . validate_experimental ( experiments [ feat . key ], strict = strict ) return experiments","title":"validate_experiments()"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_linear_constraints","text":"Validate if all features included in linear constraints are continuous ones. Parameters: Name Type Description Default v List[Constraint] List of constraints or empty if no constraints are defined required values List[InputFeature] List of input features of the domain required Exceptions: Type Description ValueError description Returns: Type Description List[Constraint] List of constraints defined for the domain Source code in bofire/domain/domain.py @validator ( \"constraints\" , always = True ) def validate_linear_constraints ( cls , v , values ): \"\"\"Validate if all features included in linear constraints are continuous ones. Args: v (List[Constraint]): List of constraints or empty if no constraints are defined values (List[InputFeature]): List of input features of the domain Raises: ValueError: _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" if \"input_features\" not in values : return v # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if non continuous input features appear in linear constraints for c in v : if isinstance ( c , LinearConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" return v","title":"validate_linear_constraints()"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_lower_bounds_in_nchoosek_constraints","text":"Validate the lower bound as well if the chosen number of allowed features is continuous. Parameters: Name Type Description Default v List[Constraint] List of all constraints defined for the domain required values List[InputFeature] description required Returns: Type Description List[Constraint] List of constraints defined for the domain Source code in bofire/domain/domain.py @validator ( \"constraints\" , always = True ) def validate_lower_bounds_in_nchoosek_constraints ( cls , v , values ): \"\"\"Validate the lower bound as well if the chosen number of allowed features is continuous. Args: v (List[Constraint]): List of all constraints defined for the domain values (List[InputFeature]): _description_ Returns: List[Constraint]: List of constraints defined for the domain \"\"\" # gather continuous input_features in dictionary continuous_input_features_dict = {} for f in values [ \"input_features\" ]: if type ( f ) is ContinuousInput : continuous_input_features_dict [ f . key ] = f # check if unfixed continuous features appearing in NChooseK constraints have lower bound of 0 for c in v : if isinstance ( c , NChooseKConstraint ): for f in c . features : assert ( f in continuous_input_features_dict ), f \" { f } must be continuous.\" assert ( continuous_input_features_dict [ f ] . lower_bound == 0 ), f \"lower bound of { f } must be 0 for NChooseK constraint.\" return v","title":"validate_lower_bounds_in_nchoosek_constraints()"},{"location":"ref-domain/#bofire.domain.domain.Domain.validate_unique_feature_keys","text":"Validates if provided input and output feature keys are unique Parameters: Name Type Description Default v OutputFeatures List of all output features of the domain. required value Dict[str, InputFeatures] Dict containing a list of input features as single entry. required Exceptions: Type Description ValueError Feature keys are not unique. Returns: Type Description OutputFeatures Keeps output features as given. Source code in bofire/domain/domain.py @validator ( \"output_features\" , always = True ) def validate_unique_feature_keys ( cls , v : OutputFeatures , values ) -> OutputFeatures : \"\"\"Validates if provided input and output feature keys are unique Args: v (OutputFeatures): List of all output features of the domain. value (Dict[str, InputFeatures]): Dict containing a list of input features as single entry. Raises: ValueError: Feature keys are not unique. Returns: OutputFeatures: Keeps output features as given. \"\"\" if \"input_features\" not in values : return v features = v + values [ \"input_features\" ] keys = [ f . key for f in features ] if len ( set ( keys )) != len ( keys ): raise ValueError ( \"feature keys are not unique\" ) return v","title":"validate_unique_feature_keys()"},{"location":"ref-domain/#bofire.domain.domain.DomainError","text":"A class defining a specific domain error Source code in bofire/domain/domain.py class DomainError ( Exception ): \"\"\"A class defining a specific domain error\"\"\" pass","title":"DomainError"},{"location":"ref-domain/#bofire.domain.domain.get_subdomain","text":"removes all features not defined as argument creating a subdomain of the provided domain Parameters: Name Type Description Default domain Domain the original domain wherefrom a subdomain should be created required feature_keys List List of features that shall be included in the subdomain required Exceptions: Type Description Assert when in total less than 2 features are provided ValueError when a provided feature key is not present in the provided domain Assert when no output feature is provided Assert when no input feature is provided ValueError description Returns: Type Description Domain A new domain containing only parts of the original domain Source code in bofire/domain/domain.py def get_subdomain ( domain : Domain , feature_keys : List , ) -> Domain : \"\"\"removes all features not defined as argument creating a subdomain of the provided domain Args: domain (Domain): the original domain wherefrom a subdomain should be created feature_keys (List): List of features that shall be included in the subdomain Raises: Assert: when in total less than 2 features are provided ValueError: when a provided feature key is not present in the provided domain Assert: when no output feature is provided Assert: when no input feature is provided ValueError: _description_ Returns: Domain: A new domain containing only parts of the original domain \"\"\" assert len ( feature_keys ) >= 2 , \"At least two features have to be provided.\" output_features = [] input_features = [] for key in feature_keys : try : feat = domain . get_feature ( key ) except KeyError : raise ValueError ( f \"Feature { key } not present in domain.\" ) if isinstance ( feat , InputFeature ): input_features . append ( feat ) else : output_features . append ( feat ) assert len ( output_features ) > 0 , \"At least one output feature has to be provided.\" assert len ( input_features ) > 0 , \"At least one input feature has to be provided.\" input_features = InputFeatures ( features = input_features ) # loop over constraints and make sure that all features used in constraints are in the input_feature_keys for c in domain . constraints : # TODO: fix type hint for key in c . features : # type: ignore if key not in input_features . get_keys (): raise ValueError ( f \"Removed input feature { key } is used in a constraint.\" ) subdomain = deepcopy ( domain ) subdomain . input_features = input_features subdomain . output_features = output_features return subdomain","title":"get_subdomain()"},{"location":"ref-features/","text":"Domain CategoricalDescriptorInput ( CategoricalInput ) pydantic-model Class for categorical input features with descriptors Attributes: Name Type Description categories List[str] Names of the categories. allowed List[bool] List of bools indicating if a category is allowed within the optimization. descriptors List[str] List of strings representing the names of the descriptors. calues List[List[float]] List of lists representing the descriptor values. Source code in bofire/domain/features.py class CategoricalDescriptorInput ( CategoricalInput ): \"\"\"Class for categorical input features with descriptors Attributes: categories (List[str]): Names of the categories. allowed (List[bool]): List of bools indicating if a category is allowed within the optimization. descriptors (List[str]): List of strings representing the names of the descriptors. calues (List[List[float]]): List of lists representing the descriptor values. \"\"\" descriptors : TDescriptors values : TCategoricalDescriptorVals @validator ( \"descriptors\" ) def validate_descriptors ( cls , descriptors ): \"\"\"validates that descriptors have unique names Args: categories (List[str]): List of descriptor names Raises: ValueError: when descriptors have non-unique names Returns: List[str]: List of the descriptors \"\"\" descriptors = [ name2key ( name ) for name in descriptors ] if len ( descriptors ) != len ( set ( descriptors )): raise ValueError ( \"descriptors must be unique\" ) return descriptors @validator ( \"values\" ) def validate_values ( cls , v , values ): \"\"\"validates the compatability of passed values for the descriptors and the defined categories Args: v (List[List[float]]): Nested list with descriptor values values (Dict): Dictionary with attributes Raises: ValueError: when values have different length than categories ValueError: when rows in values have different length than descriptors ValueError: when a descriptor shows no variance in the data Returns: List[List[float]]: Nested list with descriptor values \"\"\" if len ( v ) != len ( values [ \"categories\" ]): raise ValueError ( \"values must have same length as categories\" ) for row in v : if len ( row ) != len ( values [ \"descriptors\" ]): raise ValueError ( \"rows in values must have same length as descriptors\" ) a = np . array ( v ) for i , d in enumerate ( values [ \"descriptors\" ]): if len ( set ( a [:, i ])) == 1 : raise ValueError ( \"No variation for descriptor {d} .\" ) return v def to_df ( self ): \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" data = { cat : values for cat , values in zip ( self . categories , self . values )} return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors ) def get_real_descriptor_bounds ( self , values ) -> pd . DataFrame : \"\"\"Method to generate a dataFrame as tabular overview of lower and upper bounds of the descriptors (excluding non-allowed descriptors) Args: values (pd.Series): The categories present in the passed data for the considered feature Returns: pd.Series: Tabular overview of lower and upper bounds of the descriptors \"\"\" df = self . to_df () . loc [ self . get_possible_categories ( values )] data = { \"lower\" : [ min ( df [ desc ] . tolist ()) for desc in self . descriptors ], \"upper\" : [ max ( df [ desc ] . tolist ()) for desc in self . descriptors ], } return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors ) def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data ValueError: when no variation is present or planed for a given descriptor Returns: pd.Series: A dataFrame with experiments \"\"\" values = super () . validate_experimental ( values , strict ) if strict : bounds = self . get_real_descriptor_bounds ( values ) for desc in self . descriptors : if bounds . loc [ \"lower\" , desc ] == bounds . loc [ \"upper\" , desc ]: raise ValueError ( f \"No variation present or planned for descriptor { desc } for feature { self . key } . Remove the descriptor.\" ) return values @classmethod def from_df ( cls , key : str , df : pd . DataFrame ): \"\"\"Creates a feature from a dataframe Args: key (str): The name of the feature df (pd.DataFrame): Categories as rows and descriptors as columns Returns: _type_: _description_ \"\"\" return cls ( key = key , categories = list ( df . index ), allowed = [ True for _ in range ( len ( df ))], descriptors = list ( df . columns ), values = df . values . tolist (), ) from_df ( key , df ) classmethod Creates a feature from a dataframe Parameters: Name Type Description Default key str The name of the feature required df pd.DataFrame Categories as rows and descriptors as columns required Returns: Type Description _type_ description Source code in bofire/domain/features.py @classmethod def from_df ( cls , key : str , df : pd . DataFrame ): \"\"\"Creates a feature from a dataframe Args: key (str): The name of the feature df (pd.DataFrame): Categories as rows and descriptors as columns Returns: _type_: _description_ \"\"\" return cls ( key = key , categories = list ( df . index ), allowed = [ True for _ in range ( len ( df ))], descriptors = list ( df . columns ), values = df . values . tolist (), ) get_real_descriptor_bounds ( self , values ) Method to generate a dataFrame as tabular overview of lower and upper bounds of the descriptors (excluding non-allowed descriptors) Parameters: Name Type Description Default values pd.Series The categories present in the passed data for the considered feature required Returns: Type Description pd.Series Tabular overview of lower and upper bounds of the descriptors Source code in bofire/domain/features.py def get_real_descriptor_bounds ( self , values ) -> pd . DataFrame : \"\"\"Method to generate a dataFrame as tabular overview of lower and upper bounds of the descriptors (excluding non-allowed descriptors) Args: values (pd.Series): The categories present in the passed data for the considered feature Returns: pd.Series: Tabular overview of lower and upper bounds of the descriptors \"\"\" df = self . to_df () . loc [ self . get_possible_categories ( values )] data = { \"lower\" : [ min ( df [ desc ] . tolist ()) for desc in self . descriptors ], \"upper\" : [ max ( df [ desc ] . tolist ()) for desc in self . descriptors ], } return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors ) to_df ( self ) tabular overview of the feature as DataFrame Returns: Type Description pd.DataFrame tabular overview of the feature as DataFrame Source code in bofire/domain/features.py def to_df ( self ): \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" data = { cat : values for cat , values in zip ( self . categories , self . values )} return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors ) validate_descriptors ( descriptors ) classmethod validates that descriptors have unique names Parameters: Name Type Description Default categories List[str] List of descriptor names required Exceptions: Type Description ValueError when descriptors have non-unique names Returns: Type Description List[str] List of the descriptors Source code in bofire/domain/features.py @validator ( \"descriptors\" ) def validate_descriptors ( cls , descriptors ): \"\"\"validates that descriptors have unique names Args: categories (List[str]): List of descriptor names Raises: ValueError: when descriptors have non-unique names Returns: List[str]: List of the descriptors \"\"\" descriptors = [ name2key ( name ) for name in descriptors ] if len ( descriptors ) != len ( set ( descriptors )): raise ValueError ( \"descriptors must be unique\" ) return descriptors validate_experimental ( self , values , strict = False ) Method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Exceptions: Type Description ValueError when an entry is not in the list of allowed categories ValueError when there is no variation in a feature provided by the experimental data ValueError when no variation is present or planed for a given descriptor Returns: Type Description pd.Series A dataFrame with experiments Source code in bofire/domain/features.py def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data ValueError: when no variation is present or planed for a given descriptor Returns: pd.Series: A dataFrame with experiments \"\"\" values = super () . validate_experimental ( values , strict ) if strict : bounds = self . get_real_descriptor_bounds ( values ) for desc in self . descriptors : if bounds . loc [ \"lower\" , desc ] == bounds . loc [ \"upper\" , desc ]: raise ValueError ( f \"No variation present or planned for descriptor { desc } for feature { self . key } . Remove the descriptor.\" ) return values validate_values ( v , values ) classmethod validates the compatability of passed values for the descriptors and the defined categories Parameters: Name Type Description Default v List[List[float]] Nested list with descriptor values required values Dict Dictionary with attributes required Exceptions: Type Description ValueError when values have different length than categories ValueError when rows in values have different length than descriptors ValueError when a descriptor shows no variance in the data Returns: Type Description List[List[float]] Nested list with descriptor values Source code in bofire/domain/features.py @validator ( \"values\" ) def validate_values ( cls , v , values ): \"\"\"validates the compatability of passed values for the descriptors and the defined categories Args: v (List[List[float]]): Nested list with descriptor values values (Dict): Dictionary with attributes Raises: ValueError: when values have different length than categories ValueError: when rows in values have different length than descriptors ValueError: when a descriptor shows no variance in the data Returns: List[List[float]]: Nested list with descriptor values \"\"\" if len ( v ) != len ( values [ \"categories\" ]): raise ValueError ( \"values must have same length as categories\" ) for row in v : if len ( row ) != len ( values [ \"descriptors\" ]): raise ValueError ( \"rows in values must have same length as descriptors\" ) a = np . array ( v ) for i , d in enumerate ( values [ \"descriptors\" ]): if len ( set ( a [:, i ])) == 1 : raise ValueError ( \"No variation for descriptor {d} .\" ) return v CategoricalInput ( InputFeature ) pydantic-model Base class for all categorical input features. Attributes: Name Type Description categories List[str] Names of the categories. allowed List[bool] List of bools indicating if a category is allowed within the optimization. Source code in bofire/domain/features.py class CategoricalInput ( InputFeature ): \"\"\"Base class for all categorical input features. Attributes: categories (List[str]): Names of the categories. allowed (List[bool]): List of bools indicating if a category is allowed within the optimization. \"\"\" categories : TCategoryVals allowed : TAllowedVals = None @validator ( \"categories\" ) def validate_categories_unique ( cls , categories ): \"\"\"validates that categories have unique names Args: categories (List[str]): List of category names Raises: ValueError: when categories have non-unique names Returns: List[str]: List of the categories \"\"\" categories = [ name2key ( name ) for name in categories ] if len ( categories ) != len ( set ( categories )): raise ValueError ( \"categories must be unique\" ) return categories @root_validator ( pre = False ) def init_allowed ( cls , values ): \"\"\"validates the list of allowed/not allowed categories Args: values (Dict): Dictionary with attributes Raises: ValueError: when the number of allowences does not fit to the number of categories ValueError: when no category is allowed Returns: Dict: Dictionary with attributes \"\"\" if \"categories\" not in values or values [ \"categories\" ] is None : return values if \"allowed\" not in values or values [ \"allowed\" ] is None : values [ \"allowed\" ] = [ True for _ in range ( len ( values [ \"categories\" ]))] if len ( values [ \"allowed\" ]) != len ( values [ \"categories\" ]): raise ValueError ( \"allowed must have same length as categories\" ) if sum ( values [ \"allowed\" ]) == 0 : raise ValueError ( \"no category is allowed\" ) return values def is_fixed ( self ): \"\"\"Returns True if there is only one allowed category. Returns: [bool]: True if there is only one allowed category \"\"\" if self . allowed is None : return False return sum ( self . allowed ) == 1 def fixed_value ( self ): \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed Returns: List[str]: List of categories or None \"\"\" if self . is_fixed (): return self . get_allowed_categories ()[ 0 ] else : return None def get_allowed_categories ( self ): \"\"\"Returns the allowed categories. Returns: list of str: The allowed categories \"\"\" if self . allowed is None : return [] return [ c for c , a in zip ( self . categories , self . allowed ) if a ] def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if sum ( values . isin ( self . categories )) != len ( values ): raise ValueError ( f \"invalid values for ` { self . key } `, allowed are: ` { self . categories } `\" ) if strict : possible_categories = self . get_possible_categories ( values ) if len ( possible_categories ) != len ( self . categories ): raise ValueError ( f \"Categories { list ( set ( self . categories ) - set ( possible_categories )) } of feature { self . key } not used. Remove them.\" ) return values def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when not all values for a feature are one of the allowed categories Returns: pd.Series: The passed dataFrame with candidates \"\"\" if sum ( values . isin ( self . get_allowed_categories ())) != len ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are a valid allowed category from { self . get_allowed_categories () } \" ) return values def get_forbidden_categories ( self ): \"\"\"Returns the non-allowed categories Returns: List[str]: List of the non-allowed categories \"\"\" return list ( set ( self . categories ) - set ( self . get_allowed_categories ())) def get_possible_categories ( self , values : pd . Series ) -> list : \"\"\"Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization Args: values (pd.Series): Series with the values for this feature Returns: list: list of possible categories \"\"\" return sorted ( list ( set ( list ( set ( values . tolist ())) + self . get_allowed_categories ())) ) def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . get_allowed_categories (), n ) ) def __str__ ( self ) -> str : \"\"\"Returns the number of categories as str Returns: str: Number of categories \"\"\" return f \" { len ( self . categories ) } categories\" __str__ ( self ) special Returns the number of categories as str Returns: Type Description str Number of categories Source code in bofire/domain/features.py def __str__ ( self ) -> str : \"\"\"Returns the number of categories as str Returns: str: Number of categories \"\"\" return f \" { len ( self . categories ) } categories\" fixed_value ( self ) Returns the categories to which the feature is fixed, None if the feature is not fixed Returns: Type Description List[str] List of categories or None Source code in bofire/domain/features.py def fixed_value ( self ): \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed Returns: List[str]: List of categories or None \"\"\" if self . is_fixed (): return self . get_allowed_categories ()[ 0 ] else : return None get_allowed_categories ( self ) Returns the allowed categories. Returns: Type Description list of str The allowed categories Source code in bofire/domain/features.py def get_allowed_categories ( self ): \"\"\"Returns the allowed categories. Returns: list of str: The allowed categories \"\"\" if self . allowed is None : return [] return [ c for c , a in zip ( self . categories , self . allowed ) if a ] get_forbidden_categories ( self ) Returns the non-allowed categories Returns: Type Description List[str] List of the non-allowed categories Source code in bofire/domain/features.py def get_forbidden_categories ( self ): \"\"\"Returns the non-allowed categories Returns: List[str]: List of the non-allowed categories \"\"\" return list ( set ( self . categories ) - set ( self . get_allowed_categories ())) get_possible_categories ( self , values ) Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization Parameters: Name Type Description Default values pd.Series Series with the values for this feature required Returns: Type Description list list of possible categories Source code in bofire/domain/features.py def get_possible_categories ( self , values : pd . Series ) -> list : \"\"\"Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization Args: values (pd.Series): Series with the values for this feature Returns: list: list of possible categories \"\"\" return sorted ( list ( set ( list ( set ( values . tolist ())) + self . get_allowed_categories ())) ) init_allowed ( values ) classmethod validates the list of allowed/not allowed categories Parameters: Name Type Description Default values Dict Dictionary with attributes required Exceptions: Type Description ValueError when the number of allowences does not fit to the number of categories ValueError when no category is allowed Returns: Type Description Dict Dictionary with attributes Source code in bofire/domain/features.py @root_validator ( pre = False ) def init_allowed ( cls , values ): \"\"\"validates the list of allowed/not allowed categories Args: values (Dict): Dictionary with attributes Raises: ValueError: when the number of allowences does not fit to the number of categories ValueError: when no category is allowed Returns: Dict: Dictionary with attributes \"\"\" if \"categories\" not in values or values [ \"categories\" ] is None : return values if \"allowed\" not in values or values [ \"allowed\" ] is None : values [ \"allowed\" ] = [ True for _ in range ( len ( values [ \"categories\" ]))] if len ( values [ \"allowed\" ]) != len ( values [ \"categories\" ]): raise ValueError ( \"allowed must have same length as categories\" ) if sum ( values [ \"allowed\" ]) == 0 : raise ValueError ( \"no category is allowed\" ) return values is_fixed ( self ) Returns True if there is only one allowed category. Returns: Type Description [bool] True if there is only one allowed category Source code in bofire/domain/features.py def is_fixed ( self ): \"\"\"Returns True if there is only one allowed category. Returns: [bool]: True if there is only one allowed category \"\"\" if self . allowed is None : return False return sum ( self . allowed ) == 1 sample ( self , n ) Draw random samples from the feature. Parameters: Name Type Description Default n int number of samples. required Returns: Type Description pd.Series drawn samples. Source code in bofire/domain/features.py def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . get_allowed_categories (), n ) ) validate_candidental ( self , values ) Method to validate the suggested candidates Parameters: Name Type Description Default values pd.Series A dataFrame with candidates required Exceptions: Type Description ValueError when not all values for a feature are one of the allowed categories Returns: Type Description pd.Series The passed dataFrame with candidates Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when not all values for a feature are one of the allowed categories Returns: pd.Series: The passed dataFrame with candidates \"\"\" if sum ( values . isin ( self . get_allowed_categories ())) != len ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are a valid allowed category from { self . get_allowed_categories () } \" ) return values validate_categories_unique ( categories ) classmethod validates that categories have unique names Parameters: Name Type Description Default categories List[str] List of category names required Exceptions: Type Description ValueError when categories have non-unique names Returns: Type Description List[str] List of the categories Source code in bofire/domain/features.py @validator ( \"categories\" ) def validate_categories_unique ( cls , categories ): \"\"\"validates that categories have unique names Args: categories (List[str]): List of category names Raises: ValueError: when categories have non-unique names Returns: List[str]: List of the categories \"\"\" categories = [ name2key ( name ) for name in categories ] if len ( categories ) != len ( set ( categories )): raise ValueError ( \"categories must be unique\" ) return categories validate_experimental ( self , values , strict = False ) Method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Exceptions: Type Description ValueError when an entry is not in the list of allowed categories ValueError when there is no variation in a feature provided by the experimental data Returns: Type Description pd.Series A dataFrame with experiments Source code in bofire/domain/features.py def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if sum ( values . isin ( self . categories )) != len ( values ): raise ValueError ( f \"invalid values for ` { self . key } `, allowed are: ` { self . categories } `\" ) if strict : possible_categories = self . get_possible_categories ( values ) if len ( possible_categories ) != len ( self . categories ): raise ValueError ( f \"Categories { list ( set ( self . categories ) - set ( possible_categories )) } of feature { self . key } not used. Remove them.\" ) return values ContinuousDescriptorInput ( ContinuousInput ) pydantic-model Class for continuous input features with descriptors Attributes: Name Type Description lower_bound float Lower bound of the feature in the optimization. upper_bound float Upper bound of the feature in the optimization. descriptors List[str] Names of the descriptors. values List[float] Values of the descriptors. Source code in bofire/domain/features.py class ContinuousDescriptorInput ( ContinuousInput ): \"\"\"Class for continuous input features with descriptors Attributes: lower_bound (float): Lower bound of the feature in the optimization. upper_bound (float): Upper bound of the feature in the optimization. descriptors (List[str]): Names of the descriptors. values (List[float]): Values of the descriptors. \"\"\" descriptors : TDescriptors values : TDiscreteVals @validator ( \"descriptors\" ) def descriptors_to_keys ( cls , descriptors ): \"\"\"validates the descriptor names and transforms it to valid keys Args: descriptors (List[str]): List of descriptor names Returns: List[str]: List of valid keys \"\"\" return [ name2key ( name ) for name in descriptors ] @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"compares the length of the defined descriptors list with the provided values Args: values (Dict): Dictionary with all attribues Raises: ValueError: when the number of descriptors does not math the number of provided values Returns: Dict: Dict with the attributes \"\"\" if len ( values [ \"descriptors\" ]) != len ( values [ \"values\" ]): raise ValueError ( 'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}' ) return values def to_df ( self ) -> pd . DataFrame : \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" return pd . DataFrame ( data = [ self . values ], index = [ self . key ], columns = self . descriptors ) descriptors_to_keys ( descriptors ) classmethod validates the descriptor names and transforms it to valid keys Parameters: Name Type Description Default descriptors List[str] List of descriptor names required Returns: Type Description List[str] List of valid keys Source code in bofire/domain/features.py @validator ( \"descriptors\" ) def descriptors_to_keys ( cls , descriptors ): \"\"\"validates the descriptor names and transforms it to valid keys Args: descriptors (List[str]): List of descriptor names Returns: List[str]: List of valid keys \"\"\" return [ name2key ( name ) for name in descriptors ] to_df ( self ) tabular overview of the feature as DataFrame Returns: Type Description pd.DataFrame tabular overview of the feature as DataFrame Source code in bofire/domain/features.py def to_df ( self ) -> pd . DataFrame : \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" return pd . DataFrame ( data = [ self . values ], index = [ self . key ], columns = self . descriptors ) validate_list_lengths ( values ) classmethod compares the length of the defined descriptors list with the provided values Parameters: Name Type Description Default values Dict Dictionary with all attribues required Exceptions: Type Description ValueError when the number of descriptors does not math the number of provided values Returns: Type Description Dict Dict with the attributes Source code in bofire/domain/features.py @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"compares the length of the defined descriptors list with the provided values Args: values (Dict): Dictionary with all attribues Raises: ValueError: when the number of descriptors does not math the number of provided values Returns: Dict: Dict with the attributes \"\"\" if len ( values [ \"descriptors\" ]) != len ( values [ \"values\" ]): raise ValueError ( 'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}' ) return values ContinuousInput ( NumericalInputFeature ) pydantic-model Base class for all continuous input features. Attributes: Name Type Description lower_bound float Lower bound of the feature in the optimization. upper_bound float Upper bound of the feature in the optimization. Source code in bofire/domain/features.py class ContinuousInput ( NumericalInputFeature ): \"\"\"Base class for all continuous input features. Attributes: lower_bound (float): Lower bound of the feature in the optimization. upper_bound (float): Upper bound of the feature in the optimization. \"\"\" lower_bound : float upper_bound : float @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validates that the lower bound is lower than the upper bound Args: values (Dict): Dictionary with attributes key, lower and upper bound Raises: ValueError: when the lower bound is higher than the upper bound Returns: Dict: The attributes as dictionary \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when non numerical values are passed ValueError: when values are larger than the upper bound of the feature ValueError: when values are lower than the lower bound of the feature Returns: pd.Series: The passed dataFrame with candidates \"\"\" noise = 10e-8 super () . validate_candidental ( values ) if ( values < self . lower_bound - noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are larger than lower bound ` { self . lower_bound } ` \" ) if ( values > self . upper_bound + noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are smaller than upper bound ` { self . upper_bound } ` \" ) return values def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . uniform ( self . lower_bound , self . upper_bound , n ), ) def __str__ ( self ) -> str : \"\"\"Method to return a string of lower and upper bound Returns: str: String of a list with lower and upper bound \"\"\" return f \"[ { self . lower_bound } , { self . upper_bound } ]\" __str__ ( self ) special Method to return a string of lower and upper bound Returns: Type Description str String of a list with lower and upper bound Source code in bofire/domain/features.py def __str__ ( self ) -> str : \"\"\"Method to return a string of lower and upper bound Returns: str: String of a list with lower and upper bound \"\"\" return f \"[ { self . lower_bound } , { self . upper_bound } ]\" sample ( self , n ) Draw random samples from the feature. Parameters: Name Type Description Default n int number of samples. required Returns: Type Description pd.Series drawn samples. Source code in bofire/domain/features.py def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . uniform ( self . lower_bound , self . upper_bound , n ), ) validate_candidental ( self , values ) Method to validate the suggested candidates Parameters: Name Type Description Default values pd.Series A dataFrame with candidates required Exceptions: Type Description ValueError when non numerical values are passed ValueError when values are larger than the upper bound of the feature ValueError when values are lower than the lower bound of the feature Returns: Type Description pd.Series The passed dataFrame with candidates Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when non numerical values are passed ValueError: when values are larger than the upper bound of the feature ValueError: when values are lower than the lower bound of the feature Returns: pd.Series: The passed dataFrame with candidates \"\"\" noise = 10e-8 super () . validate_candidental ( values ) if ( values < self . lower_bound - noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are larger than lower bound ` { self . lower_bound } ` \" ) if ( values > self . upper_bound + noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are smaller than upper bound ` { self . upper_bound } ` \" ) return values validate_lower_upper ( values ) classmethod Validates that the lower bound is lower than the upper bound Parameters: Name Type Description Default values Dict Dictionary with attributes key, lower and upper bound required Exceptions: Type Description ValueError when the lower bound is higher than the upper bound Returns: Type Description Dict The attributes as dictionary Source code in bofire/domain/features.py @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validates that the lower bound is lower than the upper bound Args: values (Dict): Dictionary with attributes key, lower and upper bound Raises: ValueError: when the lower bound is higher than the upper bound Returns: Dict: The attributes as dictionary \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values ContinuousOutput ( OutputFeature ) pydantic-model The base class for a continuous output feature Attributes: Name Type Description objective objective objective of the feature indicating in which direction it should be optimzed. Defaults to MaximizeObjective . Source code in bofire/domain/features.py class ContinuousOutput ( OutputFeature ): \"\"\"The base class for a continuous output feature Attributes: objective (objective, optional): objective of the feature indicating in which direction it should be optimzed. Defaults to `MaximizeObjective`. \"\"\" objective : Optional [ Objective ] = Field ( default_factory = lambda : MaximizeObjective ( w = 1.0 ) ) def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" config : Dict [ str , Any ] = { \"type\" : self . __class__ . __name__ , \"key\" : self . key , } if self . objective is not None : config [ \"objective\" ] = self . objective . to_config () return config def plot ( self , lower : float , upper : float , experiments : Optional [ pd . DataFrame ] = None , plot_details : bool = True , line_options : Optional [ Dict ] = None , scatter_options : Optional [ Dict ] = None , label_options : Optional [ Dict ] = None , title_options : Optional [ Dict ] = None , ): \"\"\"Plot the assigned objective. Args: lower (float): lower bound for the plot upper (float): upper bound for the plot experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None. \"\"\" if self . objective is None : raise ValueError ( f \"No objective assigned for ContinuousOutputFeauture with key { self . key } .\" ) line_options = line_options or {} scatter_options = scatter_options or {} label_options = label_options or {} title_options = title_options or {} line_options [ \"color\" ] = line_options . get ( \"color\" , \"black\" ) scatter_options [ \"color\" ] = scatter_options . get ( \"color\" , \"red\" ) x = pd . Series ( np . linspace ( lower , upper , 5000 )) reward = self . objective . __call__ ( x ) fig , ax = plt . subplots () ax . plot ( x , reward , ** line_options ) # TODO: validate dataframe if experiments is not None : x_data = experiments . loc [ experiments [ self . key ] . notna (), self . key ] . values ax . scatter ( x_data , # type: ignore self . objective . __call__ ( x_data ), # type: ignore ** scatter_options , ) ax . set_title ( \"Objective %s \" % self . key , ** title_options ) ax . set_ylabel ( \"Objective\" , ** label_options ) ax . set_xlabel ( self . key , ** label_options ) if plot_details : ax = self . objective . plot_details ( ax = ax ) return fig , ax def __str__ ( self ) -> str : return \"ContinuousOutputFeature\" __str__ ( self ) special Return str(self). Source code in bofire/domain/features.py def __str__ ( self ) -> str : return \"ContinuousOutputFeature\" plot ( self , lower , upper , experiments = None , plot_details = True , line_options = None , scatter_options = None , label_options = None , title_options = None ) Plot the assigned objective. Parameters: Name Type Description Default lower float lower bound for the plot required upper float upper bound for the plot required experiments Optional[pd.DataFrame] If provided, scatter also the historical data in the plot. Defaults to None. None Source code in bofire/domain/features.py def plot ( self , lower : float , upper : float , experiments : Optional [ pd . DataFrame ] = None , plot_details : bool = True , line_options : Optional [ Dict ] = None , scatter_options : Optional [ Dict ] = None , label_options : Optional [ Dict ] = None , title_options : Optional [ Dict ] = None , ): \"\"\"Plot the assigned objective. Args: lower (float): lower bound for the plot upper (float): upper bound for the plot experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None. \"\"\" if self . objective is None : raise ValueError ( f \"No objective assigned for ContinuousOutputFeauture with key { self . key } .\" ) line_options = line_options or {} scatter_options = scatter_options or {} label_options = label_options or {} title_options = title_options or {} line_options [ \"color\" ] = line_options . get ( \"color\" , \"black\" ) scatter_options [ \"color\" ] = scatter_options . get ( \"color\" , \"red\" ) x = pd . Series ( np . linspace ( lower , upper , 5000 )) reward = self . objective . __call__ ( x ) fig , ax = plt . subplots () ax . plot ( x , reward , ** line_options ) # TODO: validate dataframe if experiments is not None : x_data = experiments . loc [ experiments [ self . key ] . notna (), self . key ] . values ax . scatter ( x_data , # type: ignore self . objective . __call__ ( x_data ), # type: ignore ** scatter_options , ) ax . set_title ( \"Objective %s \" % self . key , ** title_options ) ax . set_ylabel ( \"Objective\" , ** label_options ) ax . set_xlabel ( self . key , ** label_options ) if plot_details : ax = self . objective . plot_details ( ax = ax ) return fig , ax to_config ( self ) Generate serialized version of the feature. Returns: Type Description Dict Serialized version of the feature as dictionary. Source code in bofire/domain/features.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" config : Dict [ str , Any ] = { \"type\" : self . __class__ . __name__ , \"key\" : self . key , } if self . objective is not None : config [ \"objective\" ] = self . objective . to_config () return config DiscreteInput ( NumericalInputFeature ) pydantic-model Feature with discretized ordinal values allowed in the optimization. Attributes: Name Type Description key(str) key of the feature. values(List[float]) the discretized allowed values during the optimization. Source code in bofire/domain/features.py class DiscreteInput ( NumericalInputFeature ): \"\"\"Feature with discretized ordinal values allowed in the optimization. Attributes: key(str): key of the feature. values(List[float]): the discretized allowed values during the optimization. \"\"\" values : TDiscreteVals @validator ( \"values\" ) def validate_values_unique ( cls , values ): \"\"\"Validates that provided values are unique. Args: values (List[float]): List of values Raises: ValueError: when values are non-unique. Returns: List[values]: Sorted list of values \"\"\" if len ( values ) != len ( set ( values )): raise ValueError ( \"Discrete values must be unique\" ) return sorted ( values ) @property def lower_bound ( self ) -> float : \"\"\"Lower bound of the set of allowed values\"\"\" return min ( self . values ) @property def upper_bound ( self ) -> float : \"\"\"Upper bound of the set of allowed values\"\"\" return max ( self . values ) def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the provided candidates. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Raises error when one of the provided values is not contained in the list of allowed values. Returns: pd.Series: _uggested candidates for the feature \"\"\" super () . validate_candidental ( values ) if not np . isin ( values . to_numpy (), np . array ( self . values )) . all (): raise ValueError ( f \"Not allowed values in candidates for feature { self . key } .\" ) return values def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . values , n )) lower_bound : float property readonly Lower bound of the set of allowed values upper_bound : float property readonly Upper bound of the set of allowed values sample ( self , n ) Draw random samples from the feature. Parameters: Name Type Description Default n int number of samples. required Returns: Type Description pd.Series drawn samples. Source code in bofire/domain/features.py def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . values , n )) validate_candidental ( self , values ) Method to validate the provided candidates. Parameters: Name Type Description Default values pd.Series suggested candidates for the feature required Exceptions: Type Description ValueError Raises error when one of the provided values is not contained in the list of allowed values. Returns: Type Description pd.Series _uggested candidates for the feature Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the provided candidates. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Raises error when one of the provided values is not contained in the list of allowed values. Returns: pd.Series: _uggested candidates for the feature \"\"\" super () . validate_candidental ( values ) if not np . isin ( values . to_numpy (), np . array ( self . values )) . all (): raise ValueError ( f \"Not allowed values in candidates for feature { self . key } .\" ) return values validate_values_unique ( values ) classmethod Validates that provided values are unique. Parameters: Name Type Description Default values List[float] List of values required Exceptions: Type Description ValueError when values are non-unique. Returns: Type Description List[values] Sorted list of values Source code in bofire/domain/features.py @validator ( \"values\" ) def validate_values_unique ( cls , values ): \"\"\"Validates that provided values are unique. Args: values (List[float]): List of values Raises: ValueError: when values are non-unique. Returns: List[values]: Sorted list of values \"\"\" if len ( values ) != len ( set ( values )): raise ValueError ( \"Discrete values must be unique\" ) return sorted ( values ) Feature ( KeyModel ) pydantic-model The base class for all features. Source code in bofire/domain/features.py class Feature ( KeyModel ): \"\"\"The base class for all features.\"\"\" def __lt__ ( self , other ) -> bool : \"\"\" Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER) Args: other: The other class to compare to self Returns: bool: True if the other class is larger than self, else False \"\"\" # TODO: add order of base class to FEATURE_ORDER and remove type: ignore order_self = FEATURE_ORDER [ type ( self )] # type: ignore order_other = FEATURE_ORDER [ type ( other )] if order_self == order_other : return self . key < other . key else : return order_self < order_other def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } @staticmethod def from_config ( config : Dict ) -> \"Feature\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of a objective Returns: Objective: Instantiated objective of the type specified in the `config`. \"\"\" input_mapper = { \"ContinuousInput\" : ContinuousInput , \"DiscreteInput\" : DiscreteInput , \"CategoricalInput\" : CategoricalInput , \"CategoricalDescriptorInput\" : CategoricalDescriptorInput , \"ContinuousDescriptorInput\" : ContinuousDescriptorInput , } output_mapper = { \"ContinuousOutput\" : ContinuousOutput , } if config [ \"type\" ] in input_mapper . keys (): return input_mapper [ config [ \"type\" ]]( ** config ) else : if \"objective\" in config . keys (): obj = Objective . from_config ( config = config [ \"objective\" ]) else : obj = None return output_mapper [ config [ \"type\" ]]( key = config [ \"key\" ], objective = obj ) __lt__ ( self , other ) special Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER) Parameters: Name Type Description Default other The other class to compare to self required Returns: Type Description bool True if the other class is larger than self, else False Source code in bofire/domain/features.py def __lt__ ( self , other ) -> bool : \"\"\" Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER) Args: other: The other class to compare to self Returns: bool: True if the other class is larger than self, else False \"\"\" # TODO: add order of base class to FEATURE_ORDER and remove type: ignore order_self = FEATURE_ORDER [ type ( self )] # type: ignore order_other = FEATURE_ORDER [ type ( other )] if order_self == order_other : return self . key < other . key else : return order_self < order_other from_config ( config ) staticmethod Generate objective out of serialized version. Parameters: Name Type Description Default config Dict Serialized version of a objective required Returns: Type Description Objective Instantiated objective of the type specified in the config . Source code in bofire/domain/features.py @staticmethod def from_config ( config : Dict ) -> \"Feature\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of a objective Returns: Objective: Instantiated objective of the type specified in the `config`. \"\"\" input_mapper = { \"ContinuousInput\" : ContinuousInput , \"DiscreteInput\" : DiscreteInput , \"CategoricalInput\" : CategoricalInput , \"CategoricalDescriptorInput\" : CategoricalDescriptorInput , \"ContinuousDescriptorInput\" : ContinuousDescriptorInput , } output_mapper = { \"ContinuousOutput\" : ContinuousOutput , } if config [ \"type\" ] in input_mapper . keys (): return input_mapper [ config [ \"type\" ]]( ** config ) else : if \"objective\" in config . keys (): obj = Objective . from_config ( config = config [ \"objective\" ]) else : obj = None return output_mapper [ config [ \"type\" ]]( key = config [ \"key\" ], objective = obj ) to_config ( self ) Generate serialized version of the feature. Returns: Type Description Dict Serialized version of the feature as dictionary. Source code in bofire/domain/features.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } Features ( BaseModel ) pydantic-model Container of features, both input and output features are allowed. Attributes: Name Type Description features List(Features list of the features. Source code in bofire/domain/features.py class Features ( BaseModel ): \"\"\"Container of features, both input and output features are allowed. Attributes: features (List(Features)): list of the features. \"\"\" features : FeatureSequence = Field ( default_factory = lambda : []) def to_config ( self ) -> Dict : \"\"\"Serialize the features container. Returns: Dict: serialized features container \"\"\" return { \"type\" : \"general\" , \"features\" : [ feat . to_config () for feat in self . features ], } @staticmethod def from_config ( config : Dict ) -> \"Features\" : \"\"\"Instantiates a `Feature` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized features dictionary Returns: Features: instantiated features object \"\"\" if config [ \"type\" ] == \"inputs\" : return InputFeatures ( features = [ cast ( InputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"outputs\" : return OutputFeatures ( features = [ cast ( OutputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"general\" : return Features ( features = [ Feature . from_config ( feat ) for feat in config [ \"features\" ]] ) else : raise ValueError ( f \"Unknown type { config [ 'type' ] } provided.\" ) def __iter__ ( self ): return iter ( self . features ) def __len__ ( self ): return len ( self . features ) def __getitem__ ( self , i ): return self . features [ i ] def __add__ ( self , other : Union [ Sequence [ Feature ], Features ]): if isinstance ( other , Features ): other_feature_seq = other . features else : other_feature_seq = other new_feature_seq = list ( itertools . chain ( self . features , other_feature_seq )) def is_feats_of_type ( feats , ftype_collection , ftype_element ): return isinstance ( feats , ftype_collection ) or ( not isinstance ( feats , Features ) and ( len ( feats ) > 0 and isinstance ( feats [ 0 ], ftype_element )) ) def is_infeats ( feats ): return is_feats_of_type ( feats , InputFeatures , InputFeature ) def is_outfeats ( feats ): return is_feats_of_type ( feats , OutputFeatures , OutputFeature ) if is_infeats ( self ) and is_infeats ( other ): return InputFeatures ( features = cast ( Tuple [ InputFeature , ... ], new_feature_seq ) ) if is_outfeats ( self ) and is_outfeats ( other ): return OutputFeatures ( features = cast ( Tuple [ OutputFeature , ... ], new_feature_seq ) ) return Features ( features = new_feature_seq ) def get_by_key ( self , key : str ) -> Feature : \"\"\"Get a feature by its key. Args: key (str): Feature key of the feature of interest Returns: Feature: Feature of interest \"\"\" return { f . key : f for f in self . features }[ key ] def get ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" return self . __class__ ( features = sorted ( filter_by_class ( self . features , includes = includes , excludes = excludes , exact = exact , ) ) ) def get_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get ( includes = includes , excludes = excludes , exact = exact , ) ] from_config ( config ) staticmethod Instantiates a Feature object from a dictionary created by the to_config method. Parameters: Name Type Description Default config Dict Serialized features dictionary required Returns: Type Description Features instantiated features object Source code in bofire/domain/features.py @staticmethod def from_config ( config : Dict ) -> \"Features\" : \"\"\"Instantiates a `Feature` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized features dictionary Returns: Features: instantiated features object \"\"\" if config [ \"type\" ] == \"inputs\" : return InputFeatures ( features = [ cast ( InputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"outputs\" : return OutputFeatures ( features = [ cast ( OutputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"general\" : return Features ( features = [ Feature . from_config ( feat ) for feat in config [ \"features\" ]] ) else : raise ValueError ( f \"Unknown type { config [ 'type' ] } provided.\" ) get ( self , includes =< class ' bofire . domain . features . Feature '>, excludes=None, exact=False) get features of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False by_attribute str If set it is filtered by the attribute specified in by by_attribute . Defaults to None. required Returns: Type Description List[Feature] List of features in the domain fitting to the passed requirements. Source code in bofire/domain/features.py def get ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" return self . __class__ ( features = sorted ( filter_by_class ( self . features , includes = includes , excludes = excludes , exact = exact , ) ) ) get_by_key ( self , key ) Get a feature by its key. Parameters: Name Type Description Default key str Feature key of the feature of interest required Returns: Type Description Feature Feature of interest Source code in bofire/domain/features.py def get_by_key ( self , key : str ) -> Feature : \"\"\"Get a feature by its key. Args: key (str): Feature key of the feature of interest Returns: Feature: Feature of interest \"\"\" return { f . key : f for f in self . features }[ key ] get_keys ( self , includes =< class ' bofire . domain . features . Feature '>, excludes=None, exact=False) Method to get feature keys of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[str] List of feature keys fitting to the passed requirements. Source code in bofire/domain/features.py def get_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get ( includes = includes , excludes = excludes , exact = exact , ) ] to_config ( self ) Serialize the features container. Returns: Type Description Dict serialized features container Source code in bofire/domain/features.py def to_config ( self ) -> Dict : \"\"\"Serialize the features container. Returns: Dict: serialized features container \"\"\" return { \"type\" : \"general\" , \"features\" : [ feat . to_config () for feat in self . features ], } InputFeature ( Feature ) pydantic-model Base class for all input features. Source code in bofire/domain/features.py class InputFeature ( Feature ): \"\"\"Base class for all input features.\"\"\" @abstractmethod def is_fixed () -> bool : \"\"\"Indicates if a variable is set to a fixed value. Returns: bool: True if fixed, els False. \"\"\" pass @abstractmethod def fixed_value () -> Union [ None , str , float ]: \"\"\"Method to return the fixed value in case of a fixed feature. Returns: Union[None,str,float]: None in case the feature is not fixed, else the fixed value. \"\"\" pass @abstractmethod def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Abstract method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Returns: pd.Series: The passed dataFrame with experiments \"\"\" pass @abstractmethod def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Abstract method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Returns: pd.Series: The passed dataFrame with candidates \"\"\" pass @abstractmethod def sample ( self , n : int ) -> pd . Series : \"\"\"Sample a series of allowed values. Args: n (int): Number of samples Returns: pd.Series: Sampled values. \"\"\" pass fixed_value () Method to return the fixed value in case of a fixed feature. Returns: Type Description Union[None,str,float] None in case the feature is not fixed, else the fixed value. Source code in bofire/domain/features.py @abstractmethod def fixed_value () -> Union [ None , str , float ]: \"\"\"Method to return the fixed value in case of a fixed feature. Returns: Union[None,str,float]: None in case the feature is not fixed, else the fixed value. \"\"\" pass is_fixed () Indicates if a variable is set to a fixed value. Returns: Type Description bool True if fixed, els False. Source code in bofire/domain/features.py @abstractmethod def is_fixed () -> bool : \"\"\"Indicates if a variable is set to a fixed value. Returns: bool: True if fixed, els False. \"\"\" pass sample ( self , n ) Sample a series of allowed values. Parameters: Name Type Description Default n int Number of samples required Returns: Type Description pd.Series Sampled values. Source code in bofire/domain/features.py @abstractmethod def sample ( self , n : int ) -> pd . Series : \"\"\"Sample a series of allowed values. Args: n (int): Number of samples Returns: pd.Series: Sampled values. \"\"\" pass validate_candidental ( self , values ) Abstract method to validate the suggested candidates Parameters: Name Type Description Default values pd.Series A dataFrame with candidates required Returns: Type Description pd.Series The passed dataFrame with candidates Source code in bofire/domain/features.py @abstractmethod def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Abstract method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Returns: pd.Series: The passed dataFrame with candidates \"\"\" pass validate_experimental ( self , values , strict = False ) Abstract method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Returns: Type Description pd.Series The passed dataFrame with experiments Source code in bofire/domain/features.py @abstractmethod def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Abstract method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Returns: pd.Series: The passed dataFrame with experiments \"\"\" pass InputFeatures ( Features ) pydantic-model Container of input features, only input features are allowed. Attributes: Name Type Description features List(InputFeatures list of the features. Source code in bofire/domain/features.py class InputFeatures ( Features ): \"\"\"Container of input features, only input features are allowed. Attributes: features (List(InputFeatures)): list of the features. \"\"\" features : Sequence [ InputFeature ] = Field ( default_factory = lambda : []) def to_config ( self ) -> Dict : return { \"type\" : \"inputs\" , \"features\" : [ feat . to_config () for feat in self . features ], } def get_fixed ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if feat . is_fixed ()]) # type: ignore def get_free ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are not fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only non-fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if not feat . is_fixed ()]) # type: ignore @validate_arguments def sample ( self , n : Tnum_samples = 1 , method : SamplingMethodEnum = SamplingMethodEnum . UNIFORM , ) -> pd . DataFrame : \"\"\"Draw sobol samples Args: n (int, optional): Number of samples, has to be larger than 0. Defaults to 1. method (SamplingMethodEnum, optional): Method to use, implemented methods are `UNIFORM`, `SOBOL` and `LHS`. Defaults to `UNIFORM`. Returns: pd.DataFrame: Dataframe containing the samples. \"\"\" if method == SamplingMethodEnum . UNIFORM : return self . validate_inputs ( pd . concat ([ feat . sample ( n ) for feat in self . get ( InputFeature )], axis = 1 ) # type: ignore ) free_features = self . get_free () if method == SamplingMethodEnum . SOBOL : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( len ( free_features )) . random ( n ) else : X = LatinHypercube ( len ( free_features )) . random ( n ) res = [] for i , feat in enumerate ( free_features ): if isinstance ( feat , ContinuousInput ): x = feat . from_unit_range ( X [:, i ]) elif isinstance ( feat , ( DiscreteInput , CategoricalInput )): if isinstance ( feat , DiscreteInput ): levels = feat . values else : levels = feat . get_allowed_categories () bins = np . linspace ( 0 , 1 , len ( levels ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( levels )[ idx ] else : raise ( ValueError ( f \"Unknown input feature with key { feat . key } \" )) res . append ( pd . Series ( x , name = feat . key )) samples = pd . concat ( res , axis = 1 ) for feat in self . get_fixed (): samples [ feat . key ] = feat . fixed_value () # type: ignore return self . validate_inputs ( samples )[ self . get_keys ( InputFeature )] def validate_inputs ( self , inputs : pd . DataFrame ) -> pd . DataFrame : \"\"\"Validate a pandas dataframe with input feature values. Args: inputs (pd.Dataframe): Inputs to validate. Raises: ValueError: Raises a Valueerror if a feature based validation raises an exception. Returns: pd.Dataframe: Validated dataframe \"\"\" for feature in self : if feature . key not in inputs : raise ValueError ( f \"no col for input feature ` { feature . key } `\" ) feature . validate_candidental ( inputs [ feature . key ]) # type: ignore return inputs def get_categorical_combinations ( self , include : Type [ Feature ] = InputFeature , exclude : Optional [ Type [ InputFeature ]] = None , ): \"\"\"get a list of tuples pairing the feature keys with a list of valid categories Args: include (Feature, optional): Features to be included. Defaults to InputFeature. exclude (Feature, optional): Features to be excluded, e.g. subclasses of the included features. Defaults to None. Returns: List[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str) \"\"\" features = [ f for f in self . get ( includes = include , excludes = exclude ) if isinstance ( f , CategoricalInput ) and not f . is_fixed () ] list_of_lists = [ [( f . key , cat ) for cat in f . get_allowed_categories ()] for f in features ] return list ( itertools . product ( * list_of_lists )) get_categorical_combinations ( self , include =< class ' bofire . domain . features . InputFeature '>, exclude=None) get a list of tuples pairing the feature keys with a list of valid categories Parameters: Name Type Description Default include Feature Features to be included. Defaults to InputFeature. <class 'bofire.domain.features.InputFeature'> exclude Feature Features to be excluded, e.g. subclasses of the included features. Defaults to None. None Returns: Type Description List[(str, List[str])] Returns a list of tuples pairing the feature keys with a list of valid categories (str) Source code in bofire/domain/features.py def get_categorical_combinations ( self , include : Type [ Feature ] = InputFeature , exclude : Optional [ Type [ InputFeature ]] = None , ): \"\"\"get a list of tuples pairing the feature keys with a list of valid categories Args: include (Feature, optional): Features to be included. Defaults to InputFeature. exclude (Feature, optional): Features to be excluded, e.g. subclasses of the included features. Defaults to None. Returns: List[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str) \"\"\" features = [ f for f in self . get ( includes = include , excludes = exclude ) if isinstance ( f , CategoricalInput ) and not f . is_fixed () ] list_of_lists = [ [( f . key , cat ) for cat in f . get_allowed_categories ()] for f in features ] return list ( itertools . product ( * list_of_lists )) get_fixed ( self ) Gets all features in self that are fixed and returns them as new InputFeatures object. Returns: Type Description InputFeatures Input features object containing only fixed features. Source code in bofire/domain/features.py def get_fixed ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if feat . is_fixed ()]) # type: ignore get_free ( self ) Gets all features in self that are not fixed and returns them as new InputFeatures object. Returns: Type Description InputFeatures Input features object containing only non-fixed features. Source code in bofire/domain/features.py def get_free ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are not fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only non-fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if not feat . is_fixed ()]) # type: ignore sample ( self , n = 1 , method =< SamplingMethodEnum . UNIFORM : 'UNIFORM' > ) Draw sobol samples Parameters: Name Type Description Default n int Number of samples, has to be larger than 0. Defaults to 1. 1 method SamplingMethodEnum Method to use, implemented methods are UNIFORM , SOBOL and LHS . Defaults to UNIFORM . <SamplingMethodEnum.UNIFORM: 'UNIFORM'> Returns: Type Description pd.DataFrame Dataframe containing the samples. Source code in bofire/domain/features.py @validate_arguments def sample ( self , n : Tnum_samples = 1 , method : SamplingMethodEnum = SamplingMethodEnum . UNIFORM , ) -> pd . DataFrame : \"\"\"Draw sobol samples Args: n (int, optional): Number of samples, has to be larger than 0. Defaults to 1. method (SamplingMethodEnum, optional): Method to use, implemented methods are `UNIFORM`, `SOBOL` and `LHS`. Defaults to `UNIFORM`. Returns: pd.DataFrame: Dataframe containing the samples. \"\"\" if method == SamplingMethodEnum . UNIFORM : return self . validate_inputs ( pd . concat ([ feat . sample ( n ) for feat in self . get ( InputFeature )], axis = 1 ) # type: ignore ) free_features = self . get_free () if method == SamplingMethodEnum . SOBOL : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( len ( free_features )) . random ( n ) else : X = LatinHypercube ( len ( free_features )) . random ( n ) res = [] for i , feat in enumerate ( free_features ): if isinstance ( feat , ContinuousInput ): x = feat . from_unit_range ( X [:, i ]) elif isinstance ( feat , ( DiscreteInput , CategoricalInput )): if isinstance ( feat , DiscreteInput ): levels = feat . values else : levels = feat . get_allowed_categories () bins = np . linspace ( 0 , 1 , len ( levels ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( levels )[ idx ] else : raise ( ValueError ( f \"Unknown input feature with key { feat . key } \" )) res . append ( pd . Series ( x , name = feat . key )) samples = pd . concat ( res , axis = 1 ) for feat in self . get_fixed (): samples [ feat . key ] = feat . fixed_value () # type: ignore return self . validate_inputs ( samples )[ self . get_keys ( InputFeature )] to_config ( self ) Serialize the features container. Returns: Type Description Dict serialized features container Source code in bofire/domain/features.py def to_config ( self ) -> Dict : return { \"type\" : \"inputs\" , \"features\" : [ feat . to_config () for feat in self . features ], } validate_inputs ( self , inputs ) Validate a pandas dataframe with input feature values. Parameters: Name Type Description Default inputs pd.Dataframe Inputs to validate. required Exceptions: Type Description ValueError Raises a Valueerror if a feature based validation raises an exception. Returns: Type Description pd.Dataframe Validated dataframe Source code in bofire/domain/features.py def validate_inputs ( self , inputs : pd . DataFrame ) -> pd . DataFrame : \"\"\"Validate a pandas dataframe with input feature values. Args: inputs (pd.Dataframe): Inputs to validate. Raises: ValueError: Raises a Valueerror if a feature based validation raises an exception. Returns: pd.Dataframe: Validated dataframe \"\"\" for feature in self : if feature . key not in inputs : raise ValueError ( f \"no col for input feature ` { feature . key } `\" ) feature . validate_candidental ( inputs [ feature . key ]) # type: ignore return inputs NumericalInputFeature ( InputFeature ) pydantic-model Abstracht base class for all numerical (ordinal) input features. Source code in bofire/domain/features.py class NumericalInputFeature ( InputFeature ): \"\"\"Abstracht base class for all numerical (ordinal) input features.\"\"\" def to_unit_range ( self , values : Union [ pd . Series , np . ndarray ], use_real_bounds : bool = False ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert to the unit range between 0 and 1. Args: values (pd.Series): values to be transformed use_real_bounds (bool, optional): if True, use the bounds from the actual values else the bounds from the feature. Defaults to False. Raises: ValueError: If lower_bound == upper bound an error is raised Returns: pd.Series: transformed values. \"\"\" if use_real_bounds : lower , upper = self . get_real_feature_bounds ( values ) else : lower , upper = self . lower_bound , self . upper_bound # type: ignore if lower == upper : raise ValueError ( \"Fixed feature cannot be transformed to unit range.\" ) valrange = upper - lower return ( values - lower ) / valrange def from_unit_range ( self , values : Union [ pd . Series , np . ndarray ] ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert from unit range. Args: values (pd.Series): values to transform from. Raises: ValueError: if the feature is fixed raise a value error. Returns: pd.Series: _description_ \"\"\" if self . is_fixed (): raise ValueError ( \"Fixed feature cannot be transformed from unit range.\" ) valrange = self . upper_bound - self . lower_bound # type: ignore return ( values * valrange ) + self . lower_bound # type: ignore def is_fixed ( self ): \"\"\"Method to check if the feature is fixed Returns: Boolean: True when the feature is fixed, false otherwise. \"\"\" # TODO: the bounds are declared in the derived classes, hence the type checks fail here :(. return self . lower_bound == self . upper_bound # type: ignore def fixed_value ( self ): \"\"\"Method to get the value to which the feature is fixed Returns: Float: Return the feature value or None if the feature is not fixed. \"\"\" if self . is_fixed (): return self . lower_bound # type: ignore else : return None def validate_experimental ( self , values : pd . Series , strict = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when a value is not numerical ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) if strict : lower , upper = self . get_real_feature_bounds ( values ) if lower == upper : raise ValueError ( f \"No variation present or planned for feature { self . key } . Remove it.\" ) return values def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Validate the suggested candidates for the feature. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Error is raised when one of the values is not numerical. Returns: pd.Series: the original provided candidates \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) return values def get_real_feature_bounds ( self , values : Union [ pd . Series , np . ndarray ] ) -> Tuple [ float , float ]: \"\"\"Method to extract the feature boundaries from the provided experimental data Args: values (pd.Series): Experimental data Returns: (float, float): Returns lower and upper bound based on the passed data \"\"\" lower = min ( self . lower_bound , values . min ()) # type: ignore upper = max ( self . upper_bound , values . max ()) # type: ignore return lower , upper fixed_value ( self ) Method to get the value to which the feature is fixed Returns: Type Description Float Return the feature value or None if the feature is not fixed. Source code in bofire/domain/features.py def fixed_value ( self ): \"\"\"Method to get the value to which the feature is fixed Returns: Float: Return the feature value or None if the feature is not fixed. \"\"\" if self . is_fixed (): return self . lower_bound # type: ignore else : return None from_unit_range ( self , values ) Convert from unit range. Parameters: Name Type Description Default values pd.Series values to transform from. required Exceptions: Type Description ValueError if the feature is fixed raise a value error. Returns: Type Description pd.Series description Source code in bofire/domain/features.py def from_unit_range ( self , values : Union [ pd . Series , np . ndarray ] ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert from unit range. Args: values (pd.Series): values to transform from. Raises: ValueError: if the feature is fixed raise a value error. Returns: pd.Series: _description_ \"\"\" if self . is_fixed (): raise ValueError ( \"Fixed feature cannot be transformed from unit range.\" ) valrange = self . upper_bound - self . lower_bound # type: ignore return ( values * valrange ) + self . lower_bound # type: ignore get_real_feature_bounds ( self , values ) Method to extract the feature boundaries from the provided experimental data Parameters: Name Type Description Default values pd.Series Experimental data required Returns: Type Description (float, float) Returns lower and upper bound based on the passed data Source code in bofire/domain/features.py def get_real_feature_bounds ( self , values : Union [ pd . Series , np . ndarray ] ) -> Tuple [ float , float ]: \"\"\"Method to extract the feature boundaries from the provided experimental data Args: values (pd.Series): Experimental data Returns: (float, float): Returns lower and upper bound based on the passed data \"\"\" lower = min ( self . lower_bound , values . min ()) # type: ignore upper = max ( self . upper_bound , values . max ()) # type: ignore return lower , upper is_fixed ( self ) Method to check if the feature is fixed Returns: Type Description Boolean True when the feature is fixed, false otherwise. Source code in bofire/domain/features.py def is_fixed ( self ): \"\"\"Method to check if the feature is fixed Returns: Boolean: True when the feature is fixed, false otherwise. \"\"\" # TODO: the bounds are declared in the derived classes, hence the type checks fail here :(. return self . lower_bound == self . upper_bound # type: ignore to_unit_range ( self , values , use_real_bounds = False ) Convert to the unit range between 0 and 1. Parameters: Name Type Description Default values pd.Series values to be transformed required use_real_bounds bool if True, use the bounds from the actual values else the bounds from the feature. Defaults to False. False Exceptions: Type Description ValueError If lower_bound == upper bound an error is raised Returns: Type Description pd.Series transformed values. Source code in bofire/domain/features.py def to_unit_range ( self , values : Union [ pd . Series , np . ndarray ], use_real_bounds : bool = False ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert to the unit range between 0 and 1. Args: values (pd.Series): values to be transformed use_real_bounds (bool, optional): if True, use the bounds from the actual values else the bounds from the feature. Defaults to False. Raises: ValueError: If lower_bound == upper bound an error is raised Returns: pd.Series: transformed values. \"\"\" if use_real_bounds : lower , upper = self . get_real_feature_bounds ( values ) else : lower , upper = self . lower_bound , self . upper_bound # type: ignore if lower == upper : raise ValueError ( \"Fixed feature cannot be transformed to unit range.\" ) valrange = upper - lower return ( values - lower ) / valrange validate_candidental ( self , values ) Validate the suggested candidates for the feature. Parameters: Name Type Description Default values pd.Series suggested candidates for the feature required Exceptions: Type Description ValueError Error is raised when one of the values is not numerical. Returns: Type Description pd.Series the original provided candidates Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Validate the suggested candidates for the feature. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Error is raised when one of the values is not numerical. Returns: pd.Series: the original provided candidates \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) return values validate_experimental ( self , values , strict = False ) Method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Exceptions: Type Description ValueError when a value is not numerical ValueError when there is no variation in a feature provided by the experimental data Returns: Type Description pd.Series A dataFrame with experiments Source code in bofire/domain/features.py def validate_experimental ( self , values : pd . Series , strict = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when a value is not numerical ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) if strict : lower , upper = self . get_real_feature_bounds ( values ) if lower == upper : raise ValueError ( f \"No variation present or planned for feature { self . key } . Remove it.\" ) return values OutputFeature ( Feature ) pydantic-model Base class for all output features. Attributes: Name Type Description key(str) Key of the Feature. Source code in bofire/domain/features.py class OutputFeature ( Feature ): \"\"\"Base class for all output features. Attributes: key(str): Key of the Feature. \"\"\" objective : Optional [ Objective ] OutputFeatures ( Features ) pydantic-model Container of output features, only output features are allowed. Attributes: Name Type Description features List(OutputFeatures list of the features. Source code in bofire/domain/features.py class OutputFeatures ( Features ): \"\"\"Container of output features, only output features are allowed. Attributes: features (List(OutputFeatures)): list of the features. \"\"\" features : Sequence [ OutputFeature ] = Field ( default_factory = lambda : []) def to_config ( self ) -> Dict : return { \"type\" : \"outputs\" , \"features\" : [ feat . to_config () for feat in self . features ], } @validator ( \"features\" , pre = True ) def validate_output_features ( cls , v , values ): for feat in v : if not isinstance ( feat , OutputFeature ): raise ValueError return v def get_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> \"OutputFeatures\" : \"\"\"Get output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[OutputFeature]: List of output features fitting to the passed requirements. \"\"\" if len ( self . features ) == 0 : return OutputFeatures ( features = []) else : # TODO: why only continuous output? return OutputFeatures ( features = sorted ( filter_by_attribute ( self . get ( ContinuousOutput ) . features , lambda of : of . objective , includes , excludes , exact , ) ) ) def get_keys_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Get keys of output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of output feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_by_objective ( includes , excludes , exact )] def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective for every Args: experiments (pd.DataFrame): Experiments for which the objectives should be evaluated. Returns: pd.DataFrame: Objective values for the experiments of interest. \"\"\" return pd . concat ( [ feat . objective ( experiments [[ feat . key ]]) # type: ignore for feat in self . features if feat . objective is not None ], axis = 1 , ) __call__ ( self , experiments ) special Evaluate the objective for every Parameters: Name Type Description Default experiments pd.DataFrame Experiments for which the objectives should be evaluated. required Returns: Type Description pd.DataFrame Objective values for the experiments of interest. Source code in bofire/domain/features.py def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective for every Args: experiments (pd.DataFrame): Experiments for which the objectives should be evaluated. Returns: pd.DataFrame: Objective values for the experiments of interest. \"\"\" return pd . concat ( [ feat . objective ( experiments [[ feat . key ]]) # type: ignore for feat in self . features if feat . objective is not None ], axis = 1 , ) get_by_objective ( self , includes =< class ' bofire . domain . objectives . Objective '>, excludes=None, exact=False) Get output features filtered by the type of the attached objective. Parameters: Name Type Description Default includes Union[List[TObjective], TObjective] Objective class or list of objective classes to be returned. Defaults to Objective. <class 'bofire.domain.objectives.Objective'> excludes Union[List[TObjective], TObjective, None] Objective class or list of specific objective classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[OutputFeature] List of output features fitting to the passed requirements. Source code in bofire/domain/features.py def get_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> \"OutputFeatures\" : \"\"\"Get output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[OutputFeature]: List of output features fitting to the passed requirements. \"\"\" if len ( self . features ) == 0 : return OutputFeatures ( features = []) else : # TODO: why only continuous output? return OutputFeatures ( features = sorted ( filter_by_attribute ( self . get ( ContinuousOutput ) . features , lambda of : of . objective , includes , excludes , exact , ) ) ) get_keys_by_objective ( self , includes =< class ' bofire . domain . objectives . Objective '>, excludes=None, exact=False) Get keys of output features filtered by the type of the attached objective. Parameters: Name Type Description Default includes Union[List[TObjective], TObjective] Objective class or list of objective classes to be returned. Defaults to Objective. <class 'bofire.domain.objectives.Objective'> excludes Union[List[TObjective], TObjective, None] Objective class or list of specific objective classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[str] List of output feature keys fitting to the passed requirements. Source code in bofire/domain/features.py def get_keys_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Get keys of output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of output feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_by_objective ( includes , excludes , exact )] to_config ( self ) Serialize the features container. Returns: Type Description Dict serialized features container Source code in bofire/domain/features.py def to_config ( self ) -> Dict : return { \"type\" : \"outputs\" , \"features\" : [ feat . to_config () for feat in self . features ], } is_continuous ( var ) Checks if Feature is continous Parameters: Name Type Description Default var Feature Feature to be checked required Returns: Type Description bool True if continuous, else False Source code in bofire/domain/features.py def is_continuous ( var : Feature ) -> bool : \"\"\"Checks if Feature is continous Args: var (Feature): Feature to be checked Returns: bool: True if continuous, else False \"\"\" # TODO: generalize query via attribute continuousFeature (not existing yet!) if isinstance ( var , ContinuousInput ) or isinstance ( var , ContinuousOutput ): return True else : return False","title":"Features"},{"location":"ref-features/#domain","text":"","title":"Domain"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput","text":"Class for categorical input features with descriptors Attributes: Name Type Description categories List[str] Names of the categories. allowed List[bool] List of bools indicating if a category is allowed within the optimization. descriptors List[str] List of strings representing the names of the descriptors. calues List[List[float]] List of lists representing the descriptor values. Source code in bofire/domain/features.py class CategoricalDescriptorInput ( CategoricalInput ): \"\"\"Class for categorical input features with descriptors Attributes: categories (List[str]): Names of the categories. allowed (List[bool]): List of bools indicating if a category is allowed within the optimization. descriptors (List[str]): List of strings representing the names of the descriptors. calues (List[List[float]]): List of lists representing the descriptor values. \"\"\" descriptors : TDescriptors values : TCategoricalDescriptorVals @validator ( \"descriptors\" ) def validate_descriptors ( cls , descriptors ): \"\"\"validates that descriptors have unique names Args: categories (List[str]): List of descriptor names Raises: ValueError: when descriptors have non-unique names Returns: List[str]: List of the descriptors \"\"\" descriptors = [ name2key ( name ) for name in descriptors ] if len ( descriptors ) != len ( set ( descriptors )): raise ValueError ( \"descriptors must be unique\" ) return descriptors @validator ( \"values\" ) def validate_values ( cls , v , values ): \"\"\"validates the compatability of passed values for the descriptors and the defined categories Args: v (List[List[float]]): Nested list with descriptor values values (Dict): Dictionary with attributes Raises: ValueError: when values have different length than categories ValueError: when rows in values have different length than descriptors ValueError: when a descriptor shows no variance in the data Returns: List[List[float]]: Nested list with descriptor values \"\"\" if len ( v ) != len ( values [ \"categories\" ]): raise ValueError ( \"values must have same length as categories\" ) for row in v : if len ( row ) != len ( values [ \"descriptors\" ]): raise ValueError ( \"rows in values must have same length as descriptors\" ) a = np . array ( v ) for i , d in enumerate ( values [ \"descriptors\" ]): if len ( set ( a [:, i ])) == 1 : raise ValueError ( \"No variation for descriptor {d} .\" ) return v def to_df ( self ): \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" data = { cat : values for cat , values in zip ( self . categories , self . values )} return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors ) def get_real_descriptor_bounds ( self , values ) -> pd . DataFrame : \"\"\"Method to generate a dataFrame as tabular overview of lower and upper bounds of the descriptors (excluding non-allowed descriptors) Args: values (pd.Series): The categories present in the passed data for the considered feature Returns: pd.Series: Tabular overview of lower and upper bounds of the descriptors \"\"\" df = self . to_df () . loc [ self . get_possible_categories ( values )] data = { \"lower\" : [ min ( df [ desc ] . tolist ()) for desc in self . descriptors ], \"upper\" : [ max ( df [ desc ] . tolist ()) for desc in self . descriptors ], } return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors ) def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data ValueError: when no variation is present or planed for a given descriptor Returns: pd.Series: A dataFrame with experiments \"\"\" values = super () . validate_experimental ( values , strict ) if strict : bounds = self . get_real_descriptor_bounds ( values ) for desc in self . descriptors : if bounds . loc [ \"lower\" , desc ] == bounds . loc [ \"upper\" , desc ]: raise ValueError ( f \"No variation present or planned for descriptor { desc } for feature { self . key } . Remove the descriptor.\" ) return values @classmethod def from_df ( cls , key : str , df : pd . DataFrame ): \"\"\"Creates a feature from a dataframe Args: key (str): The name of the feature df (pd.DataFrame): Categories as rows and descriptors as columns Returns: _type_: _description_ \"\"\" return cls ( key = key , categories = list ( df . index ), allowed = [ True for _ in range ( len ( df ))], descriptors = list ( df . columns ), values = df . values . tolist (), )","title":"CategoricalDescriptorInput"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.from_df","text":"Creates a feature from a dataframe Parameters: Name Type Description Default key str The name of the feature required df pd.DataFrame Categories as rows and descriptors as columns required Returns: Type Description _type_ description Source code in bofire/domain/features.py @classmethod def from_df ( cls , key : str , df : pd . DataFrame ): \"\"\"Creates a feature from a dataframe Args: key (str): The name of the feature df (pd.DataFrame): Categories as rows and descriptors as columns Returns: _type_: _description_ \"\"\" return cls ( key = key , categories = list ( df . index ), allowed = [ True for _ in range ( len ( df ))], descriptors = list ( df . columns ), values = df . values . tolist (), )","title":"from_df()"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.get_real_descriptor_bounds","text":"Method to generate a dataFrame as tabular overview of lower and upper bounds of the descriptors (excluding non-allowed descriptors) Parameters: Name Type Description Default values pd.Series The categories present in the passed data for the considered feature required Returns: Type Description pd.Series Tabular overview of lower and upper bounds of the descriptors Source code in bofire/domain/features.py def get_real_descriptor_bounds ( self , values ) -> pd . DataFrame : \"\"\"Method to generate a dataFrame as tabular overview of lower and upper bounds of the descriptors (excluding non-allowed descriptors) Args: values (pd.Series): The categories present in the passed data for the considered feature Returns: pd.Series: Tabular overview of lower and upper bounds of the descriptors \"\"\" df = self . to_df () . loc [ self . get_possible_categories ( values )] data = { \"lower\" : [ min ( df [ desc ] . tolist ()) for desc in self . descriptors ], \"upper\" : [ max ( df [ desc ] . tolist ()) for desc in self . descriptors ], } return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors )","title":"get_real_descriptor_bounds()"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.to_df","text":"tabular overview of the feature as DataFrame Returns: Type Description pd.DataFrame tabular overview of the feature as DataFrame Source code in bofire/domain/features.py def to_df ( self ): \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" data = { cat : values for cat , values in zip ( self . categories , self . values )} return pd . DataFrame . from_dict ( data , orient = \"index\" , columns = self . descriptors )","title":"to_df()"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.validate_descriptors","text":"validates that descriptors have unique names Parameters: Name Type Description Default categories List[str] List of descriptor names required Exceptions: Type Description ValueError when descriptors have non-unique names Returns: Type Description List[str] List of the descriptors Source code in bofire/domain/features.py @validator ( \"descriptors\" ) def validate_descriptors ( cls , descriptors ): \"\"\"validates that descriptors have unique names Args: categories (List[str]): List of descriptor names Raises: ValueError: when descriptors have non-unique names Returns: List[str]: List of the descriptors \"\"\" descriptors = [ name2key ( name ) for name in descriptors ] if len ( descriptors ) != len ( set ( descriptors )): raise ValueError ( \"descriptors must be unique\" ) return descriptors","title":"validate_descriptors()"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.validate_experimental","text":"Method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Exceptions: Type Description ValueError when an entry is not in the list of allowed categories ValueError when there is no variation in a feature provided by the experimental data ValueError when no variation is present or planed for a given descriptor Returns: Type Description pd.Series A dataFrame with experiments Source code in bofire/domain/features.py def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data ValueError: when no variation is present or planed for a given descriptor Returns: pd.Series: A dataFrame with experiments \"\"\" values = super () . validate_experimental ( values , strict ) if strict : bounds = self . get_real_descriptor_bounds ( values ) for desc in self . descriptors : if bounds . loc [ \"lower\" , desc ] == bounds . loc [ \"upper\" , desc ]: raise ValueError ( f \"No variation present or planned for descriptor { desc } for feature { self . key } . Remove the descriptor.\" ) return values","title":"validate_experimental()"},{"location":"ref-features/#bofire.domain.features.CategoricalDescriptorInput.validate_values","text":"validates the compatability of passed values for the descriptors and the defined categories Parameters: Name Type Description Default v List[List[float]] Nested list with descriptor values required values Dict Dictionary with attributes required Exceptions: Type Description ValueError when values have different length than categories ValueError when rows in values have different length than descriptors ValueError when a descriptor shows no variance in the data Returns: Type Description List[List[float]] Nested list with descriptor values Source code in bofire/domain/features.py @validator ( \"values\" ) def validate_values ( cls , v , values ): \"\"\"validates the compatability of passed values for the descriptors and the defined categories Args: v (List[List[float]]): Nested list with descriptor values values (Dict): Dictionary with attributes Raises: ValueError: when values have different length than categories ValueError: when rows in values have different length than descriptors ValueError: when a descriptor shows no variance in the data Returns: List[List[float]]: Nested list with descriptor values \"\"\" if len ( v ) != len ( values [ \"categories\" ]): raise ValueError ( \"values must have same length as categories\" ) for row in v : if len ( row ) != len ( values [ \"descriptors\" ]): raise ValueError ( \"rows in values must have same length as descriptors\" ) a = np . array ( v ) for i , d in enumerate ( values [ \"descriptors\" ]): if len ( set ( a [:, i ])) == 1 : raise ValueError ( \"No variation for descriptor {d} .\" ) return v","title":"validate_values()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput","text":"Base class for all categorical input features. Attributes: Name Type Description categories List[str] Names of the categories. allowed List[bool] List of bools indicating if a category is allowed within the optimization. Source code in bofire/domain/features.py class CategoricalInput ( InputFeature ): \"\"\"Base class for all categorical input features. Attributes: categories (List[str]): Names of the categories. allowed (List[bool]): List of bools indicating if a category is allowed within the optimization. \"\"\" categories : TCategoryVals allowed : TAllowedVals = None @validator ( \"categories\" ) def validate_categories_unique ( cls , categories ): \"\"\"validates that categories have unique names Args: categories (List[str]): List of category names Raises: ValueError: when categories have non-unique names Returns: List[str]: List of the categories \"\"\" categories = [ name2key ( name ) for name in categories ] if len ( categories ) != len ( set ( categories )): raise ValueError ( \"categories must be unique\" ) return categories @root_validator ( pre = False ) def init_allowed ( cls , values ): \"\"\"validates the list of allowed/not allowed categories Args: values (Dict): Dictionary with attributes Raises: ValueError: when the number of allowences does not fit to the number of categories ValueError: when no category is allowed Returns: Dict: Dictionary with attributes \"\"\" if \"categories\" not in values or values [ \"categories\" ] is None : return values if \"allowed\" not in values or values [ \"allowed\" ] is None : values [ \"allowed\" ] = [ True for _ in range ( len ( values [ \"categories\" ]))] if len ( values [ \"allowed\" ]) != len ( values [ \"categories\" ]): raise ValueError ( \"allowed must have same length as categories\" ) if sum ( values [ \"allowed\" ]) == 0 : raise ValueError ( \"no category is allowed\" ) return values def is_fixed ( self ): \"\"\"Returns True if there is only one allowed category. Returns: [bool]: True if there is only one allowed category \"\"\" if self . allowed is None : return False return sum ( self . allowed ) == 1 def fixed_value ( self ): \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed Returns: List[str]: List of categories or None \"\"\" if self . is_fixed (): return self . get_allowed_categories ()[ 0 ] else : return None def get_allowed_categories ( self ): \"\"\"Returns the allowed categories. Returns: list of str: The allowed categories \"\"\" if self . allowed is None : return [] return [ c for c , a in zip ( self . categories , self . allowed ) if a ] def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if sum ( values . isin ( self . categories )) != len ( values ): raise ValueError ( f \"invalid values for ` { self . key } `, allowed are: ` { self . categories } `\" ) if strict : possible_categories = self . get_possible_categories ( values ) if len ( possible_categories ) != len ( self . categories ): raise ValueError ( f \"Categories { list ( set ( self . categories ) - set ( possible_categories )) } of feature { self . key } not used. Remove them.\" ) return values def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when not all values for a feature are one of the allowed categories Returns: pd.Series: The passed dataFrame with candidates \"\"\" if sum ( values . isin ( self . get_allowed_categories ())) != len ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are a valid allowed category from { self . get_allowed_categories () } \" ) return values def get_forbidden_categories ( self ): \"\"\"Returns the non-allowed categories Returns: List[str]: List of the non-allowed categories \"\"\" return list ( set ( self . categories ) - set ( self . get_allowed_categories ())) def get_possible_categories ( self , values : pd . Series ) -> list : \"\"\"Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization Args: values (pd.Series): Series with the values for this feature Returns: list: list of possible categories \"\"\" return sorted ( list ( set ( list ( set ( values . tolist ())) + self . get_allowed_categories ())) ) def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . get_allowed_categories (), n ) ) def __str__ ( self ) -> str : \"\"\"Returns the number of categories as str Returns: str: Number of categories \"\"\" return f \" { len ( self . categories ) } categories\"","title":"CategoricalInput"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.__str__","text":"Returns the number of categories as str Returns: Type Description str Number of categories Source code in bofire/domain/features.py def __str__ ( self ) -> str : \"\"\"Returns the number of categories as str Returns: str: Number of categories \"\"\" return f \" { len ( self . categories ) } categories\"","title":"__str__()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.fixed_value","text":"Returns the categories to which the feature is fixed, None if the feature is not fixed Returns: Type Description List[str] List of categories or None Source code in bofire/domain/features.py def fixed_value ( self ): \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed Returns: List[str]: List of categories or None \"\"\" if self . is_fixed (): return self . get_allowed_categories ()[ 0 ] else : return None","title":"fixed_value()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.get_allowed_categories","text":"Returns the allowed categories. Returns: Type Description list of str The allowed categories Source code in bofire/domain/features.py def get_allowed_categories ( self ): \"\"\"Returns the allowed categories. Returns: list of str: The allowed categories \"\"\" if self . allowed is None : return [] return [ c for c , a in zip ( self . categories , self . allowed ) if a ]","title":"get_allowed_categories()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.get_forbidden_categories","text":"Returns the non-allowed categories Returns: Type Description List[str] List of the non-allowed categories Source code in bofire/domain/features.py def get_forbidden_categories ( self ): \"\"\"Returns the non-allowed categories Returns: List[str]: List of the non-allowed categories \"\"\" return list ( set ( self . categories ) - set ( self . get_allowed_categories ()))","title":"get_forbidden_categories()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.get_possible_categories","text":"Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization Parameters: Name Type Description Default values pd.Series Series with the values for this feature required Returns: Type Description list list of possible categories Source code in bofire/domain/features.py def get_possible_categories ( self , values : pd . Series ) -> list : \"\"\"Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization Args: values (pd.Series): Series with the values for this feature Returns: list: list of possible categories \"\"\" return sorted ( list ( set ( list ( set ( values . tolist ())) + self . get_allowed_categories ())) )","title":"get_possible_categories()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.init_allowed","text":"validates the list of allowed/not allowed categories Parameters: Name Type Description Default values Dict Dictionary with attributes required Exceptions: Type Description ValueError when the number of allowences does not fit to the number of categories ValueError when no category is allowed Returns: Type Description Dict Dictionary with attributes Source code in bofire/domain/features.py @root_validator ( pre = False ) def init_allowed ( cls , values ): \"\"\"validates the list of allowed/not allowed categories Args: values (Dict): Dictionary with attributes Raises: ValueError: when the number of allowences does not fit to the number of categories ValueError: when no category is allowed Returns: Dict: Dictionary with attributes \"\"\" if \"categories\" not in values or values [ \"categories\" ] is None : return values if \"allowed\" not in values or values [ \"allowed\" ] is None : values [ \"allowed\" ] = [ True for _ in range ( len ( values [ \"categories\" ]))] if len ( values [ \"allowed\" ]) != len ( values [ \"categories\" ]): raise ValueError ( \"allowed must have same length as categories\" ) if sum ( values [ \"allowed\" ]) == 0 : raise ValueError ( \"no category is allowed\" ) return values","title":"init_allowed()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.is_fixed","text":"Returns True if there is only one allowed category. Returns: Type Description [bool] True if there is only one allowed category Source code in bofire/domain/features.py def is_fixed ( self ): \"\"\"Returns True if there is only one allowed category. Returns: [bool]: True if there is only one allowed category \"\"\" if self . allowed is None : return False return sum ( self . allowed ) == 1","title":"is_fixed()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.sample","text":"Draw random samples from the feature. Parameters: Name Type Description Default n int number of samples. required Returns: Type Description pd.Series drawn samples. Source code in bofire/domain/features.py def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . get_allowed_categories (), n ) )","title":"sample()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.validate_candidental","text":"Method to validate the suggested candidates Parameters: Name Type Description Default values pd.Series A dataFrame with candidates required Exceptions: Type Description ValueError when not all values for a feature are one of the allowed categories Returns: Type Description pd.Series The passed dataFrame with candidates Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when not all values for a feature are one of the allowed categories Returns: pd.Series: The passed dataFrame with candidates \"\"\" if sum ( values . isin ( self . get_allowed_categories ())) != len ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are a valid allowed category from { self . get_allowed_categories () } \" ) return values","title":"validate_candidental()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.validate_categories_unique","text":"validates that categories have unique names Parameters: Name Type Description Default categories List[str] List of category names required Exceptions: Type Description ValueError when categories have non-unique names Returns: Type Description List[str] List of the categories Source code in bofire/domain/features.py @validator ( \"categories\" ) def validate_categories_unique ( cls , categories ): \"\"\"validates that categories have unique names Args: categories (List[str]): List of category names Raises: ValueError: when categories have non-unique names Returns: List[str]: List of the categories \"\"\" categories = [ name2key ( name ) for name in categories ] if len ( categories ) != len ( set ( categories )): raise ValueError ( \"categories must be unique\" ) return categories","title":"validate_categories_unique()"},{"location":"ref-features/#bofire.domain.features.CategoricalInput.validate_experimental","text":"Method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Exceptions: Type Description ValueError when an entry is not in the list of allowed categories ValueError when there is no variation in a feature provided by the experimental data Returns: Type Description pd.Series A dataFrame with experiments Source code in bofire/domain/features.py def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when an entry is not in the list of allowed categories ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if sum ( values . isin ( self . categories )) != len ( values ): raise ValueError ( f \"invalid values for ` { self . key } `, allowed are: ` { self . categories } `\" ) if strict : possible_categories = self . get_possible_categories ( values ) if len ( possible_categories ) != len ( self . categories ): raise ValueError ( f \"Categories { list ( set ( self . categories ) - set ( possible_categories )) } of feature { self . key } not used. Remove them.\" ) return values","title":"validate_experimental()"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput","text":"Class for continuous input features with descriptors Attributes: Name Type Description lower_bound float Lower bound of the feature in the optimization. upper_bound float Upper bound of the feature in the optimization. descriptors List[str] Names of the descriptors. values List[float] Values of the descriptors. Source code in bofire/domain/features.py class ContinuousDescriptorInput ( ContinuousInput ): \"\"\"Class for continuous input features with descriptors Attributes: lower_bound (float): Lower bound of the feature in the optimization. upper_bound (float): Upper bound of the feature in the optimization. descriptors (List[str]): Names of the descriptors. values (List[float]): Values of the descriptors. \"\"\" descriptors : TDescriptors values : TDiscreteVals @validator ( \"descriptors\" ) def descriptors_to_keys ( cls , descriptors ): \"\"\"validates the descriptor names and transforms it to valid keys Args: descriptors (List[str]): List of descriptor names Returns: List[str]: List of valid keys \"\"\" return [ name2key ( name ) for name in descriptors ] @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"compares the length of the defined descriptors list with the provided values Args: values (Dict): Dictionary with all attribues Raises: ValueError: when the number of descriptors does not math the number of provided values Returns: Dict: Dict with the attributes \"\"\" if len ( values [ \"descriptors\" ]) != len ( values [ \"values\" ]): raise ValueError ( 'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}' ) return values def to_df ( self ) -> pd . DataFrame : \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" return pd . DataFrame ( data = [ self . values ], index = [ self . key ], columns = self . descriptors )","title":"ContinuousDescriptorInput"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput.descriptors_to_keys","text":"validates the descriptor names and transforms it to valid keys Parameters: Name Type Description Default descriptors List[str] List of descriptor names required Returns: Type Description List[str] List of valid keys Source code in bofire/domain/features.py @validator ( \"descriptors\" ) def descriptors_to_keys ( cls , descriptors ): \"\"\"validates the descriptor names and transforms it to valid keys Args: descriptors (List[str]): List of descriptor names Returns: List[str]: List of valid keys \"\"\" return [ name2key ( name ) for name in descriptors ]","title":"descriptors_to_keys()"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput.to_df","text":"tabular overview of the feature as DataFrame Returns: Type Description pd.DataFrame tabular overview of the feature as DataFrame Source code in bofire/domain/features.py def to_df ( self ) -> pd . DataFrame : \"\"\"tabular overview of the feature as DataFrame Returns: pd.DataFrame: tabular overview of the feature as DataFrame \"\"\" return pd . DataFrame ( data = [ self . values ], index = [ self . key ], columns = self . descriptors )","title":"to_df()"},{"location":"ref-features/#bofire.domain.features.ContinuousDescriptorInput.validate_list_lengths","text":"compares the length of the defined descriptors list with the provided values Parameters: Name Type Description Default values Dict Dictionary with all attribues required Exceptions: Type Description ValueError when the number of descriptors does not math the number of provided values Returns: Type Description Dict Dict with the attributes Source code in bofire/domain/features.py @root_validator ( pre = False ) def validate_list_lengths ( cls , values ): \"\"\"compares the length of the defined descriptors list with the provided values Args: values (Dict): Dictionary with all attribues Raises: ValueError: when the number of descriptors does not math the number of provided values Returns: Dict: Dict with the attributes \"\"\" if len ( values [ \"descriptors\" ]) != len ( values [ \"values\" ]): raise ValueError ( 'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}' ) return values","title":"validate_list_lengths()"},{"location":"ref-features/#bofire.domain.features.ContinuousInput","text":"Base class for all continuous input features. Attributes: Name Type Description lower_bound float Lower bound of the feature in the optimization. upper_bound float Upper bound of the feature in the optimization. Source code in bofire/domain/features.py class ContinuousInput ( NumericalInputFeature ): \"\"\"Base class for all continuous input features. Attributes: lower_bound (float): Lower bound of the feature in the optimization. upper_bound (float): Upper bound of the feature in the optimization. \"\"\" lower_bound : float upper_bound : float @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validates that the lower bound is lower than the upper bound Args: values (Dict): Dictionary with attributes key, lower and upper bound Raises: ValueError: when the lower bound is higher than the upper bound Returns: Dict: The attributes as dictionary \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when non numerical values are passed ValueError: when values are larger than the upper bound of the feature ValueError: when values are lower than the lower bound of the feature Returns: pd.Series: The passed dataFrame with candidates \"\"\" noise = 10e-8 super () . validate_candidental ( values ) if ( values < self . lower_bound - noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are larger than lower bound ` { self . lower_bound } ` \" ) if ( values > self . upper_bound + noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are smaller than upper bound ` { self . upper_bound } ` \" ) return values def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . uniform ( self . lower_bound , self . upper_bound , n ), ) def __str__ ( self ) -> str : \"\"\"Method to return a string of lower and upper bound Returns: str: String of a list with lower and upper bound \"\"\" return f \"[ { self . lower_bound } , { self . upper_bound } ]\"","title":"ContinuousInput"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.__str__","text":"Method to return a string of lower and upper bound Returns: Type Description str String of a list with lower and upper bound Source code in bofire/domain/features.py def __str__ ( self ) -> str : \"\"\"Method to return a string of lower and upper bound Returns: str: String of a list with lower and upper bound \"\"\" return f \"[ { self . lower_bound } , { self . upper_bound } ]\"","title":"__str__()"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.sample","text":"Draw random samples from the feature. Parameters: Name Type Description Default n int number of samples. required Returns: Type Description pd.Series drawn samples. Source code in bofire/domain/features.py def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . uniform ( self . lower_bound , self . upper_bound , n ), )","title":"sample()"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.validate_candidental","text":"Method to validate the suggested candidates Parameters: Name Type Description Default values pd.Series A dataFrame with candidates required Exceptions: Type Description ValueError when non numerical values are passed ValueError when values are larger than the upper bound of the feature ValueError when values are lower than the lower bound of the feature Returns: Type Description pd.Series The passed dataFrame with candidates Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Raises: ValueError: when non numerical values are passed ValueError: when values are larger than the upper bound of the feature ValueError: when values are lower than the lower bound of the feature Returns: pd.Series: The passed dataFrame with candidates \"\"\" noise = 10e-8 super () . validate_candidental ( values ) if ( values < self . lower_bound - noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are larger than lower bound ` { self . lower_bound } ` \" ) if ( values > self . upper_bound + noise ) . any (): raise ValueError ( f \"not all values of input feature ` { self . key } `are smaller than upper bound ` { self . upper_bound } ` \" ) return values","title":"validate_candidental()"},{"location":"ref-features/#bofire.domain.features.ContinuousInput.validate_lower_upper","text":"Validates that the lower bound is lower than the upper bound Parameters: Name Type Description Default values Dict Dictionary with attributes key, lower and upper bound required Exceptions: Type Description ValueError when the lower bound is higher than the upper bound Returns: Type Description Dict The attributes as dictionary Source code in bofire/domain/features.py @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validates that the lower bound is lower than the upper bound Args: values (Dict): Dictionary with attributes key, lower and upper bound Raises: ValueError: when the lower bound is higher than the upper bound Returns: Dict: The attributes as dictionary \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values","title":"validate_lower_upper()"},{"location":"ref-features/#bofire.domain.features.ContinuousOutput","text":"The base class for a continuous output feature Attributes: Name Type Description objective objective objective of the feature indicating in which direction it should be optimzed. Defaults to MaximizeObjective . Source code in bofire/domain/features.py class ContinuousOutput ( OutputFeature ): \"\"\"The base class for a continuous output feature Attributes: objective (objective, optional): objective of the feature indicating in which direction it should be optimzed. Defaults to `MaximizeObjective`. \"\"\" objective : Optional [ Objective ] = Field ( default_factory = lambda : MaximizeObjective ( w = 1.0 ) ) def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" config : Dict [ str , Any ] = { \"type\" : self . __class__ . __name__ , \"key\" : self . key , } if self . objective is not None : config [ \"objective\" ] = self . objective . to_config () return config def plot ( self , lower : float , upper : float , experiments : Optional [ pd . DataFrame ] = None , plot_details : bool = True , line_options : Optional [ Dict ] = None , scatter_options : Optional [ Dict ] = None , label_options : Optional [ Dict ] = None , title_options : Optional [ Dict ] = None , ): \"\"\"Plot the assigned objective. Args: lower (float): lower bound for the plot upper (float): upper bound for the plot experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None. \"\"\" if self . objective is None : raise ValueError ( f \"No objective assigned for ContinuousOutputFeauture with key { self . key } .\" ) line_options = line_options or {} scatter_options = scatter_options or {} label_options = label_options or {} title_options = title_options or {} line_options [ \"color\" ] = line_options . get ( \"color\" , \"black\" ) scatter_options [ \"color\" ] = scatter_options . get ( \"color\" , \"red\" ) x = pd . Series ( np . linspace ( lower , upper , 5000 )) reward = self . objective . __call__ ( x ) fig , ax = plt . subplots () ax . plot ( x , reward , ** line_options ) # TODO: validate dataframe if experiments is not None : x_data = experiments . loc [ experiments [ self . key ] . notna (), self . key ] . values ax . scatter ( x_data , # type: ignore self . objective . __call__ ( x_data ), # type: ignore ** scatter_options , ) ax . set_title ( \"Objective %s \" % self . key , ** title_options ) ax . set_ylabel ( \"Objective\" , ** label_options ) ax . set_xlabel ( self . key , ** label_options ) if plot_details : ax = self . objective . plot_details ( ax = ax ) return fig , ax def __str__ ( self ) -> str : return \"ContinuousOutputFeature\"","title":"ContinuousOutput"},{"location":"ref-features/#bofire.domain.features.ContinuousOutput.__str__","text":"Return str(self). Source code in bofire/domain/features.py def __str__ ( self ) -> str : return \"ContinuousOutputFeature\"","title":"__str__()"},{"location":"ref-features/#bofire.domain.features.ContinuousOutput.plot","text":"Plot the assigned objective. Parameters: Name Type Description Default lower float lower bound for the plot required upper float upper bound for the plot required experiments Optional[pd.DataFrame] If provided, scatter also the historical data in the plot. Defaults to None. None Source code in bofire/domain/features.py def plot ( self , lower : float , upper : float , experiments : Optional [ pd . DataFrame ] = None , plot_details : bool = True , line_options : Optional [ Dict ] = None , scatter_options : Optional [ Dict ] = None , label_options : Optional [ Dict ] = None , title_options : Optional [ Dict ] = None , ): \"\"\"Plot the assigned objective. Args: lower (float): lower bound for the plot upper (float): upper bound for the plot experiments (Optional[pd.DataFrame], optional): If provided, scatter also the historical data in the plot. Defaults to None. \"\"\" if self . objective is None : raise ValueError ( f \"No objective assigned for ContinuousOutputFeauture with key { self . key } .\" ) line_options = line_options or {} scatter_options = scatter_options or {} label_options = label_options or {} title_options = title_options or {} line_options [ \"color\" ] = line_options . get ( \"color\" , \"black\" ) scatter_options [ \"color\" ] = scatter_options . get ( \"color\" , \"red\" ) x = pd . Series ( np . linspace ( lower , upper , 5000 )) reward = self . objective . __call__ ( x ) fig , ax = plt . subplots () ax . plot ( x , reward , ** line_options ) # TODO: validate dataframe if experiments is not None : x_data = experiments . loc [ experiments [ self . key ] . notna (), self . key ] . values ax . scatter ( x_data , # type: ignore self . objective . __call__ ( x_data ), # type: ignore ** scatter_options , ) ax . set_title ( \"Objective %s \" % self . key , ** title_options ) ax . set_ylabel ( \"Objective\" , ** label_options ) ax . set_xlabel ( self . key , ** label_options ) if plot_details : ax = self . objective . plot_details ( ax = ax ) return fig , ax","title":"plot()"},{"location":"ref-features/#bofire.domain.features.ContinuousOutput.to_config","text":"Generate serialized version of the feature. Returns: Type Description Dict Serialized version of the feature as dictionary. Source code in bofire/domain/features.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" config : Dict [ str , Any ] = { \"type\" : self . __class__ . __name__ , \"key\" : self . key , } if self . objective is not None : config [ \"objective\" ] = self . objective . to_config () return config","title":"to_config()"},{"location":"ref-features/#bofire.domain.features.DiscreteInput","text":"Feature with discretized ordinal values allowed in the optimization. Attributes: Name Type Description key(str) key of the feature. values(List[float]) the discretized allowed values during the optimization. Source code in bofire/domain/features.py class DiscreteInput ( NumericalInputFeature ): \"\"\"Feature with discretized ordinal values allowed in the optimization. Attributes: key(str): key of the feature. values(List[float]): the discretized allowed values during the optimization. \"\"\" values : TDiscreteVals @validator ( \"values\" ) def validate_values_unique ( cls , values ): \"\"\"Validates that provided values are unique. Args: values (List[float]): List of values Raises: ValueError: when values are non-unique. Returns: List[values]: Sorted list of values \"\"\" if len ( values ) != len ( set ( values )): raise ValueError ( \"Discrete values must be unique\" ) return sorted ( values ) @property def lower_bound ( self ) -> float : \"\"\"Lower bound of the set of allowed values\"\"\" return min ( self . values ) @property def upper_bound ( self ) -> float : \"\"\"Upper bound of the set of allowed values\"\"\" return max ( self . values ) def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the provided candidates. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Raises error when one of the provided values is not contained in the list of allowed values. Returns: pd.Series: _uggested candidates for the feature \"\"\" super () . validate_candidental ( values ) if not np . isin ( values . to_numpy (), np . array ( self . values )) . all (): raise ValueError ( f \"Not allowed values in candidates for feature { self . key } .\" ) return values def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . values , n ))","title":"DiscreteInput"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.lower_bound","text":"Lower bound of the set of allowed values","title":"lower_bound"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.upper_bound","text":"Upper bound of the set of allowed values","title":"upper_bound"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.sample","text":"Draw random samples from the feature. Parameters: Name Type Description Default n int number of samples. required Returns: Type Description pd.Series drawn samples. Source code in bofire/domain/features.py def sample ( self , n : int ) -> pd . Series : \"\"\"Draw random samples from the feature. Args: n (int): number of samples. Returns: pd.Series: drawn samples. \"\"\" return pd . Series ( name = self . key , data = np . random . choice ( self . values , n ))","title":"sample()"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.validate_candidental","text":"Method to validate the provided candidates. Parameters: Name Type Description Default values pd.Series suggested candidates for the feature required Exceptions: Type Description ValueError Raises error when one of the provided values is not contained in the list of allowed values. Returns: Type Description pd.Series _uggested candidates for the feature Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Method to validate the provided candidates. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Raises error when one of the provided values is not contained in the list of allowed values. Returns: pd.Series: _uggested candidates for the feature \"\"\" super () . validate_candidental ( values ) if not np . isin ( values . to_numpy (), np . array ( self . values )) . all (): raise ValueError ( f \"Not allowed values in candidates for feature { self . key } .\" ) return values","title":"validate_candidental()"},{"location":"ref-features/#bofire.domain.features.DiscreteInput.validate_values_unique","text":"Validates that provided values are unique. Parameters: Name Type Description Default values List[float] List of values required Exceptions: Type Description ValueError when values are non-unique. Returns: Type Description List[values] Sorted list of values Source code in bofire/domain/features.py @validator ( \"values\" ) def validate_values_unique ( cls , values ): \"\"\"Validates that provided values are unique. Args: values (List[float]): List of values Raises: ValueError: when values are non-unique. Returns: List[values]: Sorted list of values \"\"\" if len ( values ) != len ( set ( values )): raise ValueError ( \"Discrete values must be unique\" ) return sorted ( values )","title":"validate_values_unique()"},{"location":"ref-features/#bofire.domain.features.Feature","text":"The base class for all features. Source code in bofire/domain/features.py class Feature ( KeyModel ): \"\"\"The base class for all features.\"\"\" def __lt__ ( self , other ) -> bool : \"\"\" Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER) Args: other: The other class to compare to self Returns: bool: True if the other class is larger than self, else False \"\"\" # TODO: add order of base class to FEATURE_ORDER and remove type: ignore order_self = FEATURE_ORDER [ type ( self )] # type: ignore order_other = FEATURE_ORDER [ type ( other )] if order_self == order_other : return self . key < other . key else : return order_self < order_other def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } @staticmethod def from_config ( config : Dict ) -> \"Feature\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of a objective Returns: Objective: Instantiated objective of the type specified in the `config`. \"\"\" input_mapper = { \"ContinuousInput\" : ContinuousInput , \"DiscreteInput\" : DiscreteInput , \"CategoricalInput\" : CategoricalInput , \"CategoricalDescriptorInput\" : CategoricalDescriptorInput , \"ContinuousDescriptorInput\" : ContinuousDescriptorInput , } output_mapper = { \"ContinuousOutput\" : ContinuousOutput , } if config [ \"type\" ] in input_mapper . keys (): return input_mapper [ config [ \"type\" ]]( ** config ) else : if \"objective\" in config . keys (): obj = Objective . from_config ( config = config [ \"objective\" ]) else : obj = None return output_mapper [ config [ \"type\" ]]( key = config [ \"key\" ], objective = obj )","title":"Feature"},{"location":"ref-features/#bofire.domain.features.Feature.__lt__","text":"Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER) Parameters: Name Type Description Default other The other class to compare to self required Returns: Type Description bool True if the other class is larger than self, else False Source code in bofire/domain/features.py def __lt__ ( self , other ) -> bool : \"\"\" Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER) Args: other: The other class to compare to self Returns: bool: True if the other class is larger than self, else False \"\"\" # TODO: add order of base class to FEATURE_ORDER and remove type: ignore order_self = FEATURE_ORDER [ type ( self )] # type: ignore order_other = FEATURE_ORDER [ type ( other )] if order_self == order_other : return self . key < other . key else : return order_self < order_other","title":"__lt__()"},{"location":"ref-features/#bofire.domain.features.Feature.from_config","text":"Generate objective out of serialized version. Parameters: Name Type Description Default config Dict Serialized version of a objective required Returns: Type Description Objective Instantiated objective of the type specified in the config . Source code in bofire/domain/features.py @staticmethod def from_config ( config : Dict ) -> \"Feature\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of a objective Returns: Objective: Instantiated objective of the type specified in the `config`. \"\"\" input_mapper = { \"ContinuousInput\" : ContinuousInput , \"DiscreteInput\" : DiscreteInput , \"CategoricalInput\" : CategoricalInput , \"CategoricalDescriptorInput\" : CategoricalDescriptorInput , \"ContinuousDescriptorInput\" : ContinuousDescriptorInput , } output_mapper = { \"ContinuousOutput\" : ContinuousOutput , } if config [ \"type\" ] in input_mapper . keys (): return input_mapper [ config [ \"type\" ]]( ** config ) else : if \"objective\" in config . keys (): obj = Objective . from_config ( config = config [ \"objective\" ]) else : obj = None return output_mapper [ config [ \"type\" ]]( key = config [ \"key\" ], objective = obj )","title":"from_config()"},{"location":"ref-features/#bofire.domain.features.Feature.to_config","text":"Generate serialized version of the feature. Returns: Type Description Dict Serialized version of the feature as dictionary. Source code in bofire/domain/features.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the feature. Returns: Dict: Serialized version of the feature as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), }","title":"to_config()"},{"location":"ref-features/#bofire.domain.features.Features","text":"Container of features, both input and output features are allowed. Attributes: Name Type Description features List(Features list of the features. Source code in bofire/domain/features.py class Features ( BaseModel ): \"\"\"Container of features, both input and output features are allowed. Attributes: features (List(Features)): list of the features. \"\"\" features : FeatureSequence = Field ( default_factory = lambda : []) def to_config ( self ) -> Dict : \"\"\"Serialize the features container. Returns: Dict: serialized features container \"\"\" return { \"type\" : \"general\" , \"features\" : [ feat . to_config () for feat in self . features ], } @staticmethod def from_config ( config : Dict ) -> \"Features\" : \"\"\"Instantiates a `Feature` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized features dictionary Returns: Features: instantiated features object \"\"\" if config [ \"type\" ] == \"inputs\" : return InputFeatures ( features = [ cast ( InputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"outputs\" : return OutputFeatures ( features = [ cast ( OutputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"general\" : return Features ( features = [ Feature . from_config ( feat ) for feat in config [ \"features\" ]] ) else : raise ValueError ( f \"Unknown type { config [ 'type' ] } provided.\" ) def __iter__ ( self ): return iter ( self . features ) def __len__ ( self ): return len ( self . features ) def __getitem__ ( self , i ): return self . features [ i ] def __add__ ( self , other : Union [ Sequence [ Feature ], Features ]): if isinstance ( other , Features ): other_feature_seq = other . features else : other_feature_seq = other new_feature_seq = list ( itertools . chain ( self . features , other_feature_seq )) def is_feats_of_type ( feats , ftype_collection , ftype_element ): return isinstance ( feats , ftype_collection ) or ( not isinstance ( feats , Features ) and ( len ( feats ) > 0 and isinstance ( feats [ 0 ], ftype_element )) ) def is_infeats ( feats ): return is_feats_of_type ( feats , InputFeatures , InputFeature ) def is_outfeats ( feats ): return is_feats_of_type ( feats , OutputFeatures , OutputFeature ) if is_infeats ( self ) and is_infeats ( other ): return InputFeatures ( features = cast ( Tuple [ InputFeature , ... ], new_feature_seq ) ) if is_outfeats ( self ) and is_outfeats ( other ): return OutputFeatures ( features = cast ( Tuple [ OutputFeature , ... ], new_feature_seq ) ) return Features ( features = new_feature_seq ) def get_by_key ( self , key : str ) -> Feature : \"\"\"Get a feature by its key. Args: key (str): Feature key of the feature of interest Returns: Feature: Feature of interest \"\"\" return { f . key : f for f in self . features }[ key ] def get ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" return self . __class__ ( features = sorted ( filter_by_class ( self . features , includes = includes , excludes = excludes , exact = exact , ) ) ) def get_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get ( includes = includes , excludes = excludes , exact = exact , ) ]","title":"Features"},{"location":"ref-features/#bofire.domain.features.Features.from_config","text":"Instantiates a Feature object from a dictionary created by the to_config method. Parameters: Name Type Description Default config Dict Serialized features dictionary required Returns: Type Description Features instantiated features object Source code in bofire/domain/features.py @staticmethod def from_config ( config : Dict ) -> \"Features\" : \"\"\"Instantiates a `Feature` object from a dictionary created by the `to_config`method. Args: config (Dict): Serialized features dictionary Returns: Features: instantiated features object \"\"\" if config [ \"type\" ] == \"inputs\" : return InputFeatures ( features = [ cast ( InputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"outputs\" : return OutputFeatures ( features = [ cast ( OutputFeature , Feature . from_config ( feat )) for feat in config [ \"features\" ] ] ) if config [ \"type\" ] == \"general\" : return Features ( features = [ Feature . from_config ( feat ) for feat in config [ \"features\" ]] ) else : raise ValueError ( f \"Unknown type { config [ 'type' ] } provided.\" )","title":"from_config()"},{"location":"ref-features/#bofire.domain.features.Features.get","text":"get features of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False by_attribute str If set it is filtered by the attribute specified in by by_attribute . Defaults to None. required Returns: Type Description List[Feature] List of features in the domain fitting to the passed requirements. Source code in bofire/domain/features.py def get ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> Features : \"\"\"get features of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. by_attribute (str, optional): If set it is filtered by the attribute specified in by `by_attribute`. Defaults to None. Returns: List[Feature]: List of features in the domain fitting to the passed requirements. \"\"\" return self . __class__ ( features = sorted ( filter_by_class ( self . features , includes = includes , excludes = excludes , exact = exact , ) ) )","title":"get()"},{"location":"ref-features/#bofire.domain.features.Features.get_by_key","text":"Get a feature by its key. Parameters: Name Type Description Default key str Feature key of the feature of interest required Returns: Type Description Feature Feature of interest Source code in bofire/domain/features.py def get_by_key ( self , key : str ) -> Feature : \"\"\"Get a feature by its key. Args: key (str): Feature key of the feature of interest Returns: Feature: Feature of interest \"\"\" return { f . key : f for f in self . features }[ key ]","title":"get_by_key()"},{"location":"ref-features/#bofire.domain.features.Features.get_keys","text":"Method to get feature keys of the domain Parameters: Name Type Description Default includes Union[Type, List[Type]] Feature class or list of specific feature classes to be returned. Defaults to Feature. <class 'bofire.domain.features.Feature'> excludes Union[Type, List[Type]] Feature class or list of specific feature classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[str] List of feature keys fitting to the passed requirements. Source code in bofire/domain/features.py def get_keys ( self , includes : Union [ Type , List [ Type ]] = Feature , excludes : Union [ Type , List [ Type ]] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Method to get feature keys of the domain Args: includes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be returned. Defaults to Feature. excludes (Union[Type, List[Type]], optional): Feature class or list of specific feature classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get ( includes = includes , excludes = excludes , exact = exact , ) ]","title":"get_keys()"},{"location":"ref-features/#bofire.domain.features.Features.to_config","text":"Serialize the features container. Returns: Type Description Dict serialized features container Source code in bofire/domain/features.py def to_config ( self ) -> Dict : \"\"\"Serialize the features container. Returns: Dict: serialized features container \"\"\" return { \"type\" : \"general\" , \"features\" : [ feat . to_config () for feat in self . features ], }","title":"to_config()"},{"location":"ref-features/#bofire.domain.features.InputFeature","text":"Base class for all input features. Source code in bofire/domain/features.py class InputFeature ( Feature ): \"\"\"Base class for all input features.\"\"\" @abstractmethod def is_fixed () -> bool : \"\"\"Indicates if a variable is set to a fixed value. Returns: bool: True if fixed, els False. \"\"\" pass @abstractmethod def fixed_value () -> Union [ None , str , float ]: \"\"\"Method to return the fixed value in case of a fixed feature. Returns: Union[None,str,float]: None in case the feature is not fixed, else the fixed value. \"\"\" pass @abstractmethod def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Abstract method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Returns: pd.Series: The passed dataFrame with experiments \"\"\" pass @abstractmethod def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Abstract method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Returns: pd.Series: The passed dataFrame with candidates \"\"\" pass @abstractmethod def sample ( self , n : int ) -> pd . Series : \"\"\"Sample a series of allowed values. Args: n (int): Number of samples Returns: pd.Series: Sampled values. \"\"\" pass","title":"InputFeature"},{"location":"ref-features/#bofire.domain.features.InputFeature.fixed_value","text":"Method to return the fixed value in case of a fixed feature. Returns: Type Description Union[None,str,float] None in case the feature is not fixed, else the fixed value. Source code in bofire/domain/features.py @abstractmethod def fixed_value () -> Union [ None , str , float ]: \"\"\"Method to return the fixed value in case of a fixed feature. Returns: Union[None,str,float]: None in case the feature is not fixed, else the fixed value. \"\"\" pass","title":"fixed_value()"},{"location":"ref-features/#bofire.domain.features.InputFeature.is_fixed","text":"Indicates if a variable is set to a fixed value. Returns: Type Description bool True if fixed, els False. Source code in bofire/domain/features.py @abstractmethod def is_fixed () -> bool : \"\"\"Indicates if a variable is set to a fixed value. Returns: bool: True if fixed, els False. \"\"\" pass","title":"is_fixed()"},{"location":"ref-features/#bofire.domain.features.InputFeature.sample","text":"Sample a series of allowed values. Parameters: Name Type Description Default n int Number of samples required Returns: Type Description pd.Series Sampled values. Source code in bofire/domain/features.py @abstractmethod def sample ( self , n : int ) -> pd . Series : \"\"\"Sample a series of allowed values. Args: n (int): Number of samples Returns: pd.Series: Sampled values. \"\"\" pass","title":"sample()"},{"location":"ref-features/#bofire.domain.features.InputFeature.validate_candidental","text":"Abstract method to validate the suggested candidates Parameters: Name Type Description Default values pd.Series A dataFrame with candidates required Returns: Type Description pd.Series The passed dataFrame with candidates Source code in bofire/domain/features.py @abstractmethod def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Abstract method to validate the suggested candidates Args: values (pd.Series): A dataFrame with candidates Returns: pd.Series: The passed dataFrame with candidates \"\"\" pass","title":"validate_candidental()"},{"location":"ref-features/#bofire.domain.features.InputFeature.validate_experimental","text":"Abstract method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Returns: Type Description pd.Series The passed dataFrame with experiments Source code in bofire/domain/features.py @abstractmethod def validate_experimental ( self , values : pd . Series , strict : bool = False ) -> pd . Series : \"\"\"Abstract method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Returns: pd.Series: The passed dataFrame with experiments \"\"\" pass","title":"validate_experimental()"},{"location":"ref-features/#bofire.domain.features.InputFeatures","text":"Container of input features, only input features are allowed. Attributes: Name Type Description features List(InputFeatures list of the features. Source code in bofire/domain/features.py class InputFeatures ( Features ): \"\"\"Container of input features, only input features are allowed. Attributes: features (List(InputFeatures)): list of the features. \"\"\" features : Sequence [ InputFeature ] = Field ( default_factory = lambda : []) def to_config ( self ) -> Dict : return { \"type\" : \"inputs\" , \"features\" : [ feat . to_config () for feat in self . features ], } def get_fixed ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if feat . is_fixed ()]) # type: ignore def get_free ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are not fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only non-fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if not feat . is_fixed ()]) # type: ignore @validate_arguments def sample ( self , n : Tnum_samples = 1 , method : SamplingMethodEnum = SamplingMethodEnum . UNIFORM , ) -> pd . DataFrame : \"\"\"Draw sobol samples Args: n (int, optional): Number of samples, has to be larger than 0. Defaults to 1. method (SamplingMethodEnum, optional): Method to use, implemented methods are `UNIFORM`, `SOBOL` and `LHS`. Defaults to `UNIFORM`. Returns: pd.DataFrame: Dataframe containing the samples. \"\"\" if method == SamplingMethodEnum . UNIFORM : return self . validate_inputs ( pd . concat ([ feat . sample ( n ) for feat in self . get ( InputFeature )], axis = 1 ) # type: ignore ) free_features = self . get_free () if method == SamplingMethodEnum . SOBOL : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( len ( free_features )) . random ( n ) else : X = LatinHypercube ( len ( free_features )) . random ( n ) res = [] for i , feat in enumerate ( free_features ): if isinstance ( feat , ContinuousInput ): x = feat . from_unit_range ( X [:, i ]) elif isinstance ( feat , ( DiscreteInput , CategoricalInput )): if isinstance ( feat , DiscreteInput ): levels = feat . values else : levels = feat . get_allowed_categories () bins = np . linspace ( 0 , 1 , len ( levels ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( levels )[ idx ] else : raise ( ValueError ( f \"Unknown input feature with key { feat . key } \" )) res . append ( pd . Series ( x , name = feat . key )) samples = pd . concat ( res , axis = 1 ) for feat in self . get_fixed (): samples [ feat . key ] = feat . fixed_value () # type: ignore return self . validate_inputs ( samples )[ self . get_keys ( InputFeature )] def validate_inputs ( self , inputs : pd . DataFrame ) -> pd . DataFrame : \"\"\"Validate a pandas dataframe with input feature values. Args: inputs (pd.Dataframe): Inputs to validate. Raises: ValueError: Raises a Valueerror if a feature based validation raises an exception. Returns: pd.Dataframe: Validated dataframe \"\"\" for feature in self : if feature . key not in inputs : raise ValueError ( f \"no col for input feature ` { feature . key } `\" ) feature . validate_candidental ( inputs [ feature . key ]) # type: ignore return inputs def get_categorical_combinations ( self , include : Type [ Feature ] = InputFeature , exclude : Optional [ Type [ InputFeature ]] = None , ): \"\"\"get a list of tuples pairing the feature keys with a list of valid categories Args: include (Feature, optional): Features to be included. Defaults to InputFeature. exclude (Feature, optional): Features to be excluded, e.g. subclasses of the included features. Defaults to None. Returns: List[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str) \"\"\" features = [ f for f in self . get ( includes = include , excludes = exclude ) if isinstance ( f , CategoricalInput ) and not f . is_fixed () ] list_of_lists = [ [( f . key , cat ) for cat in f . get_allowed_categories ()] for f in features ] return list ( itertools . product ( * list_of_lists ))","title":"InputFeatures"},{"location":"ref-features/#bofire.domain.features.InputFeatures.get_categorical_combinations","text":"get a list of tuples pairing the feature keys with a list of valid categories Parameters: Name Type Description Default include Feature Features to be included. Defaults to InputFeature. <class 'bofire.domain.features.InputFeature'> exclude Feature Features to be excluded, e.g. subclasses of the included features. Defaults to None. None Returns: Type Description List[(str, List[str])] Returns a list of tuples pairing the feature keys with a list of valid categories (str) Source code in bofire/domain/features.py def get_categorical_combinations ( self , include : Type [ Feature ] = InputFeature , exclude : Optional [ Type [ InputFeature ]] = None , ): \"\"\"get a list of tuples pairing the feature keys with a list of valid categories Args: include (Feature, optional): Features to be included. Defaults to InputFeature. exclude (Feature, optional): Features to be excluded, e.g. subclasses of the included features. Defaults to None. Returns: List[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str) \"\"\" features = [ f for f in self . get ( includes = include , excludes = exclude ) if isinstance ( f , CategoricalInput ) and not f . is_fixed () ] list_of_lists = [ [( f . key , cat ) for cat in f . get_allowed_categories ()] for f in features ] return list ( itertools . product ( * list_of_lists ))","title":"get_categorical_combinations()"},{"location":"ref-features/#bofire.domain.features.InputFeatures.get_fixed","text":"Gets all features in self that are fixed and returns them as new InputFeatures object. Returns: Type Description InputFeatures Input features object containing only fixed features. Source code in bofire/domain/features.py def get_fixed ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if feat . is_fixed ()]) # type: ignore","title":"get_fixed()"},{"location":"ref-features/#bofire.domain.features.InputFeatures.get_free","text":"Gets all features in self that are not fixed and returns them as new InputFeatures object. Returns: Type Description InputFeatures Input features object containing only non-fixed features. Source code in bofire/domain/features.py def get_free ( self ) -> \"InputFeatures\" : \"\"\"Gets all features in `self` that are not fixed and returns them as new `InputFeatures` object. Returns: InputFeatures: Input features object containing only non-fixed features. \"\"\" return InputFeatures ( features = [ feat for feat in self if not feat . is_fixed ()]) # type: ignore","title":"get_free()"},{"location":"ref-features/#bofire.domain.features.InputFeatures.sample","text":"Draw sobol samples Parameters: Name Type Description Default n int Number of samples, has to be larger than 0. Defaults to 1. 1 method SamplingMethodEnum Method to use, implemented methods are UNIFORM , SOBOL and LHS . Defaults to UNIFORM . <SamplingMethodEnum.UNIFORM: 'UNIFORM'> Returns: Type Description pd.DataFrame Dataframe containing the samples. Source code in bofire/domain/features.py @validate_arguments def sample ( self , n : Tnum_samples = 1 , method : SamplingMethodEnum = SamplingMethodEnum . UNIFORM , ) -> pd . DataFrame : \"\"\"Draw sobol samples Args: n (int, optional): Number of samples, has to be larger than 0. Defaults to 1. method (SamplingMethodEnum, optional): Method to use, implemented methods are `UNIFORM`, `SOBOL` and `LHS`. Defaults to `UNIFORM`. Returns: pd.DataFrame: Dataframe containing the samples. \"\"\" if method == SamplingMethodEnum . UNIFORM : return self . validate_inputs ( pd . concat ([ feat . sample ( n ) for feat in self . get ( InputFeature )], axis = 1 ) # type: ignore ) free_features = self . get_free () if method == SamplingMethodEnum . SOBOL : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" ) X = Sobol ( len ( free_features )) . random ( n ) else : X = LatinHypercube ( len ( free_features )) . random ( n ) res = [] for i , feat in enumerate ( free_features ): if isinstance ( feat , ContinuousInput ): x = feat . from_unit_range ( X [:, i ]) elif isinstance ( feat , ( DiscreteInput , CategoricalInput )): if isinstance ( feat , DiscreteInput ): levels = feat . values else : levels = feat . get_allowed_categories () bins = np . linspace ( 0 , 1 , len ( levels ) + 1 ) idx = np . digitize ( X [:, i ], bins ) - 1 x = np . array ( levels )[ idx ] else : raise ( ValueError ( f \"Unknown input feature with key { feat . key } \" )) res . append ( pd . Series ( x , name = feat . key )) samples = pd . concat ( res , axis = 1 ) for feat in self . get_fixed (): samples [ feat . key ] = feat . fixed_value () # type: ignore return self . validate_inputs ( samples )[ self . get_keys ( InputFeature )]","title":"sample()"},{"location":"ref-features/#bofire.domain.features.InputFeatures.to_config","text":"Serialize the features container. Returns: Type Description Dict serialized features container Source code in bofire/domain/features.py def to_config ( self ) -> Dict : return { \"type\" : \"inputs\" , \"features\" : [ feat . to_config () for feat in self . features ], }","title":"to_config()"},{"location":"ref-features/#bofire.domain.features.InputFeatures.validate_inputs","text":"Validate a pandas dataframe with input feature values. Parameters: Name Type Description Default inputs pd.Dataframe Inputs to validate. required Exceptions: Type Description ValueError Raises a Valueerror if a feature based validation raises an exception. Returns: Type Description pd.Dataframe Validated dataframe Source code in bofire/domain/features.py def validate_inputs ( self , inputs : pd . DataFrame ) -> pd . DataFrame : \"\"\"Validate a pandas dataframe with input feature values. Args: inputs (pd.Dataframe): Inputs to validate. Raises: ValueError: Raises a Valueerror if a feature based validation raises an exception. Returns: pd.Dataframe: Validated dataframe \"\"\" for feature in self : if feature . key not in inputs : raise ValueError ( f \"no col for input feature ` { feature . key } `\" ) feature . validate_candidental ( inputs [ feature . key ]) # type: ignore return inputs","title":"validate_inputs()"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature","text":"Abstracht base class for all numerical (ordinal) input features. Source code in bofire/domain/features.py class NumericalInputFeature ( InputFeature ): \"\"\"Abstracht base class for all numerical (ordinal) input features.\"\"\" def to_unit_range ( self , values : Union [ pd . Series , np . ndarray ], use_real_bounds : bool = False ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert to the unit range between 0 and 1. Args: values (pd.Series): values to be transformed use_real_bounds (bool, optional): if True, use the bounds from the actual values else the bounds from the feature. Defaults to False. Raises: ValueError: If lower_bound == upper bound an error is raised Returns: pd.Series: transformed values. \"\"\" if use_real_bounds : lower , upper = self . get_real_feature_bounds ( values ) else : lower , upper = self . lower_bound , self . upper_bound # type: ignore if lower == upper : raise ValueError ( \"Fixed feature cannot be transformed to unit range.\" ) valrange = upper - lower return ( values - lower ) / valrange def from_unit_range ( self , values : Union [ pd . Series , np . ndarray ] ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert from unit range. Args: values (pd.Series): values to transform from. Raises: ValueError: if the feature is fixed raise a value error. Returns: pd.Series: _description_ \"\"\" if self . is_fixed (): raise ValueError ( \"Fixed feature cannot be transformed from unit range.\" ) valrange = self . upper_bound - self . lower_bound # type: ignore return ( values * valrange ) + self . lower_bound # type: ignore def is_fixed ( self ): \"\"\"Method to check if the feature is fixed Returns: Boolean: True when the feature is fixed, false otherwise. \"\"\" # TODO: the bounds are declared in the derived classes, hence the type checks fail here :(. return self . lower_bound == self . upper_bound # type: ignore def fixed_value ( self ): \"\"\"Method to get the value to which the feature is fixed Returns: Float: Return the feature value or None if the feature is not fixed. \"\"\" if self . is_fixed (): return self . lower_bound # type: ignore else : return None def validate_experimental ( self , values : pd . Series , strict = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when a value is not numerical ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) if strict : lower , upper = self . get_real_feature_bounds ( values ) if lower == upper : raise ValueError ( f \"No variation present or planned for feature { self . key } . Remove it.\" ) return values def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Validate the suggested candidates for the feature. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Error is raised when one of the values is not numerical. Returns: pd.Series: the original provided candidates \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) return values def get_real_feature_bounds ( self , values : Union [ pd . Series , np . ndarray ] ) -> Tuple [ float , float ]: \"\"\"Method to extract the feature boundaries from the provided experimental data Args: values (pd.Series): Experimental data Returns: (float, float): Returns lower and upper bound based on the passed data \"\"\" lower = min ( self . lower_bound , values . min ()) # type: ignore upper = max ( self . upper_bound , values . max ()) # type: ignore return lower , upper","title":"NumericalInputFeature"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature.fixed_value","text":"Method to get the value to which the feature is fixed Returns: Type Description Float Return the feature value or None if the feature is not fixed. Source code in bofire/domain/features.py def fixed_value ( self ): \"\"\"Method to get the value to which the feature is fixed Returns: Float: Return the feature value or None if the feature is not fixed. \"\"\" if self . is_fixed (): return self . lower_bound # type: ignore else : return None","title":"fixed_value()"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature.from_unit_range","text":"Convert from unit range. Parameters: Name Type Description Default values pd.Series values to transform from. required Exceptions: Type Description ValueError if the feature is fixed raise a value error. Returns: Type Description pd.Series description Source code in bofire/domain/features.py def from_unit_range ( self , values : Union [ pd . Series , np . ndarray ] ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert from unit range. Args: values (pd.Series): values to transform from. Raises: ValueError: if the feature is fixed raise a value error. Returns: pd.Series: _description_ \"\"\" if self . is_fixed (): raise ValueError ( \"Fixed feature cannot be transformed from unit range.\" ) valrange = self . upper_bound - self . lower_bound # type: ignore return ( values * valrange ) + self . lower_bound # type: ignore","title":"from_unit_range()"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature.get_real_feature_bounds","text":"Method to extract the feature boundaries from the provided experimental data Parameters: Name Type Description Default values pd.Series Experimental data required Returns: Type Description (float, float) Returns lower and upper bound based on the passed data Source code in bofire/domain/features.py def get_real_feature_bounds ( self , values : Union [ pd . Series , np . ndarray ] ) -> Tuple [ float , float ]: \"\"\"Method to extract the feature boundaries from the provided experimental data Args: values (pd.Series): Experimental data Returns: (float, float): Returns lower and upper bound based on the passed data \"\"\" lower = min ( self . lower_bound , values . min ()) # type: ignore upper = max ( self . upper_bound , values . max ()) # type: ignore return lower , upper","title":"get_real_feature_bounds()"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature.is_fixed","text":"Method to check if the feature is fixed Returns: Type Description Boolean True when the feature is fixed, false otherwise. Source code in bofire/domain/features.py def is_fixed ( self ): \"\"\"Method to check if the feature is fixed Returns: Boolean: True when the feature is fixed, false otherwise. \"\"\" # TODO: the bounds are declared in the derived classes, hence the type checks fail here :(. return self . lower_bound == self . upper_bound # type: ignore","title":"is_fixed()"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature.to_unit_range","text":"Convert to the unit range between 0 and 1. Parameters: Name Type Description Default values pd.Series values to be transformed required use_real_bounds bool if True, use the bounds from the actual values else the bounds from the feature. Defaults to False. False Exceptions: Type Description ValueError If lower_bound == upper bound an error is raised Returns: Type Description pd.Series transformed values. Source code in bofire/domain/features.py def to_unit_range ( self , values : Union [ pd . Series , np . ndarray ], use_real_bounds : bool = False ) -> Union [ pd . Series , np . ndarray ]: \"\"\"Convert to the unit range between 0 and 1. Args: values (pd.Series): values to be transformed use_real_bounds (bool, optional): if True, use the bounds from the actual values else the bounds from the feature. Defaults to False. Raises: ValueError: If lower_bound == upper bound an error is raised Returns: pd.Series: transformed values. \"\"\" if use_real_bounds : lower , upper = self . get_real_feature_bounds ( values ) else : lower , upper = self . lower_bound , self . upper_bound # type: ignore if lower == upper : raise ValueError ( \"Fixed feature cannot be transformed to unit range.\" ) valrange = upper - lower return ( values - lower ) / valrange","title":"to_unit_range()"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature.validate_candidental","text":"Validate the suggested candidates for the feature. Parameters: Name Type Description Default values pd.Series suggested candidates for the feature required Exceptions: Type Description ValueError Error is raised when one of the values is not numerical. Returns: Type Description pd.Series the original provided candidates Source code in bofire/domain/features.py def validate_candidental ( self , values : pd . Series ) -> pd . Series : \"\"\"Validate the suggested candidates for the feature. Args: values (pd.Series): suggested candidates for the feature Raises: ValueError: Error is raised when one of the values is not numerical. Returns: pd.Series: the original provided candidates \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) return values","title":"validate_candidental()"},{"location":"ref-features/#bofire.domain.features.NumericalInputFeature.validate_experimental","text":"Method to validate the experimental dataFrame Parameters: Name Type Description Default values pd.Series A dataFrame with experiments required strict bool Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. False Exceptions: Type Description ValueError when a value is not numerical ValueError when there is no variation in a feature provided by the experimental data Returns: Type Description pd.Series A dataFrame with experiments Source code in bofire/domain/features.py def validate_experimental ( self , values : pd . Series , strict = False ) -> pd . Series : \"\"\"Method to validate the experimental dataFrame Args: values (pd.Series): A dataFrame with experiments strict (bool, optional): Boolean to distinguish if the occurence of fixed features in the dataset should be considered or not. Defaults to False. Raises: ValueError: when a value is not numerical ValueError: when there is no variation in a feature provided by the experimental data Returns: pd.Series: A dataFrame with experiments \"\"\" if not is_numeric ( values ): raise ValueError ( f \"not all values of input feature ` { self . key } ` are numerical\" ) if strict : lower , upper = self . get_real_feature_bounds ( values ) if lower == upper : raise ValueError ( f \"No variation present or planned for feature { self . key } . Remove it.\" ) return values","title":"validate_experimental()"},{"location":"ref-features/#bofire.domain.features.OutputFeature","text":"Base class for all output features. Attributes: Name Type Description key(str) Key of the Feature. Source code in bofire/domain/features.py class OutputFeature ( Feature ): \"\"\"Base class for all output features. Attributes: key(str): Key of the Feature. \"\"\" objective : Optional [ Objective ]","title":"OutputFeature"},{"location":"ref-features/#bofire.domain.features.OutputFeatures","text":"Container of output features, only output features are allowed. Attributes: Name Type Description features List(OutputFeatures list of the features. Source code in bofire/domain/features.py class OutputFeatures ( Features ): \"\"\"Container of output features, only output features are allowed. Attributes: features (List(OutputFeatures)): list of the features. \"\"\" features : Sequence [ OutputFeature ] = Field ( default_factory = lambda : []) def to_config ( self ) -> Dict : return { \"type\" : \"outputs\" , \"features\" : [ feat . to_config () for feat in self . features ], } @validator ( \"features\" , pre = True ) def validate_output_features ( cls , v , values ): for feat in v : if not isinstance ( feat , OutputFeature ): raise ValueError return v def get_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> \"OutputFeatures\" : \"\"\"Get output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[OutputFeature]: List of output features fitting to the passed requirements. \"\"\" if len ( self . features ) == 0 : return OutputFeatures ( features = []) else : # TODO: why only continuous output? return OutputFeatures ( features = sorted ( filter_by_attribute ( self . get ( ContinuousOutput ) . features , lambda of : of . objective , includes , excludes , exact , ) ) ) def get_keys_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Get keys of output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of output feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_by_objective ( includes , excludes , exact )] def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective for every Args: experiments (pd.DataFrame): Experiments for which the objectives should be evaluated. Returns: pd.DataFrame: Objective values for the experiments of interest. \"\"\" return pd . concat ( [ feat . objective ( experiments [[ feat . key ]]) # type: ignore for feat in self . features if feat . objective is not None ], axis = 1 , )","title":"OutputFeatures"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.__call__","text":"Evaluate the objective for every Parameters: Name Type Description Default experiments pd.DataFrame Experiments for which the objectives should be evaluated. required Returns: Type Description pd.DataFrame Objective values for the experiments of interest. Source code in bofire/domain/features.py def __call__ ( self , experiments : pd . DataFrame ) -> pd . DataFrame : \"\"\"Evaluate the objective for every Args: experiments (pd.DataFrame): Experiments for which the objectives should be evaluated. Returns: pd.DataFrame: Objective values for the experiments of interest. \"\"\" return pd . concat ( [ feat . objective ( experiments [[ feat . key ]]) # type: ignore for feat in self . features if feat . objective is not None ], axis = 1 , )","title":"__call__()"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.get_by_objective","text":"Get output features filtered by the type of the attached objective. Parameters: Name Type Description Default includes Union[List[TObjective], TObjective] Objective class or list of objective classes to be returned. Defaults to Objective. <class 'bofire.domain.objectives.Objective'> excludes Union[List[TObjective], TObjective, None] Objective class or list of specific objective classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[OutputFeature] List of output features fitting to the passed requirements. Source code in bofire/domain/features.py def get_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> \"OutputFeatures\" : \"\"\"Get output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[OutputFeature]: List of output features fitting to the passed requirements. \"\"\" if len ( self . features ) == 0 : return OutputFeatures ( features = []) else : # TODO: why only continuous output? return OutputFeatures ( features = sorted ( filter_by_attribute ( self . get ( ContinuousOutput ) . features , lambda of : of . objective , includes , excludes , exact , ) ) )","title":"get_by_objective()"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.get_keys_by_objective","text":"Get keys of output features filtered by the type of the attached objective. Parameters: Name Type Description Default includes Union[List[TObjective], TObjective] Objective class or list of objective classes to be returned. Defaults to Objective. <class 'bofire.domain.objectives.Objective'> excludes Union[List[TObjective], TObjective, None] Objective class or list of specific objective classes to be excluded from the return. Defaults to None. None exact bool Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. False Returns: Type Description List[str] List of output feature keys fitting to the passed requirements. Source code in bofire/domain/features.py def get_keys_by_objective ( self , includes : Union [ List [ Type [ Objective ]], Type [ Objective ]] = Objective , excludes : Union [ List [ Type [ Objective ]], Type [ Objective ], None ] = None , exact : bool = False , ) -> List [ str ]: \"\"\"Get keys of output features filtered by the type of the attached objective. Args: includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes to be returned. Defaults to Objective. excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None. exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False. Returns: List[str]: List of output feature keys fitting to the passed requirements. \"\"\" return [ f . key for f in self . get_by_objective ( includes , excludes , exact )]","title":"get_keys_by_objective()"},{"location":"ref-features/#bofire.domain.features.OutputFeatures.to_config","text":"Serialize the features container. Returns: Type Description Dict serialized features container Source code in bofire/domain/features.py def to_config ( self ) -> Dict : return { \"type\" : \"outputs\" , \"features\" : [ feat . to_config () for feat in self . features ], }","title":"to_config()"},{"location":"ref-features/#bofire.domain.features.is_continuous","text":"Checks if Feature is continous Parameters: Name Type Description Default var Feature Feature to be checked required Returns: Type Description bool True if continuous, else False Source code in bofire/domain/features.py def is_continuous ( var : Feature ) -> bool : \"\"\"Checks if Feature is continous Args: var (Feature): Feature to be checked Returns: bool: True if continuous, else False \"\"\" # TODO: generalize query via attribute continuousFeature (not existing yet!) if isinstance ( var , ContinuousInput ) or isinstance ( var , ContinuousOutput ): return True else : return False","title":"is_continuous()"},{"location":"ref-mappers/","text":"Opti Mappers","title":"Mappers"},{"location":"ref-mappers/#opti-mappers","text":"","title":"Opti Mappers"},{"location":"ref-objectives/","text":"Domain AbstractTargetObjective ( Objective ) pydantic-model Source code in bofire/domain/objectives.py class AbstractTargetObjective ( Objective ): w : TWeight target_value : float tolerance : TGe0 def plot_details ( self , ax ): \"\"\"Plot function highlighting the tolerance area of the objective Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" ax . axvline ( self . target_value , color = \"black\" ) ax . axvspan ( self . target_value - self . tolerance , self . target_value + self . tolerance , color = \"gray\" , alpha = 0.5 , ) return ax plot_details ( self , ax ) Plot function highlighting the tolerance area of the objective Parameters: Name Type Description Default ax matplotlib.axes.Axes Matplotlib axes object required Returns: Type Description matplotlib.axes.Axes The object to be plotted Source code in bofire/domain/objectives.py def plot_details ( self , ax ): \"\"\"Plot function highlighting the tolerance area of the objective Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" ax . axvline ( self . target_value , color = \"black\" ) ax . axvspan ( self . target_value - self . tolerance , self . target_value + self . tolerance , color = \"gray\" , alpha = 0.5 , ) return ax BotorchConstrainedObjective This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints. Source code in bofire/domain/objectives.py class BotorchConstrainedObjective : \"\"\"This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints.\"\"\" @abstractmethod def to_constraints ( self , idx : int ) -> List [ Callable [[ Tensor ], Tensor ]]: \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" pass to_constraints ( self , idx ) Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py @abstractmethod def to_constraints ( self , idx : int ) -> List [ Callable [[ Tensor ], Tensor ]]: \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" pass ConstantObjective ( Objective ) pydantic-model Constant objective to allow constrained output features which should not be optimized Attributes: Name Type Description w float float between zero and one for weighting the objective. value float constant return value Source code in bofire/domain/objectives.py class ConstantObjective ( Objective ): \"\"\"Constant objective to allow constrained output features which should not be optimized Attributes: w (float): float between zero and one for weighting the objective. value (float): constant return value \"\"\" w : TWeight value : float def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning the fixed value as reward Args: x (np.ndarray): An array of x values Returns: np.ndarray: An array of passed constants with the shape of the passed x values array. \"\"\" return np . ones ( x . shape ) * self . value __call__ ( self , x ) special The call function returning the fixed value as reward Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray An array of passed constants with the shape of the passed x values array. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning the fixed value as reward Args: x (np.ndarray): An array of x values Returns: np.ndarray: An array of passed constants with the shape of the passed x values array. \"\"\" return np . ones ( x . shape ) * self . value DeltaObjective ( IdentityObjective ) pydantic-model Class returning the difference between a reference value and identity as reward Attributes: Name Type Description w float float between zero and one for weighting the objective ref_point float Reference value. scale float Scaling factor for the difference. Defaults to one. Source code in bofire/domain/objectives.py class DeltaObjective ( IdentityObjective ): \"\"\"Class returning the difference between a reference value and identity as reward Attributes: w (float): float between zero and one for weighting the objective ref_point (float): Reference value. scale (float, optional): Scaling factor for the difference. Defaults to one. \"\"\" ref_point : float scale : float = 1 def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The difference between reference and the x value as reward, might be scaled with a passed scaling value \"\"\" return ( self . ref_point - x ) * self . scale __call__ ( self , x ) special The call function returning a reward for passed x values Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The difference between reference and the x value as reward, might be scaled with a passed scaling value Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The difference between reference and the x value as reward, might be scaled with a passed scaling value \"\"\" return ( self . ref_point - x ) * self . scale IdentityObjective ( Objective ) pydantic-model An objective returning the identity as reward. The return can be scaled, when a lower and upper bound are provided. Attributes: Name Type Description w float float between zero and one for weighting the objective lower_bound float Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound float Upper bound for normalizing the objective between zero and one. Defaults to one. Source code in bofire/domain/objectives.py class IdentityObjective ( Objective ): \"\"\"An objective returning the identity as reward. The return can be scaled, when a lower and upper bound are provided. Attributes: w (float): float between zero and one for weighting the objective lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one. \"\"\" w : TWeight lower_bound : float = 0 upper_bound : float = 1 @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validation function to ensure that lower bound is always greater the upper bound Args: values (Dict): The attributes of the class Raises: ValueError: when a lower bound higher than the upper bound is passed Returns: Dict: The attributes of the class \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound ) __call__ ( self , x ) special The call function returning a reward for passed x values Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The identity as reward, might be normalized to the passed lower and upper bounds Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound ) validate_lower_upper ( values ) classmethod Validation function to ensure that lower bound is always greater the upper bound Parameters: Name Type Description Default values Dict The attributes of the class required Exceptions: Type Description ValueError when a lower bound higher than the upper bound is passed Returns: Type Description Dict The attributes of the class Source code in bofire/domain/objectives.py @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validation function to ensure that lower bound is always greater the upper bound Args: values (Dict): The attributes of the class Raises: ValueError: when a lower bound higher than the upper bound is passed Returns: Dict: The attributes of the class \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values MaximizeObjective ( IdentityObjective ) pydantic-model Child class from the identity function without modifications, since the parent class is already defined as maximization Attributes: Name Type Description w float float between zero and one for weighting the objective lower_bound float Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound float Upper bound for normalizing the objective between zero and one. Defaults to one. Source code in bofire/domain/objectives.py class MaximizeObjective ( IdentityObjective ): \"\"\"Child class from the identity function without modifications, since the parent class is already defined as maximization Attributes: w (float): float between zero and one for weighting the objective lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one. \"\"\" pass MaximizeSigmoidObjective ( SigmoidObjective ) pydantic-model Class for a maximizing sigmoid objective Attributes: Name Type Description w float float between zero and one for weighting the objective. steepness float Steepness of the sigmoid function. Has to be greater than zero. tp float Turning point of the sigmoid function. Source code in bofire/domain/objectives.py class MaximizeSigmoidObjective ( SigmoidObjective ): \"\"\"Class for a maximizing sigmoid objective Attributes: w (float): float between zero and one for weighting the objective. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. tp (float): Turning point of the sigmoid function. \"\"\" def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp ))) def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp ) * - 1.0 ] __call__ ( self , x ) special The call function returning a sigmoid shaped reward for passed x values. Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp ))) to_constraints ( self , idx ) Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp ) * - 1.0 ] MinimizeObjective ( IdentityObjective ) pydantic-model Class returning the negative identity as reward. Attributes: Name Type Description w float float between zero and one for weighting the objective lower_bound float Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound float Upper bound for normalizing the objective between zero and one. Defaults to one. Source code in bofire/domain/objectives.py class MinimizeObjective ( IdentityObjective ): \"\"\"Class returning the negative identity as reward. Attributes: w (float): float between zero and one for weighting the objective lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one. \"\"\" def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return - 1.0 * ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound ) __call__ ( self , x ) special The call function returning a reward for passed x values Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The negative identity as reward, might be normalized to the passed lower and upper bounds Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return - 1.0 * ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound ) MinimizeSigmoidObjective ( SigmoidObjective ) pydantic-model Class for a minimizing a sigmoid objective Attributes: Name Type Description w float float between zero and one for weighting the objective. steepness float Steepness of the sigmoid function. Has to be greater than zero. tp float Turning point of the sigmoid function. Source code in bofire/domain/objectives.py class MinimizeSigmoidObjective ( SigmoidObjective ): \"\"\"Class for a minimizing a sigmoid objective Attributes: w (float): float between zero and one for weighting the objective. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. tp (float): Turning point of the sigmoid function. \"\"\" def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 - 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp ))) def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp )] __call__ ( self , x ) special The call function returning a sigmoid shaped reward for passed x values. Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 - 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp ))) to_constraints ( self , idx ) Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp )] Objective ( BaseModel ) pydantic-model The base class for all objectives Source code in bofire/domain/objectives.py class Objective ( BaseModel ): \"\"\"The base class for all objectives\"\"\" @abstractmethod def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"Abstract method to define the call function for the class Objective Args: x (np.ndarray): An array of x values Returns: np.ndarray: The desirability of the passed x values \"\"\" pass def plot_details ( self , ax ): \"\"\" Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" return ax def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the objective. Returns: Dict: Serialized version of the objective as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } @staticmethod def from_config ( config : Dict ) -> \"Objective\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of an objective. Returns: Objective: Instaniated objective of the type specified in the `config`. \"\"\" mapper = { \"MaximizeObjective\" : MaximizeObjective , \"MinimizeObjective\" : MinimizeObjective , \"DeltaObjective\" : DeltaObjective , \"MaximizeSigmoidObjective\" : MaximizeSigmoidObjective , \"MinimizeSigmoidObjective\" : MinimizeSigmoidObjective , \"ConstantObjective\" : ConstantObjective , \"TargetObjective\" : TargetObjective , \"CloseToTargetObjective\" : CloseToTargetObjective , } return mapper [ config [ \"type\" ]]( ** config ) __call__ ( self , x ) special Abstract method to define the call function for the class Objective Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The desirability of the passed x values Source code in bofire/domain/objectives.py @abstractmethod def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"Abstract method to define the call function for the class Objective Args: x (np.ndarray): An array of x values Returns: np.ndarray: The desirability of the passed x values \"\"\" pass from_config ( config ) staticmethod Generate objective out of serialized version. Parameters: Name Type Description Default config Dict Serialized version of an objective. required Returns: Type Description Objective Instaniated objective of the type specified in the config . Source code in bofire/domain/objectives.py @staticmethod def from_config ( config : Dict ) -> \"Objective\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of an objective. Returns: Objective: Instaniated objective of the type specified in the `config`. \"\"\" mapper = { \"MaximizeObjective\" : MaximizeObjective , \"MinimizeObjective\" : MinimizeObjective , \"DeltaObjective\" : DeltaObjective , \"MaximizeSigmoidObjective\" : MaximizeSigmoidObjective , \"MinimizeSigmoidObjective\" : MinimizeSigmoidObjective , \"ConstantObjective\" : ConstantObjective , \"TargetObjective\" : TargetObjective , \"CloseToTargetObjective\" : CloseToTargetObjective , } return mapper [ config [ \"type\" ]]( ** config ) plot_details ( self , ax ) Parameters: Name Type Description Default ax matplotlib.axes.Axes Matplotlib axes object required Returns: Type Description matplotlib.axes.Axes The object to be plotted Source code in bofire/domain/objectives.py def plot_details ( self , ax ): \"\"\" Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" return ax to_config ( self ) Generate serialized version of the objective. Returns: Type Description Dict Serialized version of the objective as dictionary. Source code in bofire/domain/objectives.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the objective. Returns: Dict: Serialized version of the objective as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } SigmoidObjective ( Objective , BotorchConstrainedObjective ) pydantic-model Base class for all sigmoid shaped objectives Attributes: Name Type Description w float float between zero and one for weighting the objective. steepness float Steepness of the sigmoid function. Has to be greater than zero. tp float Turning point of the sigmoid function. Source code in bofire/domain/objectives.py class SigmoidObjective ( Objective , BotorchConstrainedObjective ): \"\"\"Base class for all sigmoid shaped objectives Attributes: w (float): float between zero and one for weighting the objective. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. tp (float): Turning point of the sigmoid function. \"\"\" steepness : TGt0 tp : float w : TWeight TargetObjective ( AbstractTargetObjective , BotorchConstrainedObjective ) pydantic-model Class for objectives for optimizing towards a target value Attributes: Name Type Description w float float between zero and one for weighting the objective. target_value float target value that should be reached. tolerance float Tolerance for reaching the target. Has to be greater than zero. steepness float Steepness of the sigmoid function. Has to be greater than zero. Source code in bofire/domain/objectives.py class TargetObjective ( AbstractTargetObjective , BotorchConstrainedObjective ): \"\"\"Class for objectives for optimizing towards a target value Attributes: w (float): float between zero and one for weighting the objective. target_value (float): target value that should be reached. tolerance (float): Tolerance for reaching the target. Has to be greater than zero. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. \"\"\" steepness : TGt0 def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values. Args: x (np.array): An array of x values Returns: np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value. \"\"\" return ( 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value - self . tolerance )) ) ) * ( 1 - 1 / ( 1.0 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value + self . tolerance )) ) ) ) ) def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Here two callables are returned as the constraint is a product of the `MaximizeSigmoidObjective` and `MinimizeSigmoidObjective`. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - ( self . target_value - self . tolerance )) * - 1.0 , lambda Z : ( Z [ ... , idx ] - ( self . target_value + self . tolerance )), ] __call__ ( self , x ) special The call function returning a reward for passed x values. Parameters: Name Type Description Default x np.array An array of x values required Returns: Type Description np.array An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values. Args: x (np.array): An array of x values Returns: np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value. \"\"\" return ( 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value - self . tolerance )) ) ) * ( 1 - 1 / ( 1.0 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value + self . tolerance )) ) ) ) ) to_constraints ( self , idx ) Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Here two callables are returned as the constraint is a product of the MaximizeSigmoidObjective and MinimizeSigmoidObjective . Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Here two callables are returned as the constraint is a product of the `MaximizeSigmoidObjective` and `MinimizeSigmoidObjective`. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - ( self . target_value - self . tolerance )) * - 1.0 , lambda Z : ( Z [ ... , idx ] - ( self . target_value + self . tolerance )), ]","title":"Objectives"},{"location":"ref-objectives/#domain","text":"","title":"Domain"},{"location":"ref-objectives/#bofire.domain.objectives.AbstractTargetObjective","text":"Source code in bofire/domain/objectives.py class AbstractTargetObjective ( Objective ): w : TWeight target_value : float tolerance : TGe0 def plot_details ( self , ax ): \"\"\"Plot function highlighting the tolerance area of the objective Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" ax . axvline ( self . target_value , color = \"black\" ) ax . axvspan ( self . target_value - self . tolerance , self . target_value + self . tolerance , color = \"gray\" , alpha = 0.5 , ) return ax","title":"AbstractTargetObjective"},{"location":"ref-objectives/#bofire.domain.objectives.AbstractTargetObjective.plot_details","text":"Plot function highlighting the tolerance area of the objective Parameters: Name Type Description Default ax matplotlib.axes.Axes Matplotlib axes object required Returns: Type Description matplotlib.axes.Axes The object to be plotted Source code in bofire/domain/objectives.py def plot_details ( self , ax ): \"\"\"Plot function highlighting the tolerance area of the objective Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" ax . axvline ( self . target_value , color = \"black\" ) ax . axvspan ( self . target_value - self . tolerance , self . target_value + self . tolerance , color = \"gray\" , alpha = 0.5 , ) return ax","title":"plot_details()"},{"location":"ref-objectives/#bofire.domain.objectives.BotorchConstrainedObjective","text":"This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints. Source code in bofire/domain/objectives.py class BotorchConstrainedObjective : \"\"\"This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints.\"\"\" @abstractmethod def to_constraints ( self , idx : int ) -> List [ Callable [[ Tensor ], Tensor ]]: \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" pass","title":"BotorchConstrainedObjective"},{"location":"ref-objectives/#bofire.domain.objectives.BotorchConstrainedObjective.to_constraints","text":"Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py @abstractmethod def to_constraints ( self , idx : int ) -> List [ Callable [[ Tensor ], Tensor ]]: \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" pass","title":"to_constraints()"},{"location":"ref-objectives/#bofire.domain.objectives.ConstantObjective","text":"Constant objective to allow constrained output features which should not be optimized Attributes: Name Type Description w float float between zero and one for weighting the objective. value float constant return value Source code in bofire/domain/objectives.py class ConstantObjective ( Objective ): \"\"\"Constant objective to allow constrained output features which should not be optimized Attributes: w (float): float between zero and one for weighting the objective. value (float): constant return value \"\"\" w : TWeight value : float def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning the fixed value as reward Args: x (np.ndarray): An array of x values Returns: np.ndarray: An array of passed constants with the shape of the passed x values array. \"\"\" return np . ones ( x . shape ) * self . value","title":"ConstantObjective"},{"location":"ref-objectives/#bofire.domain.objectives.ConstantObjective.__call__","text":"The call function returning the fixed value as reward Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray An array of passed constants with the shape of the passed x values array. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning the fixed value as reward Args: x (np.ndarray): An array of x values Returns: np.ndarray: An array of passed constants with the shape of the passed x values array. \"\"\" return np . ones ( x . shape ) * self . value","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.DeltaObjective","text":"Class returning the difference between a reference value and identity as reward Attributes: Name Type Description w float float between zero and one for weighting the objective ref_point float Reference value. scale float Scaling factor for the difference. Defaults to one. Source code in bofire/domain/objectives.py class DeltaObjective ( IdentityObjective ): \"\"\"Class returning the difference between a reference value and identity as reward Attributes: w (float): float between zero and one for weighting the objective ref_point (float): Reference value. scale (float, optional): Scaling factor for the difference. Defaults to one. \"\"\" ref_point : float scale : float = 1 def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The difference between reference and the x value as reward, might be scaled with a passed scaling value \"\"\" return ( self . ref_point - x ) * self . scale","title":"DeltaObjective"},{"location":"ref-objectives/#bofire.domain.objectives.DeltaObjective.__call__","text":"The call function returning a reward for passed x values Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The difference between reference and the x value as reward, might be scaled with a passed scaling value Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The difference between reference and the x value as reward, might be scaled with a passed scaling value \"\"\" return ( self . ref_point - x ) * self . scale","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.IdentityObjective","text":"An objective returning the identity as reward. The return can be scaled, when a lower and upper bound are provided. Attributes: Name Type Description w float float between zero and one for weighting the objective lower_bound float Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound float Upper bound for normalizing the objective between zero and one. Defaults to one. Source code in bofire/domain/objectives.py class IdentityObjective ( Objective ): \"\"\"An objective returning the identity as reward. The return can be scaled, when a lower and upper bound are provided. Attributes: w (float): float between zero and one for weighting the objective lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one. \"\"\" w : TWeight lower_bound : float = 0 upper_bound : float = 1 @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validation function to ensure that lower bound is always greater the upper bound Args: values (Dict): The attributes of the class Raises: ValueError: when a lower bound higher than the upper bound is passed Returns: Dict: The attributes of the class \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound )","title":"IdentityObjective"},{"location":"ref-objectives/#bofire.domain.objectives.IdentityObjective.__call__","text":"The call function returning a reward for passed x values Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The identity as reward, might be normalized to the passed lower and upper bounds Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound )","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.IdentityObjective.validate_lower_upper","text":"Validation function to ensure that lower bound is always greater the upper bound Parameters: Name Type Description Default values Dict The attributes of the class required Exceptions: Type Description ValueError when a lower bound higher than the upper bound is passed Returns: Type Description Dict The attributes of the class Source code in bofire/domain/objectives.py @root_validator ( pre = False ) def validate_lower_upper ( cls , values ): \"\"\"Validation function to ensure that lower bound is always greater the upper bound Args: values (Dict): The attributes of the class Raises: ValueError: when a lower bound higher than the upper bound is passed Returns: Dict: The attributes of the class \"\"\" if values [ \"lower_bound\" ] > values [ \"upper_bound\" ]: raise ValueError ( f 'lower bound must be <= upper bound, got { values [ \"lower_bound\" ] } > { values [ \"upper_bound\" ] } ' ) return values","title":"validate_lower_upper()"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeObjective","text":"Child class from the identity function without modifications, since the parent class is already defined as maximization Attributes: Name Type Description w float float between zero and one for weighting the objective lower_bound float Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound float Upper bound for normalizing the objective between zero and one. Defaults to one. Source code in bofire/domain/objectives.py class MaximizeObjective ( IdentityObjective ): \"\"\"Child class from the identity function without modifications, since the parent class is already defined as maximization Attributes: w (float): float between zero and one for weighting the objective lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one. \"\"\" pass","title":"MaximizeObjective"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeSigmoidObjective","text":"Class for a maximizing sigmoid objective Attributes: Name Type Description w float float between zero and one for weighting the objective. steepness float Steepness of the sigmoid function. Has to be greater than zero. tp float Turning point of the sigmoid function. Source code in bofire/domain/objectives.py class MaximizeSigmoidObjective ( SigmoidObjective ): \"\"\"Class for a maximizing sigmoid objective Attributes: w (float): float between zero and one for weighting the objective. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. tp (float): Turning point of the sigmoid function. \"\"\" def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp ))) def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp ) * - 1.0 ]","title":"MaximizeSigmoidObjective"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeSigmoidObjective.__call__","text":"The call function returning a sigmoid shaped reward for passed x values. Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp )))","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.MaximizeSigmoidObjective.to_constraints","text":"Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp ) * - 1.0 ]","title":"to_constraints()"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeObjective","text":"Class returning the negative identity as reward. Attributes: Name Type Description w float float between zero and one for weighting the objective lower_bound float Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound float Upper bound for normalizing the objective between zero and one. Defaults to one. Source code in bofire/domain/objectives.py class MinimizeObjective ( IdentityObjective ): \"\"\"Class returning the negative identity as reward. Attributes: w (float): float between zero and one for weighting the objective lower_bound (float, optional): Lower bound for normalizing the objective between zero and one. Defaults to zero. upper_bound (float, optional): Upper bound for normalizing the objective between zero and one. Defaults to one. \"\"\" def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return - 1.0 * ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound )","title":"MinimizeObjective"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeObjective.__call__","text":"The call function returning a reward for passed x values Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The negative identity as reward, might be normalized to the passed lower and upper bounds Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values Args: x (np.ndarray): An array of x values Returns: np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds \"\"\" return - 1.0 * ( x - self . lower_bound ) / ( self . upper_bound - self . lower_bound )","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeSigmoidObjective","text":"Class for a minimizing a sigmoid objective Attributes: Name Type Description w float float between zero and one for weighting the objective. steepness float Steepness of the sigmoid function. Has to be greater than zero. tp float Turning point of the sigmoid function. Source code in bofire/domain/objectives.py class MinimizeSigmoidObjective ( SigmoidObjective ): \"\"\"Class for a minimizing a sigmoid objective Attributes: w (float): float between zero and one for weighting the objective. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. tp (float): Turning point of the sigmoid function. \"\"\" def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 - 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp ))) def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp )]","title":"MinimizeSigmoidObjective"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeSigmoidObjective.__call__","text":"The call function returning a sigmoid shaped reward for passed x values. Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a sigmoid shaped reward for passed x values. Args: x (np.ndarray): An array of x values Returns: np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments. \"\"\" return 1 - 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - self . tp )))","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.MinimizeSigmoidObjective.to_constraints","text":"Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - self . tp )]","title":"to_constraints()"},{"location":"ref-objectives/#bofire.domain.objectives.Objective","text":"The base class for all objectives Source code in bofire/domain/objectives.py class Objective ( BaseModel ): \"\"\"The base class for all objectives\"\"\" @abstractmethod def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"Abstract method to define the call function for the class Objective Args: x (np.ndarray): An array of x values Returns: np.ndarray: The desirability of the passed x values \"\"\" pass def plot_details ( self , ax ): \"\"\" Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" return ax def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the objective. Returns: Dict: Serialized version of the objective as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), } @staticmethod def from_config ( config : Dict ) -> \"Objective\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of an objective. Returns: Objective: Instaniated objective of the type specified in the `config`. \"\"\" mapper = { \"MaximizeObjective\" : MaximizeObjective , \"MinimizeObjective\" : MinimizeObjective , \"DeltaObjective\" : DeltaObjective , \"MaximizeSigmoidObjective\" : MaximizeSigmoidObjective , \"MinimizeSigmoidObjective\" : MinimizeSigmoidObjective , \"ConstantObjective\" : ConstantObjective , \"TargetObjective\" : TargetObjective , \"CloseToTargetObjective\" : CloseToTargetObjective , } return mapper [ config [ \"type\" ]]( ** config )","title":"Objective"},{"location":"ref-objectives/#bofire.domain.objectives.Objective.__call__","text":"Abstract method to define the call function for the class Objective Parameters: Name Type Description Default x np.ndarray An array of x values required Returns: Type Description np.ndarray The desirability of the passed x values Source code in bofire/domain/objectives.py @abstractmethod def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"Abstract method to define the call function for the class Objective Args: x (np.ndarray): An array of x values Returns: np.ndarray: The desirability of the passed x values \"\"\" pass","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.Objective.from_config","text":"Generate objective out of serialized version. Parameters: Name Type Description Default config Dict Serialized version of an objective. required Returns: Type Description Objective Instaniated objective of the type specified in the config . Source code in bofire/domain/objectives.py @staticmethod def from_config ( config : Dict ) -> \"Objective\" : \"\"\"Generate objective out of serialized version. Args: config (Dict): Serialized version of an objective. Returns: Objective: Instaniated objective of the type specified in the `config`. \"\"\" mapper = { \"MaximizeObjective\" : MaximizeObjective , \"MinimizeObjective\" : MinimizeObjective , \"DeltaObjective\" : DeltaObjective , \"MaximizeSigmoidObjective\" : MaximizeSigmoidObjective , \"MinimizeSigmoidObjective\" : MinimizeSigmoidObjective , \"ConstantObjective\" : ConstantObjective , \"TargetObjective\" : TargetObjective , \"CloseToTargetObjective\" : CloseToTargetObjective , } return mapper [ config [ \"type\" ]]( ** config )","title":"from_config()"},{"location":"ref-objectives/#bofire.domain.objectives.Objective.plot_details","text":"Parameters: Name Type Description Default ax matplotlib.axes.Axes Matplotlib axes object required Returns: Type Description matplotlib.axes.Axes The object to be plotted Source code in bofire/domain/objectives.py def plot_details ( self , ax ): \"\"\" Args: ax (matplotlib.axes.Axes): Matplotlib axes object Returns: matplotlib.axes.Axes: The object to be plotted \"\"\" return ax","title":"plot_details()"},{"location":"ref-objectives/#bofire.domain.objectives.Objective.to_config","text":"Generate serialized version of the objective. Returns: Type Description Dict Serialized version of the objective as dictionary. Source code in bofire/domain/objectives.py def to_config ( self ) -> Dict : \"\"\"Generate serialized version of the objective. Returns: Dict: Serialized version of the objective as dictionary. \"\"\" return { \"type\" : self . __class__ . __name__ , ** self . dict (), }","title":"to_config()"},{"location":"ref-objectives/#bofire.domain.objectives.SigmoidObjective","text":"Base class for all sigmoid shaped objectives Attributes: Name Type Description w float float between zero and one for weighting the objective. steepness float Steepness of the sigmoid function. Has to be greater than zero. tp float Turning point of the sigmoid function. Source code in bofire/domain/objectives.py class SigmoidObjective ( Objective , BotorchConstrainedObjective ): \"\"\"Base class for all sigmoid shaped objectives Attributes: w (float): float between zero and one for weighting the objective. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. tp (float): Turning point of the sigmoid function. \"\"\" steepness : TGt0 tp : float w : TWeight","title":"SigmoidObjective"},{"location":"ref-objectives/#bofire.domain.objectives.TargetObjective","text":"Class for objectives for optimizing towards a target value Attributes: Name Type Description w float float between zero and one for weighting the objective. target_value float target value that should be reached. tolerance float Tolerance for reaching the target. Has to be greater than zero. steepness float Steepness of the sigmoid function. Has to be greater than zero. Source code in bofire/domain/objectives.py class TargetObjective ( AbstractTargetObjective , BotorchConstrainedObjective ): \"\"\"Class for objectives for optimizing towards a target value Attributes: w (float): float between zero and one for weighting the objective. target_value (float): target value that should be reached. tolerance (float): Tolerance for reaching the target. Has to be greater than zero. steepness (float): Steepness of the sigmoid function. Has to be greater than zero. \"\"\" steepness : TGt0 def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values. Args: x (np.array): An array of x values Returns: np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value. \"\"\" return ( 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value - self . tolerance )) ) ) * ( 1 - 1 / ( 1.0 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value + self . tolerance )) ) ) ) ) def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Here two callables are returned as the constraint is a product of the `MaximizeSigmoidObjective` and `MinimizeSigmoidObjective`. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - ( self . target_value - self . tolerance )) * - 1.0 , lambda Z : ( Z [ ... , idx ] - ( self . target_value + self . tolerance )), ]","title":"TargetObjective"},{"location":"ref-objectives/#bofire.domain.objectives.TargetObjective.__call__","text":"The call function returning a reward for passed x values. Parameters: Name Type Description Default x np.array An array of x values required Returns: Type Description np.array An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value. Source code in bofire/domain/objectives.py def __call__ ( self , x : Union [ pd . Series , np . ndarray ]) -> Union [ pd . Series , np . ndarray ]: \"\"\"The call function returning a reward for passed x values. Args: x (np.array): An array of x values Returns: np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value. \"\"\" return ( 1 / ( 1 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value - self . tolerance )) ) ) * ( 1 - 1 / ( 1.0 + np . exp ( - 1 * self . steepness * ( x - ( self . target_value + self . tolerance )) ) ) ) )","title":"__call__()"},{"location":"ref-objectives/#bofire.domain.objectives.TargetObjective.to_constraints","text":"Create a callable that can be used by botorch.utils.objective.apply_constraints to setup ouput constrained optimizations. Here two callables are returned as the constraint is a product of the MaximizeSigmoidObjective and MinimizeSigmoidObjective . Parameters: Name Type Description Default idx int Index of the constraint objective in the list of outputs. required Returns: Type Description List[Callable[[Tensor], Tensor]] List of callables that can be used by botorch for setting up the constrained objective. Source code in bofire/domain/objectives.py def to_constraints ( self , idx : int ): \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints` to setup ouput constrained optimizations. Here two callables are returned as the constraint is a product of the `MaximizeSigmoidObjective` and `MinimizeSigmoidObjective`. Args: idx (int): Index of the constraint objective in the list of outputs. Returns: List[Callable[[Tensor], Tensor]]: List of callables that can be used by botorch for setting up the constrained objective. \"\"\" return [ lambda Z : ( Z [ ... , idx ] - ( self . target_value - self . tolerance )) * - 1.0 , lambda Z : ( Z [ ... , idx ] - ( self . target_value + self . tolerance )), ]","title":"to_constraints()"},{"location":"ref-utils/","text":"Utils categoricalDescriptorEncoder CategoricalDescriptorEncoder ( _BaseEncoder ) Encoder to translate categorical parameters into continuous descriptor values. Source code in bofire/utils/categoricalDescriptorEncoder.py class CategoricalDescriptorEncoder ( _BaseEncoder ): \"\"\" Encoder to translate categorical parameters into continuous descriptor values. \"\"\" def __init__ ( self , * , categories : Union [ str , List [ List [ str ]]] = \"auto\" , descriptors : Union [ str , List [ List [ str ]]] = \"auto\" , values : List [ List [ List [ float ]]], sparse : bool = False , dtype = np . float64 , handle_unknown = \"error\" , ): \"\"\"Encoder to translate categorical parameters into continuous descriptor values. Args: values (List[List[List[float]]]): Nested list of descriptor values. Must be of shape (n_features, n_categories_per_feature, n_descriptors). categories (List[List[str]], optional): List of strings referring to the categories (not the feature names!) occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) descriptors (List[List[str]], optional): List of strings referring to the descriptor names occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) sparse (bool, optional): Sparse matrix output is currently not supported. Defaults to False. dtype (dtype, optional): [description]. Defaults to np.float64. handle_unknown (str, optional): Allows to distinguish between \"error\" and \"ignore\". When \"ignore\", onknown categories are encoded as zeros. Defaults to \"error\". \"\"\" self . categories = categories self . descriptors = descriptors self . values = values # check, whether we have only 1D data or multiple descriptors provided at once if not isinstance ( self . values [ 0 ][ 0 ], list ): self . values = [ values ] if not isinstance ( self . categories [ 0 ], list ) and self . categories != \"auto\" : self . categories = [ categories ] if not isinstance ( self . descriptors [ 0 ], list ) and self . descriptors != \"auto\" : self . descriptors = [ descriptors ] self . sparse = sparse self . dtype = dtype self . handle_unknown = handle_unknown def _validate_keywords ( self ): \"\"\"Validate `handle_unknown` argument. Raises: ValueError: if `handle_unknown`not `error` or `ignore`. \"\"\" if self . handle_unknown not in ( \"error\" , \"ignore\" ): msg = ( \"handle_unknown should be either 'error' or 'ignore', got {0} .\" . format ( self . handle_unknown ) ) raise ValueError ( msg ) def _fit ( self , X , handle_unknown = \"error\" , force_all_finite : Union [ str , bool ] = True ): self . _check_n_features ( X , reset = True ) self . _check_feature_names ( X , reset = True ) X_list , n_samples , n_features = self . _check_X ( X , force_all_finite = force_all_finite # type: ignore ) self . n_features_in_ = n_features if self . categories != \"auto\" : if len ( self . categories ) != n_features : raise ValueError ( \"Shape mismatch: if categories is an array,\" \" it has to be of shape (n_features,).\" ) if self . descriptors != \"auto\" : for i , des in enumerate ( self . descriptors ): if len ( des ) != len ( self . values [ i ][ 0 ]): raise ValueError ( \"Shape mismatch: number of descriptors\" \" do not fit to the dimension of provided values.\" ) self . values_ = [] self . n_categories_i = [] self . n_descriptors_in_ = [] for values in self . values : descriptor_list , n_categories_i , n_descriptors = self . _check_X ( values , force_all_finite = force_all_finite # type: ignore ) self . values_ . append ( descriptor_list ) self . n_categories_i . append ( n_categories_i ) self . n_descriptors_in_ . append ( n_descriptors ) self . categories_ = [] self . descriptors_ = [] for i in range ( n_features ): Xi = X_list [ i ] if self . categories == \"auto\" : cats = _unique ( Xi ) else : cats = np . array ( self . categories [ i ], dtype = Xi . dtype ) if Xi . dtype . kind not in \"OUS\" : sorted_cats = np . sort ( cats ) error_msg = ( \"Unsorted categories are not supported for numerical categories\" ) # if there are nans, nan should be the last element stop_idx = - 1 if np . isnan ( sorted_cats [ - 1 ]) else None if np . any ( sorted_cats [: stop_idx ] != cats [: stop_idx ]) or ( np . isnan ( sorted_cats [ - 1 ]) and not np . isnan ( sorted_cats [ - 1 ]) ): raise ValueError ( error_msg ) if handle_unknown == \"error\" : diff = _check_unknown ( Xi , cats ) if diff : msg = ( \"Found unknown categories {0} in column {1} \" \" during fit\" . format ( diff , i ) ) raise ValueError ( msg ) self . categories_ . append ( cats ) des_list = [] for j in range ( self . n_descriptors_in_ [ i ]): if len ( self . values_ [ i ][ j ]) != len ( cats ): raise ValueError ( \"Shape mismatch: descriptor values has to be of shape (n_categories_per_feature, n_descriptors).\" ) if self . descriptors == \"auto\" : des_list . append ( f \"Descriptor_ { i } _ { j } \" ) else : des_list . append ( np . array ( self . descriptors [ i ][ j ])) self . descriptors_ . append ( des_list ) def fit ( self , X , y = None ): \"\"\" Fit Encoder to X. Args: X (array-like of shape (n_samples, n_features)): The data to determine the categories of each feature. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: self: Fitted encoder. \"\"\" self . _validate_keywords () self . _fit ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" ) return self def fit_transform ( self , X , y = None ): \"\"\" Fit categoricalDescriptorEncoder to X, then transform X. Equivalent to fit(X).transform(X) but more convenient. Args: X (array-like of shape (n_samples, n_features)): The data to encode. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: X_out ({ndarray, matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" self . _validate_keywords () return super () . fit_transform ( X , y ) def transform ( self , X ): \"\"\" Transform X using descriptors. Args: X (array-like of shape (n_samples, n_features)): The data to encode. Returns: X_out ({ndarray, sparse matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" check_is_fitted ( self ) # validation of X happens in _check_X called by _transform warn_on_unknown = self . handle_unknown == \"ignore\" X_int , X_mask = self . _transform ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" , # type: ignore warn_on_unknown = warn_on_unknown , ) n_samples = X . shape [ 0 ] n_values = [ len ( cats ) for cats in self . categories_ ] X_tr = np . zeros (( n_samples , sum ( self . n_descriptors_in_ )), dtype = float ) for c , categories in enumerate ( self . categories_ ): for i , category in enumerate ( categories ): for j in range ( self . n_descriptors_in_ [ c ]): col = sum ( self . n_descriptors_in_ [: c ]) # insert values to categorical data row = np . where ( X == category )[ 0 ] X_tr [ row , col + j ] = self . values_ [ c ][ j ][ i ] ### check whats going on here ### #TODO: sparse matrix is currently not supported! mask = X_mask . ravel () feature_indices = np . cumsum ([ 0 ] + n_values ) indices = ( X_int + feature_indices [: - 1 ]) . ravel ()[ mask ] indptr = np . empty ( n_samples + 1 , dtype = int ) indptr [ 0 ] = 0 np . sum ( X_mask , axis = 1 , out = indptr [ 1 :]) np . cumsum ( indptr [ 1 :], out = indptr [ 1 :]) data = np . ones ( indptr [ - 1 ]) _ = sparse . csr_matrix ( ( data , indices , indptr ), shape = ( n_samples , feature_indices [ - 1 ]), dtype = self . dtype , ) #################################### if not self . sparse : return X_tr else : raise NotImplementedError ( \"Sparse matrices as output are not implemented yet\" ) # return X_tr_sparse def inverse_transform ( self , X ): \"\"\" Convert the data back to the original representation. Args: X ({array-like, sparse matrix} of shape (n_samples, n_encoded_features*n_descriptors_per_feature)): The transformed data. Returns: X_tr (ndarray of shape (n_samples, n_features)): Inverse transformed array. \"\"\" check_is_fitted ( self ) X = check_array ( X , accept_sparse = \"csr\" ) n_samples = X . shape [ 0 ] # validate shape of passed X msg = ( \"Shape of the passed X data is not correct. Expected {0} columns, got {1} .\" ) if X . shape [ 1 ] != sum ( self . n_descriptors_in_ ): raise ValueError ( msg . format ( len ( self . descriptors ), X . shape [ 1 ])) X_tr = np . empty (( n_samples , self . n_features_in_ ), dtype = object ) for c , categories in enumerate ( self . categories_ ): indices = np . cumsum ([ 0 ] + self . n_descriptors_in_ ) var_descriptor_conditions = X [:, indices [ c ] : indices [ c + 1 ]] var_descriptor_orig_data = np . column_stack ( self . values_ [ c ]) var_categorical_transformed = [] # Find the closest points by euclidean distance for i in range ( n_samples ): # Euclidean distance calculation eucl_distance_squ = np . sum ( np . square ( np . subtract ( var_descriptor_orig_data , var_descriptor_conditions [ i , :], ) ), axis = 1 , ) # Choose closest point category_index = np . where ( eucl_distance_squ == np . min ( eucl_distance_squ ) )[ 0 ][ 0 ] # Find the matching name of the categorical variable category = categories [ category_index ] # Add the original categorical variables name to the dataset var_categorical_transformed . append ( category ) X_tr [:, c ] = np . asarray ( var_categorical_transformed ) return X_tr def get_feature_names_out ( self , input_features = None ): \"\"\"Get output feature names for transformation. Args: input_features (array-like of str or None): default=None. Input features. - If `input_features` is `None`, then `feature_names_in_` is used as feature names in. If `feature_names_in_` is not defined, then names are generated: `[x0, x1, ..., x(n_features_in_)]`. - If `input_features` is an array-like, then `input_features` must match `feature_names_in_` if `feature_names_in_` is defined. Returns: feature_names_out (ndarray(str)): Transformed feature names. \"\"\" check_is_fitted ( self ) desc = self . descriptors_ input_features = _check_feature_names_in ( self , input_features ) feature_names = [] for i in range ( len ( desc )): names = [ input_features [ i ] + \"_\" + str ( t ) for t in desc [ i ]] feature_names . extend ( names ) return np . asarray ( feature_names , dtype = object ) __init__ ( self , * , categories = 'auto' , descriptors = 'auto' , values , sparse = False , dtype =< class ' numpy . float64 '>, handle_unknown=' error ') special Encoder to translate categorical parameters into continuous descriptor values. Parameters: Name Type Description Default values List[List[List[float]]] Nested list of descriptor values. Must be of shape (n_features, n_categories_per_feature, n_descriptors). required categories List[List[str]] List of strings referring to the categories (not the feature names!) occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) 'auto' descriptors List[List[str]] List of strings referring to the descriptor names occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) 'auto' sparse bool Sparse matrix output is currently not supported. Defaults to False. False dtype dtype [description]. Defaults to np.float64. <class 'numpy.float64'> handle_unknown str Allows to distinguish between \"error\" and \"ignore\". When \"ignore\", onknown categories are encoded as zeros. Defaults to \"error\". 'error' Source code in bofire/utils/categoricalDescriptorEncoder.py def __init__ ( self , * , categories : Union [ str , List [ List [ str ]]] = \"auto\" , descriptors : Union [ str , List [ List [ str ]]] = \"auto\" , values : List [ List [ List [ float ]]], sparse : bool = False , dtype = np . float64 , handle_unknown = \"error\" , ): \"\"\"Encoder to translate categorical parameters into continuous descriptor values. Args: values (List[List[List[float]]]): Nested list of descriptor values. Must be of shape (n_features, n_categories_per_feature, n_descriptors). categories (List[List[str]], optional): List of strings referring to the categories (not the feature names!) occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) descriptors (List[List[str]], optional): List of strings referring to the descriptor names occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) sparse (bool, optional): Sparse matrix output is currently not supported. Defaults to False. dtype (dtype, optional): [description]. Defaults to np.float64. handle_unknown (str, optional): Allows to distinguish between \"error\" and \"ignore\". When \"ignore\", onknown categories are encoded as zeros. Defaults to \"error\". \"\"\" self . categories = categories self . descriptors = descriptors self . values = values # check, whether we have only 1D data or multiple descriptors provided at once if not isinstance ( self . values [ 0 ][ 0 ], list ): self . values = [ values ] if not isinstance ( self . categories [ 0 ], list ) and self . categories != \"auto\" : self . categories = [ categories ] if not isinstance ( self . descriptors [ 0 ], list ) and self . descriptors != \"auto\" : self . descriptors = [ descriptors ] self . sparse = sparse self . dtype = dtype self . handle_unknown = handle_unknown fit ( self , X , y = None ) Fit Encoder to X. Parameters: Name Type Description Default X array-like of shape (n_samples, n_features The data to determine the categories of each feature. required y None. Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . None Returns: Type Description self Fitted encoder. Source code in bofire/utils/categoricalDescriptorEncoder.py def fit ( self , X , y = None ): \"\"\" Fit Encoder to X. Args: X (array-like of shape (n_samples, n_features)): The data to determine the categories of each feature. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: self: Fitted encoder. \"\"\" self . _validate_keywords () self . _fit ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" ) return self fit_transform ( self , X , y = None ) Fit categoricalDescriptorEncoder to X, then transform X. Equivalent to fit(X).transform(X) but more convenient. Parameters: Name Type Description Default X array-like of shape (n_samples, n_features The data to encode. required y None. Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . None Returns: Type Description X_out ({ndarray, matrix} of shape (n_samples, n_encoded_features)) Transformed input. Source code in bofire/utils/categoricalDescriptorEncoder.py def fit_transform ( self , X , y = None ): \"\"\" Fit categoricalDescriptorEncoder to X, then transform X. Equivalent to fit(X).transform(X) but more convenient. Args: X (array-like of shape (n_samples, n_features)): The data to encode. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: X_out ({ndarray, matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" self . _validate_keywords () return super () . fit_transform ( X , y ) get_feature_names_out ( self , input_features = None ) Get output feature names for transformation. Parameters: Name Type Description Default input_features array-like of str or None default=None. Input features. - If input_features is None , then feature_names_in_ is used as feature names in. If feature_names_in_ is not defined, then names are generated: [x0, x1, ..., x(n_features_in_)] . - If input_features is an array-like, then input_features must match feature_names_in_ if feature_names_in_ is defined. None Returns: Type Description feature_names_out (ndarray(str)) Transformed feature names. Source code in bofire/utils/categoricalDescriptorEncoder.py def get_feature_names_out ( self , input_features = None ): \"\"\"Get output feature names for transformation. Args: input_features (array-like of str or None): default=None. Input features. - If `input_features` is `None`, then `feature_names_in_` is used as feature names in. If `feature_names_in_` is not defined, then names are generated: `[x0, x1, ..., x(n_features_in_)]`. - If `input_features` is an array-like, then `input_features` must match `feature_names_in_` if `feature_names_in_` is defined. Returns: feature_names_out (ndarray(str)): Transformed feature names. \"\"\" check_is_fitted ( self ) desc = self . descriptors_ input_features = _check_feature_names_in ( self , input_features ) feature_names = [] for i in range ( len ( desc )): names = [ input_features [ i ] + \"_\" + str ( t ) for t in desc [ i ]] feature_names . extend ( names ) return np . asarray ( feature_names , dtype = object ) inverse_transform ( self , X ) Convert the data back to the original representation. Parameters: Name Type Description Default X {array-like, sparse matrix} of shape (n_samples, n_encoded_features*n_descriptors_per_feature The transformed data. required Returns: Type Description X_tr (ndarray of shape (n_samples, n_features)) Inverse transformed array. Source code in bofire/utils/categoricalDescriptorEncoder.py def inverse_transform ( self , X ): \"\"\" Convert the data back to the original representation. Args: X ({array-like, sparse matrix} of shape (n_samples, n_encoded_features*n_descriptors_per_feature)): The transformed data. Returns: X_tr (ndarray of shape (n_samples, n_features)): Inverse transformed array. \"\"\" check_is_fitted ( self ) X = check_array ( X , accept_sparse = \"csr\" ) n_samples = X . shape [ 0 ] # validate shape of passed X msg = ( \"Shape of the passed X data is not correct. Expected {0} columns, got {1} .\" ) if X . shape [ 1 ] != sum ( self . n_descriptors_in_ ): raise ValueError ( msg . format ( len ( self . descriptors ), X . shape [ 1 ])) X_tr = np . empty (( n_samples , self . n_features_in_ ), dtype = object ) for c , categories in enumerate ( self . categories_ ): indices = np . cumsum ([ 0 ] + self . n_descriptors_in_ ) var_descriptor_conditions = X [:, indices [ c ] : indices [ c + 1 ]] var_descriptor_orig_data = np . column_stack ( self . values_ [ c ]) var_categorical_transformed = [] # Find the closest points by euclidean distance for i in range ( n_samples ): # Euclidean distance calculation eucl_distance_squ = np . sum ( np . square ( np . subtract ( var_descriptor_orig_data , var_descriptor_conditions [ i , :], ) ), axis = 1 , ) # Choose closest point category_index = np . where ( eucl_distance_squ == np . min ( eucl_distance_squ ) )[ 0 ][ 0 ] # Find the matching name of the categorical variable category = categories [ category_index ] # Add the original categorical variables name to the dataset var_categorical_transformed . append ( category ) X_tr [:, c ] = np . asarray ( var_categorical_transformed ) return X_tr transform ( self , X ) Transform X using descriptors. Parameters: Name Type Description Default X array-like of shape (n_samples, n_features The data to encode. required Returns: Type Description X_out ({ndarray, sparse matrix} of shape (n_samples, n_encoded_features)) Transformed input. Source code in bofire/utils/categoricalDescriptorEncoder.py def transform ( self , X ): \"\"\" Transform X using descriptors. Args: X (array-like of shape (n_samples, n_features)): The data to encode. Returns: X_out ({ndarray, sparse matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" check_is_fitted ( self ) # validation of X happens in _check_X called by _transform warn_on_unknown = self . handle_unknown == \"ignore\" X_int , X_mask = self . _transform ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" , # type: ignore warn_on_unknown = warn_on_unknown , ) n_samples = X . shape [ 0 ] n_values = [ len ( cats ) for cats in self . categories_ ] X_tr = np . zeros (( n_samples , sum ( self . n_descriptors_in_ )), dtype = float ) for c , categories in enumerate ( self . categories_ ): for i , category in enumerate ( categories ): for j in range ( self . n_descriptors_in_ [ c ]): col = sum ( self . n_descriptors_in_ [: c ]) # insert values to categorical data row = np . where ( X == category )[ 0 ] X_tr [ row , col + j ] = self . values_ [ c ][ j ][ i ] ### check whats going on here ### #TODO: sparse matrix is currently not supported! mask = X_mask . ravel () feature_indices = np . cumsum ([ 0 ] + n_values ) indices = ( X_int + feature_indices [: - 1 ]) . ravel ()[ mask ] indptr = np . empty ( n_samples + 1 , dtype = int ) indptr [ 0 ] = 0 np . sum ( X_mask , axis = 1 , out = indptr [ 1 :]) np . cumsum ( indptr [ 1 :], out = indptr [ 1 :]) data = np . ones ( indptr [ - 1 ]) _ = sparse . csr_matrix ( ( data , indices , indptr ), shape = ( n_samples , feature_indices [ - 1 ]), dtype = self . dtype , ) #################################### if not self . sparse : return X_tr else : raise NotImplementedError ( \"Sparse matrices as output are not implemented yet\" ) # return X_tr_sparse enum AcquisitionFunctionEnum ( Enum ) An enumeration. Source code in bofire/utils/enum.py class AcquisitionFunctionEnum ( Enum ): QNEI = \"qNEI\" QUCB = \"qUCB\" QEI = \"qEI\" QPI = \"qPI\" QSR = \"qSR\" # QEHVI = \"qEHVI\" # QNEHVI = \"qNEHVI\" CategoricalEncodingEnum ( Enum ) Enumeration class of implemented categorical encodings Currently, one-hot and ordinal encoding are implemented. Source code in bofire/utils/enum.py class CategoricalEncodingEnum ( Enum ): \"\"\"Enumeration class of implemented categorical encodings Currently, one-hot and ordinal encoding are implemented. \"\"\" ONE_HOT = \"ONE_HOT\" ORDINAL = \"ORDINAL\" CategoricalMethodEnum ( Enum ) Enumeration class of supported methods how to handle categorical features Currently, exhaustive search and free relaxation are implemented. Source code in bofire/utils/enum.py class CategoricalMethodEnum ( Enum ): \"\"\"Enumeration class of supported methods how to handle categorical features Currently, exhaustive search and free relaxation are implemented. \"\"\" EXHAUSTIVE = \"EXHAUSTIVE\" FREE = \"FREE\" DescriptorEncodingEnum ( Enum ) Enumeration class how categorical features with descriptors should be encoded Categoricals with descriptors can be handled similar to categoricals, or the descriptors can be used. Source code in bofire/utils/enum.py class DescriptorEncodingEnum ( Enum ): \"\"\"Enumeration class how categorical features with descriptors should be encoded Categoricals with descriptors can be handled similar to categoricals, or the descriptors can be used. \"\"\" DESCRIPTOR = \"DESCRIPTOR\" CATEGORICAL = \"CATEGORICAL\" DescriptorMethodEnum ( Enum ) Enumeration class of implemented methods how to handle discrete descriptors Currently, exhaustive search and free relaxation are implemented. Source code in bofire/utils/enum.py class DescriptorMethodEnum ( Enum ): \"\"\"Enumeration class of implemented methods how to handle discrete descriptors Currently, exhaustive search and free relaxation are implemented. \"\"\" EXHAUSTIVE = \"EXHAUSTIVE\" FREE = \"FREE\" KernelEnum ( Enum ) Enumeration class of all supported kernels Currently, RBF and matern kernel (1/2, 3/2 and 5/2) are implemented. Source code in bofire/utils/enum.py class KernelEnum ( Enum ): \"\"\"Enumeration class of all supported kernels Currently, RBF and matern kernel (1/2, 3/2 and 5/2) are implemented. \"\"\" RBF = \"RBF\" MATERN_25 = \"MATERN_25\" MATERN_15 = \"MATERN_15\" MATERN_05 = \"MATERN_05\" SamplingMethodEnum ( Enum ) An enumeration. Source code in bofire/utils/enum.py class SamplingMethodEnum ( Enum ): UNIFORM = \"UNIFORM\" SOBOL = \"SOBOL\" LHS = \"LHS\" ScalerEnum ( Enum ) Enumeration class of supported scalers Currently, normalization and standardization are implemented. Source code in bofire/utils/enum.py class ScalerEnum ( Enum ): \"\"\"Enumeration class of supported scalers Currently, normalization and standardization are implemented. \"\"\" NORMALIZE = \"NORMALIZE\" STANDARDIZE = \"STANDARDIZE\" reduce AffineTransform Class to switch back and forth from the reduced to the original domain. Source code in bofire/utils/reduce.py class AffineTransform : \"\"\"Class to switch back and forth from the reduced to the original domain.\"\"\" def __init__ ( self , equalities : List [ Tuple [ str , List [ str ], List [ float ]]]): \"\"\"Initializes a `AffineTransformation` object. Args: equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients. \"\"\" self . equalities = equalities def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore the eliminated features in a dataframe Args: data (pd.DataFrame): Dataframe that should be restored. Returns: pd.DataFrame: Restored dataframe \"\"\" if len ( self . equalities ) == 0 : return data data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated features from a dataframe. Args: data (pd.DataFrame): Dataframe with features to be dropped. Returns: pd.DataFrame: Reduced dataframe. \"\"\" if len ( self . equalities ) == 0 : return data drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop ) __init__ ( self , equalities ) special Initializes a AffineTransformation object. Parameters: Name Type Description Default equalities List[Tuple[str,List[str],List[float]]] List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients. required Source code in bofire/utils/reduce.py def __init__ ( self , equalities : List [ Tuple [ str , List [ str ], List [ float ]]]): \"\"\"Initializes a `AffineTransformation` object. Args: equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients. \"\"\" self . equalities = equalities augment_data ( self , data ) Restore the eliminated features in a dataframe Parameters: Name Type Description Default data pd.DataFrame Dataframe that should be restored. required Returns: Type Description pd.DataFrame Restored dataframe Source code in bofire/utils/reduce.py def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore the eliminated features in a dataframe Args: data (pd.DataFrame): Dataframe that should be restored. Returns: pd.DataFrame: Restored dataframe \"\"\" if len ( self . equalities ) == 0 : return data data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data drop_data ( self , data ) Drop eliminated features from a dataframe. Parameters: Name Type Description Default data pd.DataFrame Dataframe with features to be dropped. required Returns: Type Description pd.DataFrame Reduced dataframe. Source code in bofire/utils/reduce.py def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated features from a dataframe. Args: data (pd.DataFrame): Dataframe with features to be dropped. Returns: pd.DataFrame: Reduced dataframe. \"\"\" if len ( self . equalities ) == 0 : return data drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop ) adjust_boundary ( feature , coef , rhs ) Adjusts the boundaries of a feature. Parameters: Name Type Description Default feature ContinuousInputFeature Feature to be adjusted. required coef float Coefficient. required rhs float Right-hand-side of the constraint. required Source code in bofire/utils/reduce.py def adjust_boundary ( feature : ContinuousInput , coef : float , rhs : float ): \"\"\"Adjusts the boundaries of a feature. Args: feature (ContinuousInputFeature): Feature to be adjusted. coef (float): Coefficient. rhs (float): Right-hand-side of the constraint. \"\"\" boundary = rhs / coef if coef > 0 : if boundary > feature . lower_bound : feature . lower_bound = boundary else : if boundary < feature . upper_bound : feature . upper_bound = boundary check_domain_for_reduction ( domain ) Check if the reduction can be applied or if a trivial case is present. Parameters: Name Type Description Default domain Domain Domain to be checked. required Returns: Type Description bool True if reducable, else False. Source code in bofire/utils/reduce.py def check_domain_for_reduction ( domain : Domain ) -> bool : \"\"\"Check if the reduction can be applied or if a trivial case is present. Args: domain (Domain): Domain to be checked. Returns: bool: True if reducable, else False. \"\"\" # are there any constraints? if len ( domain . constraints ) == 0 : return False # are there any linear equality constraints? linear_equalities = domain . cnstrs . get ( LinearEqualityConstraint ) if len ( linear_equalities ) == 0 : return False # are there no NChooseKConstraint constraints? if len ( domain . cnstrs . get ([ NChooseKConstraint ])) > 0 : return False # are there continuous inputs continuous_inputs = domain . get_features ( ContinuousInput ) if len ( continuous_inputs ) == 0 : return False # check that equality constraints only contain continuous inputs for c in linear_equalities : assert isinstance ( c , LinearConstraint ) for feat in c . features : if feat not in domain . get_feature_keys ( ContinuousInput ): return False return True check_existence_of_solution ( A_aug ) Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem. Source code in bofire/utils/reduce.py def check_existence_of_solution ( A_aug ): \"\"\"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.\"\"\" A = A_aug [:, : - 1 ] b = A_aug [:, - 1 ] len_inputs = np . shape ( A )[ 1 ] # catch special cases rk_A_aug = np . linalg . matrix_rank ( A_aug ) rk_A = np . linalg . matrix_rank ( A ) if rk_A == rk_A_aug : if rk_A < len_inputs : return # all good else : x = np . linalg . solve ( A , b ) raise Exception ( f \"There is a unique solution x for the linear equality constraints: x= { x } \" ) elif rk_A < rk_A_aug : raise Exception ( \"There is no solution fulfilling the linear equality constraints.\" ) reduce_domain ( domain ) Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated. Parameters: Name Type Description Default domain Domain Domain to be reduced. required Returns: Type Description Tuple[Domain, AffineTransform] reduced domain and the according transformation to switch between the reduced and orginal domain. Source code in bofire/utils/reduce.py def reduce_domain ( domain : Domain ) -> Tuple [ Domain , AffineTransform ]: \"\"\"Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated. Args: domain (Domain): Domain to be reduced. Returns: Tuple[Domain, AffineTransform]: reduced domain and the according transformation to switch between the reduced and orginal domain. \"\"\" # check if the domain can be reduced if not check_domain_for_reduction ( domain ): return domain , AffineTransform ([]) # find linear equality constraints linear_equalities = domain . cnstrs . get ( LinearEqualityConstraint ) other_constraints = domain . cnstrs . get ( Constraint , excludes = [ LinearEqualityConstraint ] ) # only consider continuous inputs continuous_inputs = [ cast ( ContinuousInput , f ) for f in domain . get_features ( ContinuousInput ) ] other_inputs = domain . inputs . get ( InputFeature , excludes = [ ContinuousInput ]) # assemble Matrix A from equality constraints N = len ( linear_equalities ) M = len ( continuous_inputs ) + 1 names = np . concatenate (([ feat . key for feat in continuous_inputs ], [ \"rhs\" ])) A_aug = pd . DataFrame ( data = np . zeros ( shape = ( N , M )), columns = names ) for i in range ( len ( linear_equalities )): c = linear_equalities [ i ] assert isinstance ( c , LinearEqualityConstraint ) A_aug . loc [ i , c . features ] = c . coefficients # type: ignore A_aug . loc [ i , \"rhs\" ] = c . rhs A_aug = A_aug . values # catch special cases check_existence_of_solution ( A_aug ) # bring A_aug to reduced row-echelon form A_aug_rref , pivots = rref ( A_aug ) pivots = np . array ( pivots ) A_aug_rref = np . array ( A_aug_rref ) . astype ( np . float64 ) # formulate box bounds as linear inequality constraints in matrix form B = np . zeros ( shape = ( 2 * ( M - 1 ), M )) B [: M - 1 , : M - 1 ] = np . eye ( M - 1 ) B [ M - 1 :, : M - 1 ] = - np . eye ( M - 1 ) B [: M - 1 , - 1 ] = np . array ([ feat . upper_bound for feat in continuous_inputs ]) B [ M - 1 :, - 1 ] = - 1.0 * np . array ([ feat . lower_bound for feat in continuous_inputs ]) # eliminate columns with pivot element for i in range ( len ( pivots )): p = pivots [ i ] B [ p , :] -= A_aug_rref [ i , :] B [ p + M - 1 , :] += A_aug_rref [ i , :] # build up reduced domain _domain = Domain . construct ( # _fields_set = {\"input_features\", \"output_features\", \"constraints\"} input_features = deepcopy ( other_inputs ), output_features = deepcopy ( domain . output_features ), constraints = deepcopy ( other_constraints ), ) new_inputs = [ deepcopy ( feat ) for i , feat in enumerate ( continuous_inputs ) if i not in pivots ] all_inputs = _domain . inputs + new_inputs assert isinstance ( all_inputs , InputFeatures ) _domain . input_features = all_inputs constraints : List [ Constraint ] = [] for i in pivots : # reduce equation system of upper bounds ind = np . where ( B [ i , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i , - 1 ] < np . inf : if len ( list ( names [ ind ])) > 1 : c = LinearInequalityConstraint . from_greater_equal ( features = list ( names [ ind ]), coefficients = ( - 1.0 * B [ i , ind ]) . tolist (), rhs = B [ i , - 1 ] * - 1.0 , ) constraints . append ( c ) else : key = names [ ind ][ 0 ] feat = cast ( ContinuousInput , _domain . get_feature ( key )) adjust_boundary ( feat , ( - 1.0 * B [ i , ind ])[ 0 ], B [ i , - 1 ] * - 1.0 ) else : if B [ i , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) # reduce equation system of lower bounds ind = np . where ( B [ i + M - 1 , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i + M - 1 , - 1 ] < np . inf : if len ( list ( names [ ind ])) > 1 : c = LinearInequalityConstraint . from_greater_equal ( features = list ( names [ ind ]), coefficients = ( - 1.0 * B [ i + M - 1 , ind ]) . tolist (), rhs = B [ i + M - 1 , - 1 ] * - 1.0 , ) constraints . append ( c ) else : key = names [ ind ][ 0 ] feat = cast ( ContinuousInput , _domain . get_feature ( key )) adjust_boundary ( feat , ( - 1.0 * B [ i + M - 1 , ind ])[ 0 ], B [ i + M - 1 , - 1 ] * - 1.0 , ) else : if B [ i + M - 1 , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) if len ( constraints ) > 0 : _domain . _set_constraints_unvalidated ( _domain . cnstrs + constraints ) # assemble equalities _equalities = [] for i in range ( len ( pivots )): name_lhs = names [ pivots [ i ]] names_rhs = [] coeffs = [] for j in range ( len ( names ) - 1 ): if A_aug_rref [ i , j ] != 0 and j != pivots [ i ]: coeffs . append ( - A_aug_rref [ i , j ]) names_rhs . append ( names [ j ]) coeffs . append ( A_aug_rref [ i , - 1 ]) _equalities . append (( name_lhs , names_rhs , coeffs )) trafo = AffineTransform ( _equalities ) # remove remaining dependencies of eliminated inputs from the problem _domain = remove_eliminated_inputs ( _domain , trafo ) return _domain , trafo remove_eliminated_inputs ( domain , transform ) Eliminates remaining occurences of eliminated inputs in linear constraints. Parameters: Name Type Description Default domain Domain Domain in which the linear constraints should be purged. required transform AffineTransform Affine transformation object that defines the obsolete features. required Exceptions: Type Description ValueError If feature occurs in a constraint different from a linear one. Returns: Type Description Domain Purged domain. Source code in bofire/utils/reduce.py def remove_eliminated_inputs ( domain : Domain , transform : AffineTransform ) -> Domain : \"\"\"Eliminates remaining occurences of eliminated inputs in linear constraints. Args: domain (Domain): Domain in which the linear constraints should be purged. transform (AffineTransform): Affine transformation object that defines the obsolete features. Raises: ValueError: If feature occurs in a constraint different from a linear one. Returns: Domain: Purged domain. \"\"\" inputs_names = domain . get_feature_keys () M = len ( inputs_names ) # write the equalities for the backtransformation into one matrix inputs_dict = { inputs_names [ i ]: i for i in range ( M )} # build up dict from domain.equalities e.g. {\"xi1\": [coeff(xj1), ..., coeff(xjn)], ... \"xik\":...} coeffs_dict = {} for i , e in enumerate ( transform . equalities ): coeffs = np . zeros ( M + 1 ) for j , name in enumerate ( e [ 1 ]): coeffs [ inputs_dict [ name ]] = e [ 2 ][ j ] coeffs [ - 1 ] = e [ 2 ][ - 1 ] coeffs_dict [ e [ 0 ]] = coeffs constraints = [] for c in domain . cnstrs . get (): # Nonlinear constraints not supported if not isinstance ( c , LinearConstraint ): raise ValueError ( \"Elimination of variables is only supported for LinearEquality and LinearInequality constraints.\" ) # no changes, if the constraint does not contain eliminated inputs elif all ( name in inputs_names for name in c . features ): constraints . append ( c ) # remove inputs from the constraint that were eliminated from the inputs before else : totally_removed = False _features = np . array ( inputs_names ) _rhs = c . rhs # create new lhs and rhs from the old one and knowledge from problem._equalities _coefficients = np . zeros ( M ) for j , name in enumerate ( c . features ): if name in inputs_names : _coefficients [ inputs_dict [ name ]] += c . coefficients [ j ] else : _coefficients += c . coefficients [ j ] * coeffs_dict [ name ][: - 1 ] _rhs -= c . coefficients [ j ] * coeffs_dict [ name ][ - 1 ] _features = _features [ np . abs ( _coefficients ) > 1e-16 ] _coefficients = _coefficients [ np . abs ( _coefficients ) > 1e-16 ] _c = None if isinstance ( c , LinearEqualityConstraint ): if len ( _features ) > 1 : _c = LinearEqualityConstraint ( features = _features . tolist (), coefficients = _coefficients . tolist (), rhs = _rhs , ) elif len ( _features ) == 0 : totally_removed = True else : feat = domain . get_feature ( _features [ 0 ]) feat . lower_bound = _coefficients [ 0 ] feat . upper_bound = _coefficients [ 0 ] totally_removed = True else : if len ( _features ) > 1 : _c = LinearInequalityConstraint ( features = _features . tolist (), coefficients = _coefficients . tolist (), rhs = _rhs , ) elif len ( _features ) == 0 : totally_removed = True else : feat = cast ( ContinuousInput , domain . get_feature ( _features [ 0 ])) adjust_boundary ( feat , _coefficients [ 0 ], _rhs ) totally_removed = True # check if constraint is always fulfilled/not fulfilled if not totally_removed : assert _c is not None if len ( _c . features ) == 0 and _c . rhs >= 0 : pass elif len ( _c . features ) == 0 and _c . rhs < 0 : raise Exception ( \"Linear constraints cannot be fulfilled.\" ) elif np . isinf ( _c . rhs ): pass else : constraints . append ( _c ) domain . constraints = Constraints ( constraints = constraints ) return domain rref ( A , tol = 1e-08 ) Computes the reduced row echelon form of a Matrix Parameters: Name Type Description Default A ndarray 2d array representing a matrix. required tol float tolerance for rounding to 0. Defaults to 1e-8. 1e-08 Returns: Type Description Tuple[numpy.ndarray, List[int]] (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref Source code in bofire/utils/reduce.py def rref ( A : np . ndarray , tol : float = 1e-8 ) -> Tuple [ np . ndarray , List [ int ]]: \"\"\"Computes the reduced row echelon form of a Matrix Args: A (ndarray): 2d array representing a matrix. tol (float, optional): tolerance for rounding to 0. Defaults to 1e-8. Returns: (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref \"\"\" A = np . array ( A , dtype = np . float64 ) n , m = np . shape ( A ) col = 0 row = 0 pivots = [] for col in range ( m ): # does a pivot element exist? if all ( np . abs ( A [ row :, col ]) < tol ): pass # if yes: start elimination else : pivots . append ( col ) max_row = np . argmax ( np . abs ( A [ row :, col ])) + row # switch to most stable row A [[ row , max_row ], :] = A [[ max_row , row ], :] # type: ignore # normalize row A [ row , :] /= A [ row , col ] # eliminate other elements from column for r in range ( n ): if r != row : A [ r , :] -= A [ r , col ] / A [ row , col ] * A [ row , :] row += 1 prec = int ( - np . log10 ( tol )) return np . round ( A , prec ), pivots study MetricsEnum ( Enum ) An enumeration. Source code in bofire/utils/study.py class MetricsEnum ( Enum ): HYPERVOLUME = \"HYPERVOLUME\" STRATEGY = \"STRATEGY\" PoolStudy ( BaseModel ) pydantic-model Source code in bofire/utils/study.py class PoolStudy ( BaseModel ): class Config : arbitrary_types_allowed = True domain : Domain num_starting_experiments : int experiments : pd . DataFrame meta : Optional [ pd . DataFrame ] strategy : Optional [ Strategy ] starting_point_generator : Optional [ Callable ] metrics : Optional [ MetricsEnum ] = None ref_point : Optional [ dict ] @validator ( \"domain\" ) def validate_feature_count ( cls , domain : Domain ): if len ( domain . input_features ) == 0 : raise ValueError ( \"no input feature specified\" ) if len ( domain . output_features ) == 0 : raise ValueError ( \"no output feature specified\" ) return domain @validator ( \"experiments\" ) def validate_experiments ( cls , experiments , values ): experiments = values [ \"domain\" ] . validate_experiments ( experiments ) # # we pick only those experiments where at least one output is valid cleaned = ( values [ \"domain\" ] . preprocess_experiments_any_valid_output ( experiments ) . copy () . reset_index ( drop = True ) ) if len ( experiments ) < 2 : raise ValueError ( \"too less experiments available for PoolStudy.\" ) return cleaned @validator ( \"metrics\" ) def validate_metrics ( cls , metrics , values ): if ( metrics is MetricsEnum . HYPERVOLUME and len ( values [ \"domain\" ] . output_features . get_by_objective ( excludes = None )) < 2 ): raise ValueError ( \"For metrics HYPERVOLUME at least two output features has to be defined.\" ) return metrics @validator ( \"ref_point\" ) def validate_ref_point ( cls , ref_point , values ): if ref_point is None : return None if len ( ref_point ) != len ( values [ \"domain\" ] . output_features . get_by_objective ( excludes = None ) ): raise ValueError ( \"Length of refpoint does not match number of output features.\" ) for feat in values [ \"domain\" ] . output_features . get_keys_by_objective ( excludes = None ): assert feat in ref_point . keys () return ref_point @staticmethod def generate_uniform ( experiments : pd . DataFrame , num_starting_experiments ): assert num_starting_experiments > 0 return np . random . choice ( np . arange ( experiments . shape [ 0 ]), size = num_starting_experiments , replace = False , ) def __init__ ( self , ** data ): super () . __init__ ( ** data ) self . meta = pd . DataFrame ( index = range ( self . experiments . shape [ 0 ]), columns = [ \"iteration\" ], data = np . nan ) if self . starting_point_generator is None : start_idx = self . generate_uniform ( self . experiments , self . num_starting_experiments ) else : start_idx = self . starting_point_generator ( self . experiments , self . num_starting_experiments ) # if ( self . metrics == MetricsEnum . HYPERVOLUME ) and ( self . ref_point is None ): self . ref_point = infer_ref_point ( self . domain , self . experiments , return_masked = False ) self . meta . loc [ start_idx , \"iteration\" ] = 0 @property def picked_experiments ( self ): return self . experiments . loc [ self . meta . iteration . notna ()] # type: ignore @property def num_picked_experiments ( self ): return self . picked_experiments . shape [ 0 ] @property def open_experiments ( self ): return self . experiments . loc [ self . meta . iteration . isna ()] # type: ignore @property def num_open_experiments ( self ): return self . open_experiments . shape [ 0 ] @property def num_iterations ( self ): return int ( self . meta . iteration . max ()) # type: ignore def get_fbest ( self , experiments : pd . DataFrame ): if self . metrics is None : return np . nan elif self . metrics == MetricsEnum . HYPERVOLUME : opt_exps = get_pareto_front ( self . domain , experiments ) return compute_hypervolume ( domain = self . domain , optimal_experiments = opt_exps , ref_point = self . ref_point , # type: ignore ) elif self . metrics == MetricsEnum . STRATEGY : return self . strategy . get_fbest ( experiments ) # type: ignore @property def expected_random ( self ): \"\"\"Expected value for number of random picks until finding the best one. According to https://arxiv.org/abs/1404.1161 it is defined as E(X) = (N+1)/(K+1) where N is the number of possible solutions and K the number of good solutions. \"\"\" K = ( get_pareto_front ( domain = self . domain , experiments = self . experiments ) . shape [ 0 ] if self . metrics == MetricsEnum . HYPERVOLUME else 1 ) return ( self . experiments . shape [ 0 ] + 1 ) / ( K + 1 ) def optimize ( self , strategy : Strategy , num_iterations : int = 100 , candidate_count : int = 1 , progress_bar : bool = True , early_stopping = False , ): self . strategy = strategy ( self . domain ) # type: ignore if candidate_count > 1 : raise ValueError ( \"batch_size > 1 not yet implemented.\" ) if num_iterations < 1 : raise ValueError ( \"At least one iteration has to be performed!\" ) if num_iterations > self . num_open_experiments : num_iterations = self . num_open_experiments fbest = self . get_fbest ( self . experiments ) with tqdm ( range ( num_iterations ), disable = True if progress_bar is False else False , postfix = { \"dist2best\" : \"?\" }, ) as pbar : for i in pbar : strategy . tell ( self . picked_experiments ) acqf_values = strategy . _choose_from_pool ( self . open_experiments ) picked_idx = self . open_experiments . iloc [ acqf_values . argmax ()] . name # type: ignore self . meta . loc [ picked_idx , \"iteration\" ] = i + 1 # type: ignore cfbest = self . get_fbest ( self . picked_experiments ) pbar . set_postfix ({ \"dist2best\" : fbest - cfbest }) # type: ignore if np . allclose ( fbest , cfbest ) and early_stopping : # type: ignore break expected_random property readonly Expected value for number of random picks until finding the best one. According to https://arxiv.org/abs/1404.1161 it is defined as E(X) = (N+1)/(K+1) where N is the number of possible solutions and K the number of good solutions. torch_tools get_linear_constraints ( domain , constraint , unit_scaled = False ) Converts linear constraints to the form required by BoTorch. Parameters: Name Type Description Default domain Domain Optimization problem definition. required constraint Union[LinearEqualityConstraint, LinearInequalityConstraint] Type of constraint that should be converted. required unit_scaled bool If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False. False Returns: Type Description List[Tuple[Tensor, Tensor, float]] List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs. Source code in bofire/utils/torch_tools.py def get_linear_constraints ( domain : Domain , constraint : Union [ LinearEqualityConstraint , LinearInequalityConstraint ], unit_scaled : bool = False , ) -> List [ Tuple [ Tensor , Tensor , float ]]: \"\"\"Converts linear constraints to the form required by BoTorch. Args: domain (Domain): Optimization problem definition. constraint (Union[LinearEqualityConstraint, LinearInequalityConstraint]): Type of constraint that should be converted. unit_scaled (bool, optional): If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False. Returns: List[Tuple[Tensor, Tensor, float]]: List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs. \"\"\" constraints = [] for c in domain . cnstrs . get ( constraint ): indices = [] coefficients = [] lower = [] upper = [] rhs = 0.0 for i , featkey in enumerate ( c . features ): # type: ignore idx = domain . get_feature_keys ( InputFeature ) . index ( featkey ) feat = domain . get_feature ( featkey ) if feat . is_fixed (): # type: ignore rhs -= feat . fixed_value () * c . coefficients [ i ] # type: ignore else : lower . append ( feat . lower_bound ) # type: ignore upper . append ( feat . upper_bound ) # type: ignore indices . append ( idx ) coefficients . append ( c . coefficients [ i ] # type: ignore ) # if unit_scaled == False else c_scaled.coefficients[i]) if unit_scaled : lower = np . array ( lower ) upper = np . array ( upper ) s = upper - lower scaled_coefficients = s * np . array ( coefficients ) constraints . append ( ( torch . tensor ( indices ), - torch . tensor ( scaled_coefficients ) . to ( ** tkwargs ), - ( rhs + c . rhs - np . sum ( np . array ( coefficients ) * lower )), # type: ignore ) ) else : constraints . append ( ( torch . tensor ( indices ), - torch . tensor ( coefficients ) . to ( ** tkwargs ), - ( rhs + c . rhs ), # type: ignore ) ) return constraints transformer Transformer ( BaseModel ) pydantic-model Source code in bofire/utils/transformer.py class Transformer ( BaseModel ): domain : Domain descriptor_encoding : Optional [ Union [ DescriptorEncodingEnum , None ]] categorical_encoding : Optional [ Union [ CategoricalEncodingEnum , None ]] scale_inputs : Optional [ ScalerEnum ] = None scale_outputs : Optional [ ScalerEnum ] = None is_fitted : bool = False features2transformedFeatures : Dict = Field ( default_factory = lambda : {}) encoders : Dict = Field ( default_factory = lambda : {}) \"\"\"Pre/post-processing of data for strategies Parameters: is_fitted: bool features2transformedFeatures: Dict encoders: Dict \"\"\" def __init__ ( self , domain , descriptor_encoding = None , # TODO: default tbd! categorical_encoding = None , # TODO: default tbd! scale_inputs = None , scale_outputs = None , ) -> None : \"\"\"Pre/post-processing of data for strategies Args: domain (Domain): A domain for that is being used in the strategy descriptor_encoding (DescriptorEncodingEnum, optional): Transform the descriptors into continuous features (\"DESCRIPTOR\")/ numerical representation of categoricals (\"CATEGORICAL\"). Defaults to None. categorical_encoding (CategoricalEncodingEnum, optional): Distinction between one hot and ordinal encoding for categoricals. Defaults to None. scale_inputs/ scale_outputs (ScalerEnum, optional): In-/Outputs can be standardized, normalized or not scaled. Defaults to None. \"\"\" super () . __init__ ( domain = domain , descriptor_encoding = descriptor_encoding , # ( # descriptor_encoding.name # if isinstance(descriptor_encoding, Enum) # else None # ), categorical_encoding = categorical_encoding , # ( # categorical_encoding.name # if isinstance(categorical_encoding, Enum) # else None # ), scale_inputs = scale_inputs , scale_outputs = scale_outputs , ) for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . descriptors ] self . features2transformedFeatures [ feature . key ] = keys elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . categories ] self . features2transformedFeatures [ feature . key ] = keys return def fit ( self , input : pd . DataFrame ): \"\"\"Fit the encoders to provided input data Args: input (pd.DataFrame): The data, the encoders are fitted to. Raises: NotImplementedError: User-provided descriptor values are currently ignored. The descriptor values are set in the CategoricalDescriptorInputFeature. DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformer object: The fitted transformer \"\"\" experiment = input . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): if all ( np . isin ( feature . descriptors , experiment . columns )): # how should we deal with descriptors entered by the user, which are not in line with the values assigned in the features`? # TODO: Check if user-provided decriptors are valid # TODO: implement hierarchy which descriptors are preferred raise NotImplementedError ( \"User-provided descriptor values are currently ignored\" ) enc = CategoricalDescriptorEncoder ( categories = [ feature . categories ], descriptors = [ feature . descriptors ], values = [ feature . values ], ) values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) column_names = enc . get_feature_names_out () self . encoders [ feature . key ] = enc values = self . encoders [ feature . key ] . transform ( values ) for loc , column_name in enumerate ( column_names ): experiment [ column_name ] = values [:, loc ] var_min = min ([ val [ loc ] for val in feature . values ]) var_max = max ([ val [ loc ] for val in feature . values ]) # Scale inputs self . fit_scaling ( column_name , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) experiment = experiment . drop ( column_name , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): enc = OrdinalEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Create one-hot encoding columns & insert to DataSet # TODO: drop in oneHot encoder testen enc = OneHotEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif isinstance ( feature , ContinuousInput ): var_min , var_max = feature . lower_bound , feature . upper_bound self . fit_scaling ( feature . key , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) self . fit_scaling ( feature . key , experiment , scaler_type = self . scale_outputs ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) self . is_fitted = True return self def transform ( self , experiment : pd . DataFrame ): \"\"\"Transform data inputs and outputs for a strategy given already fitted encoders Args: experiment (pd.DataFrame): Input data to be transformed Raises: DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformed_experiment (pd.DataFrame): the transformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" transformed_experiment = experiment . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) values = self . encoders [ feature . key ] . transform ( values ) column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , values [:, i ]) # Scale inputs if self . encoders [ column_name ] is not None : values_unscaled = np . atleast_2d ( transformed_experiment [ column_name ] . to_numpy () ) . T transformed_experiment [ column_name ] = self . encoders [ column_name ] . transform ( values_unscaled ) # Ensure descriptor features are floats transformed_experiment [ column_names ] = transformed_experiment [ column_names ] . astype ( float ) # drop categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) transformed_experiment [ feature . key ] = enc_values . astype ( \"int32\" ) # categorical kernel needs int as input to avoid numerical trouble. Thus, these columns are also not scaled elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) . toarray () column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , enc_values [:, i ] ) # Drop old categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif isinstance ( feature , ContinuousInput ): if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) return transformed_experiment . copy () def fit_transform ( self , experiment : pd . DataFrame ): \"\"\"A combination of self.fit and self.transform Args: experiment (pd.DataFrame): [description] Returns: transformed_experiment (pd.DataFrame): the transfered input data \"\"\" self . fit ( experiment ) transformed_experiment = self . transform ( experiment ) return transformed_experiment def inverse_transform ( self , transformed_candidate : pd . DataFrame ): \"\"\"backtransformation of data using the fitted encoders Args: transformed_candidate (pd.DataFrame): input data to be backtransformed Raises: DomainError: Feature is defined in the domain, but no feature data is provided in the dataset to be backtransformed. NotImplementedError: Acctually, categorical outputs are not supported. Returns: candidate (pd.DataFrame): backtransformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" # Determine input and output columns in dataset candidate = transformed_candidate . copy () for feature in self . get_features_to_be_transformed (): # Categorical variables with descriptors if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): enc = self . encoders [ feature . key ] column_names = enc . get_feature_names_out () for column_name in column_names : candidate = self . un_scale ( column_name , candidate ) values = candidate [ column_names ] . to_numpy () enc_values = enc . inverse_transform ( values ) index = candidate . columns . get_loc ( column_names [ 0 ]) candidate . insert ( index , feature . key , enc_values ) # Delete the descriptor columns candidate = candidate . drop ( columns = column_names , axis = 1 ) # Categorical features using one-hot encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed column_names = enc . get_feature_names_out () values = candidate [ column_names ] . to_numpy () # Do inverse transform index = candidate . columns . get_loc ( column_names [ 0 ]) enc_values = enc . inverse_transform ( values ) candidate . insert ( index , feature . key , enc_values ) # Add to dataset and drop one-hot encoding candidate = candidate . drop ( column_names , axis = 1 ) # Categorical features using ordinal encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed values = np . atleast_2d ( candidate [ feature . key ] . to_numpy ()) . T # Do inverse transform candidate [ feature . key ] = enc . inverse_transform ( values ) # Continuous features elif isinstance ( feature , ContinuousInput ): candidate = self . un_scale ( feature . key , candidate ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): pass else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) for feature in self . domain . get_features ( OutputFeature ): if feature . key in transformed_candidate . columns : if isinstance ( feature , ContinuousOutput ): candidate = self . un_scale ( feature . key , candidate ) else : raise NotImplementedError ( \"Acctually, only continuous outputs are implemented\" ) return candidate def get_features_to_be_transformed ( self ): excludes = [] if self . categorical_encoding is None : excludes . append ( CategoricalInput ) features_cat_desc = self . domain . get_features ( CategoricalDescriptorInput ) else : features_cat_desc = [] if self . descriptor_encoding is None : excludes . append ( CategoricalDescriptorInput ) features_cat_desc = [] return ( self . domain . get_features ( InputFeature , excludes = excludes ) + features_cat_desc ) def fit_scaling ( self , key : str , experiment : pd . DataFrame , var_min : float = np . nan , var_max : float = np . nan , scaler_type : Optional [ ScalerEnum ] = None , ) -> \"Transformer\" : # TODO: switch to Self some time in the future which is available in Python 3.11 \"\"\"fitting the chosen scaler type to provided input data Args: key (str): column name in input data of the feature to be scaled experiment (pd.DataFrame): input data to be fitted to var_min (float, optional): Lower bound to be used for min max scaling. When not defined or nan, the minimum of the input data is used. Defaults to np.nan. var_max (float, optional): Upper bound to be used for min max scaling. When not defined or nan, the maximum of the input data is used. Defaults to np.nan. scaler_type (ScalerEnum, optional): Defines the type of scaling (Normalize/ standardize/ None). Defaults to None. Returns: transformer object: fitted encoders are added to self.encoders \"\"\" values = np . atleast_2d ( experiment [ key ] . to_numpy ()) . T if scaler_type == ScalerEnum . STANDARDIZE : enc = StandardScaler () enc . fit ( values ) elif scaler_type == ScalerEnum . NORMALIZE : enc = MinMaxScaler () if np . isnan ( var_min ): var_min = min ( values ) if np . isnan ( var_max ): var_max = max ( values ) enc . fit ( np . array ([ var_min , var_max ]) . reshape ( - 1 , 1 )) else : enc = None self . encoders [ key ] = enc return self def un_scale ( self , key : str , candidate : pd . DataFrame ) -> pd . DataFrame : \"\"\"uses the fitted encoders to back-scale the input data to original scale Args: key (str): column name in input data of the feature to be back-scaled candidate (pd.DataFrame): input data to be un-scaled Returns: candidate (pd.DataFrame): The un-scaled input data \"\"\" if ( key in self . encoders . keys ()) and ( self . encoders [ key ] is not None ): values = np . atleast_2d ( candidate [ key ] . to_numpy ()) . T candidate [ key ] = self . encoders [ key ] . inverse_transform ( values ) candidate [ key ] = candidate [ key ] . astype ( float ) return candidate __init__ ( self , domain , descriptor_encoding = None , categorical_encoding = None , scale_inputs = None , scale_outputs = None ) special Pre/post-processing of data for strategies Parameters: Name Type Description Default domain Domain A domain for that is being used in the strategy required descriptor_encoding DescriptorEncodingEnum Transform the descriptors into continuous features (\"DESCRIPTOR\")/ numerical representation of categoricals (\"CATEGORICAL\"). Defaults to None. None categorical_encoding CategoricalEncodingEnum Distinction between one hot and ordinal encoding for categoricals. Defaults to None. None scale_inputs/ scale_outputs (ScalerEnum In-/Outputs can be standardized, normalized or not scaled. Defaults to None. required Source code in bofire/utils/transformer.py def __init__ ( self , domain , descriptor_encoding = None , # TODO: default tbd! categorical_encoding = None , # TODO: default tbd! scale_inputs = None , scale_outputs = None , ) -> None : \"\"\"Pre/post-processing of data for strategies Args: domain (Domain): A domain for that is being used in the strategy descriptor_encoding (DescriptorEncodingEnum, optional): Transform the descriptors into continuous features (\"DESCRIPTOR\")/ numerical representation of categoricals (\"CATEGORICAL\"). Defaults to None. categorical_encoding (CategoricalEncodingEnum, optional): Distinction between one hot and ordinal encoding for categoricals. Defaults to None. scale_inputs/ scale_outputs (ScalerEnum, optional): In-/Outputs can be standardized, normalized or not scaled. Defaults to None. \"\"\" super () . __init__ ( domain = domain , descriptor_encoding = descriptor_encoding , # ( # descriptor_encoding.name # if isinstance(descriptor_encoding, Enum) # else None # ), categorical_encoding = categorical_encoding , # ( # categorical_encoding.name # if isinstance(categorical_encoding, Enum) # else None # ), scale_inputs = scale_inputs , scale_outputs = scale_outputs , ) for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . descriptors ] self . features2transformedFeatures [ feature . key ] = keys elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . categories ] self . features2transformedFeatures [ feature . key ] = keys return fit ( self , input ) Fit the encoders to provided input data Parameters: Name Type Description Default input pd.DataFrame The data, the encoders are fitted to. required Exceptions: Type Description DomainError Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError Output features cannot be categorical features currently. DomainError Unknown output feature type Returns: Type Description transformer object The fitted transformer Source code in bofire/utils/transformer.py def fit ( self , input : pd . DataFrame ): \"\"\"Fit the encoders to provided input data Args: input (pd.DataFrame): The data, the encoders are fitted to. Raises: NotImplementedError: User-provided descriptor values are currently ignored. The descriptor values are set in the CategoricalDescriptorInputFeature. DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformer object: The fitted transformer \"\"\" experiment = input . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): if all ( np . isin ( feature . descriptors , experiment . columns )): # how should we deal with descriptors entered by the user, which are not in line with the values assigned in the features`? # TODO: Check if user-provided decriptors are valid # TODO: implement hierarchy which descriptors are preferred raise NotImplementedError ( \"User-provided descriptor values are currently ignored\" ) enc = CategoricalDescriptorEncoder ( categories = [ feature . categories ], descriptors = [ feature . descriptors ], values = [ feature . values ], ) values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) column_names = enc . get_feature_names_out () self . encoders [ feature . key ] = enc values = self . encoders [ feature . key ] . transform ( values ) for loc , column_name in enumerate ( column_names ): experiment [ column_name ] = values [:, loc ] var_min = min ([ val [ loc ] for val in feature . values ]) var_max = max ([ val [ loc ] for val in feature . values ]) # Scale inputs self . fit_scaling ( column_name , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) experiment = experiment . drop ( column_name , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): enc = OrdinalEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Create one-hot encoding columns & insert to DataSet # TODO: drop in oneHot encoder testen enc = OneHotEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif isinstance ( feature , ContinuousInput ): var_min , var_max = feature . lower_bound , feature . upper_bound self . fit_scaling ( feature . key , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) self . fit_scaling ( feature . key , experiment , scaler_type = self . scale_outputs ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) self . is_fitted = True return self fit_scaling ( self , key , experiment , var_min = nan , var_max = nan , scaler_type = None ) fitting the chosen scaler type to provided input data Parameters: Name Type Description Default key str column name in input data of the feature to be scaled required experiment pd.DataFrame input data to be fitted to required var_min float Lower bound to be used for min max scaling. When not defined or nan, the minimum of the input data is used. Defaults to np.nan. nan var_max float Upper bound to be used for min max scaling. When not defined or nan, the maximum of the input data is used. Defaults to np.nan. nan scaler_type ScalerEnum Defines the type of scaling (Normalize/ standardize/ None). Defaults to None. None Returns: Type Description transformer object fitted encoders are added to self.encoders Source code in bofire/utils/transformer.py def fit_scaling ( self , key : str , experiment : pd . DataFrame , var_min : float = np . nan , var_max : float = np . nan , scaler_type : Optional [ ScalerEnum ] = None , ) -> \"Transformer\" : # TODO: switch to Self some time in the future which is available in Python 3.11 \"\"\"fitting the chosen scaler type to provided input data Args: key (str): column name in input data of the feature to be scaled experiment (pd.DataFrame): input data to be fitted to var_min (float, optional): Lower bound to be used for min max scaling. When not defined or nan, the minimum of the input data is used. Defaults to np.nan. var_max (float, optional): Upper bound to be used for min max scaling. When not defined or nan, the maximum of the input data is used. Defaults to np.nan. scaler_type (ScalerEnum, optional): Defines the type of scaling (Normalize/ standardize/ None). Defaults to None. Returns: transformer object: fitted encoders are added to self.encoders \"\"\" values = np . atleast_2d ( experiment [ key ] . to_numpy ()) . T if scaler_type == ScalerEnum . STANDARDIZE : enc = StandardScaler () enc . fit ( values ) elif scaler_type == ScalerEnum . NORMALIZE : enc = MinMaxScaler () if np . isnan ( var_min ): var_min = min ( values ) if np . isnan ( var_max ): var_max = max ( values ) enc . fit ( np . array ([ var_min , var_max ]) . reshape ( - 1 , 1 )) else : enc = None self . encoders [ key ] = enc return self fit_transform ( self , experiment ) A combination of self.fit and self.transform Parameters: Name Type Description Default experiment pd.DataFrame [description] required Returns: Type Description transformed_experiment (pd.DataFrame) the transfered input data Source code in bofire/utils/transformer.py def fit_transform ( self , experiment : pd . DataFrame ): \"\"\"A combination of self.fit and self.transform Args: experiment (pd.DataFrame): [description] Returns: transformed_experiment (pd.DataFrame): the transfered input data \"\"\" self . fit ( experiment ) transformed_experiment = self . transform ( experiment ) return transformed_experiment inverse_transform ( self , transformed_candidate ) backtransformation of data using the fitted encoders Parameters: Name Type Description Default transformed_candidate pd.DataFrame input data to be backtransformed required Returns: Type Description candidate (pd.DataFrame) backtransformed input data Source code in bofire/utils/transformer.py def inverse_transform ( self , transformed_candidate : pd . DataFrame ): \"\"\"backtransformation of data using the fitted encoders Args: transformed_candidate (pd.DataFrame): input data to be backtransformed Raises: DomainError: Feature is defined in the domain, but no feature data is provided in the dataset to be backtransformed. NotImplementedError: Acctually, categorical outputs are not supported. Returns: candidate (pd.DataFrame): backtransformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" # Determine input and output columns in dataset candidate = transformed_candidate . copy () for feature in self . get_features_to_be_transformed (): # Categorical variables with descriptors if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): enc = self . encoders [ feature . key ] column_names = enc . get_feature_names_out () for column_name in column_names : candidate = self . un_scale ( column_name , candidate ) values = candidate [ column_names ] . to_numpy () enc_values = enc . inverse_transform ( values ) index = candidate . columns . get_loc ( column_names [ 0 ]) candidate . insert ( index , feature . key , enc_values ) # Delete the descriptor columns candidate = candidate . drop ( columns = column_names , axis = 1 ) # Categorical features using one-hot encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed column_names = enc . get_feature_names_out () values = candidate [ column_names ] . to_numpy () # Do inverse transform index = candidate . columns . get_loc ( column_names [ 0 ]) enc_values = enc . inverse_transform ( values ) candidate . insert ( index , feature . key , enc_values ) # Add to dataset and drop one-hot encoding candidate = candidate . drop ( column_names , axis = 1 ) # Categorical features using ordinal encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed values = np . atleast_2d ( candidate [ feature . key ] . to_numpy ()) . T # Do inverse transform candidate [ feature . key ] = enc . inverse_transform ( values ) # Continuous features elif isinstance ( feature , ContinuousInput ): candidate = self . un_scale ( feature . key , candidate ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): pass else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) for feature in self . domain . get_features ( OutputFeature ): if feature . key in transformed_candidate . columns : if isinstance ( feature , ContinuousOutput ): candidate = self . un_scale ( feature . key , candidate ) else : raise NotImplementedError ( \"Acctually, only continuous outputs are implemented\" ) return candidate transform ( self , experiment ) Transform data inputs and outputs for a strategy given already fitted encoders Parameters: Name Type Description Default experiment pd.DataFrame Input data to be transformed required Returns: Type Description transformed_experiment (pd.DataFrame) the transformed input data Source code in bofire/utils/transformer.py def transform ( self , experiment : pd . DataFrame ): \"\"\"Transform data inputs and outputs for a strategy given already fitted encoders Args: experiment (pd.DataFrame): Input data to be transformed Raises: DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformed_experiment (pd.DataFrame): the transformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" transformed_experiment = experiment . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) values = self . encoders [ feature . key ] . transform ( values ) column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , values [:, i ]) # Scale inputs if self . encoders [ column_name ] is not None : values_unscaled = np . atleast_2d ( transformed_experiment [ column_name ] . to_numpy () ) . T transformed_experiment [ column_name ] = self . encoders [ column_name ] . transform ( values_unscaled ) # Ensure descriptor features are floats transformed_experiment [ column_names ] = transformed_experiment [ column_names ] . astype ( float ) # drop categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) transformed_experiment [ feature . key ] = enc_values . astype ( \"int32\" ) # categorical kernel needs int as input to avoid numerical trouble. Thus, these columns are also not scaled elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) . toarray () column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , enc_values [:, i ] ) # Drop old categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif isinstance ( feature , ContinuousInput ): if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) return transformed_experiment . copy () un_scale ( self , key , candidate ) uses the fitted encoders to back-scale the input data to original scale Parameters: Name Type Description Default key str column name in input data of the feature to be back-scaled required candidate pd.DataFrame input data to be un-scaled required Returns: Type Description candidate (pd.DataFrame) The un-scaled input data Source code in bofire/utils/transformer.py def un_scale ( self , key : str , candidate : pd . DataFrame ) -> pd . DataFrame : \"\"\"uses the fitted encoders to back-scale the input data to original scale Args: key (str): column name in input data of the feature to be back-scaled candidate (pd.DataFrame): input data to be un-scaled Returns: candidate (pd.DataFrame): The un-scaled input data \"\"\" if ( key in self . encoders . keys ()) and ( self . encoders [ key ] is not None ): values = np . atleast_2d ( candidate [ key ] . to_numpy ()) . T candidate [ key ] = self . encoders [ key ] . inverse_transform ( values ) candidate [ key ] = candidate [ key ] . astype ( float ) return candidate","title":"Utils"},{"location":"ref-utils/#utils","text":"","title":"Utils"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder","text":"","title":"categoricalDescriptorEncoder"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder.CategoricalDescriptorEncoder","text":"Encoder to translate categorical parameters into continuous descriptor values. Source code in bofire/utils/categoricalDescriptorEncoder.py class CategoricalDescriptorEncoder ( _BaseEncoder ): \"\"\" Encoder to translate categorical parameters into continuous descriptor values. \"\"\" def __init__ ( self , * , categories : Union [ str , List [ List [ str ]]] = \"auto\" , descriptors : Union [ str , List [ List [ str ]]] = \"auto\" , values : List [ List [ List [ float ]]], sparse : bool = False , dtype = np . float64 , handle_unknown = \"error\" , ): \"\"\"Encoder to translate categorical parameters into continuous descriptor values. Args: values (List[List[List[float]]]): Nested list of descriptor values. Must be of shape (n_features, n_categories_per_feature, n_descriptors). categories (List[List[str]], optional): List of strings referring to the categories (not the feature names!) occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) descriptors (List[List[str]], optional): List of strings referring to the descriptor names occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) sparse (bool, optional): Sparse matrix output is currently not supported. Defaults to False. dtype (dtype, optional): [description]. Defaults to np.float64. handle_unknown (str, optional): Allows to distinguish between \"error\" and \"ignore\". When \"ignore\", onknown categories are encoded as zeros. Defaults to \"error\". \"\"\" self . categories = categories self . descriptors = descriptors self . values = values # check, whether we have only 1D data or multiple descriptors provided at once if not isinstance ( self . values [ 0 ][ 0 ], list ): self . values = [ values ] if not isinstance ( self . categories [ 0 ], list ) and self . categories != \"auto\" : self . categories = [ categories ] if not isinstance ( self . descriptors [ 0 ], list ) and self . descriptors != \"auto\" : self . descriptors = [ descriptors ] self . sparse = sparse self . dtype = dtype self . handle_unknown = handle_unknown def _validate_keywords ( self ): \"\"\"Validate `handle_unknown` argument. Raises: ValueError: if `handle_unknown`not `error` or `ignore`. \"\"\" if self . handle_unknown not in ( \"error\" , \"ignore\" ): msg = ( \"handle_unknown should be either 'error' or 'ignore', got {0} .\" . format ( self . handle_unknown ) ) raise ValueError ( msg ) def _fit ( self , X , handle_unknown = \"error\" , force_all_finite : Union [ str , bool ] = True ): self . _check_n_features ( X , reset = True ) self . _check_feature_names ( X , reset = True ) X_list , n_samples , n_features = self . _check_X ( X , force_all_finite = force_all_finite # type: ignore ) self . n_features_in_ = n_features if self . categories != \"auto\" : if len ( self . categories ) != n_features : raise ValueError ( \"Shape mismatch: if categories is an array,\" \" it has to be of shape (n_features,).\" ) if self . descriptors != \"auto\" : for i , des in enumerate ( self . descriptors ): if len ( des ) != len ( self . values [ i ][ 0 ]): raise ValueError ( \"Shape mismatch: number of descriptors\" \" do not fit to the dimension of provided values.\" ) self . values_ = [] self . n_categories_i = [] self . n_descriptors_in_ = [] for values in self . values : descriptor_list , n_categories_i , n_descriptors = self . _check_X ( values , force_all_finite = force_all_finite # type: ignore ) self . values_ . append ( descriptor_list ) self . n_categories_i . append ( n_categories_i ) self . n_descriptors_in_ . append ( n_descriptors ) self . categories_ = [] self . descriptors_ = [] for i in range ( n_features ): Xi = X_list [ i ] if self . categories == \"auto\" : cats = _unique ( Xi ) else : cats = np . array ( self . categories [ i ], dtype = Xi . dtype ) if Xi . dtype . kind not in \"OUS\" : sorted_cats = np . sort ( cats ) error_msg = ( \"Unsorted categories are not supported for numerical categories\" ) # if there are nans, nan should be the last element stop_idx = - 1 if np . isnan ( sorted_cats [ - 1 ]) else None if np . any ( sorted_cats [: stop_idx ] != cats [: stop_idx ]) or ( np . isnan ( sorted_cats [ - 1 ]) and not np . isnan ( sorted_cats [ - 1 ]) ): raise ValueError ( error_msg ) if handle_unknown == \"error\" : diff = _check_unknown ( Xi , cats ) if diff : msg = ( \"Found unknown categories {0} in column {1} \" \" during fit\" . format ( diff , i ) ) raise ValueError ( msg ) self . categories_ . append ( cats ) des_list = [] for j in range ( self . n_descriptors_in_ [ i ]): if len ( self . values_ [ i ][ j ]) != len ( cats ): raise ValueError ( \"Shape mismatch: descriptor values has to be of shape (n_categories_per_feature, n_descriptors).\" ) if self . descriptors == \"auto\" : des_list . append ( f \"Descriptor_ { i } _ { j } \" ) else : des_list . append ( np . array ( self . descriptors [ i ][ j ])) self . descriptors_ . append ( des_list ) def fit ( self , X , y = None ): \"\"\" Fit Encoder to X. Args: X (array-like of shape (n_samples, n_features)): The data to determine the categories of each feature. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: self: Fitted encoder. \"\"\" self . _validate_keywords () self . _fit ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" ) return self def fit_transform ( self , X , y = None ): \"\"\" Fit categoricalDescriptorEncoder to X, then transform X. Equivalent to fit(X).transform(X) but more convenient. Args: X (array-like of shape (n_samples, n_features)): The data to encode. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: X_out ({ndarray, matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" self . _validate_keywords () return super () . fit_transform ( X , y ) def transform ( self , X ): \"\"\" Transform X using descriptors. Args: X (array-like of shape (n_samples, n_features)): The data to encode. Returns: X_out ({ndarray, sparse matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" check_is_fitted ( self ) # validation of X happens in _check_X called by _transform warn_on_unknown = self . handle_unknown == \"ignore\" X_int , X_mask = self . _transform ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" , # type: ignore warn_on_unknown = warn_on_unknown , ) n_samples = X . shape [ 0 ] n_values = [ len ( cats ) for cats in self . categories_ ] X_tr = np . zeros (( n_samples , sum ( self . n_descriptors_in_ )), dtype = float ) for c , categories in enumerate ( self . categories_ ): for i , category in enumerate ( categories ): for j in range ( self . n_descriptors_in_ [ c ]): col = sum ( self . n_descriptors_in_ [: c ]) # insert values to categorical data row = np . where ( X == category )[ 0 ] X_tr [ row , col + j ] = self . values_ [ c ][ j ][ i ] ### check whats going on here ### #TODO: sparse matrix is currently not supported! mask = X_mask . ravel () feature_indices = np . cumsum ([ 0 ] + n_values ) indices = ( X_int + feature_indices [: - 1 ]) . ravel ()[ mask ] indptr = np . empty ( n_samples + 1 , dtype = int ) indptr [ 0 ] = 0 np . sum ( X_mask , axis = 1 , out = indptr [ 1 :]) np . cumsum ( indptr [ 1 :], out = indptr [ 1 :]) data = np . ones ( indptr [ - 1 ]) _ = sparse . csr_matrix ( ( data , indices , indptr ), shape = ( n_samples , feature_indices [ - 1 ]), dtype = self . dtype , ) #################################### if not self . sparse : return X_tr else : raise NotImplementedError ( \"Sparse matrices as output are not implemented yet\" ) # return X_tr_sparse def inverse_transform ( self , X ): \"\"\" Convert the data back to the original representation. Args: X ({array-like, sparse matrix} of shape (n_samples, n_encoded_features*n_descriptors_per_feature)): The transformed data. Returns: X_tr (ndarray of shape (n_samples, n_features)): Inverse transformed array. \"\"\" check_is_fitted ( self ) X = check_array ( X , accept_sparse = \"csr\" ) n_samples = X . shape [ 0 ] # validate shape of passed X msg = ( \"Shape of the passed X data is not correct. Expected {0} columns, got {1} .\" ) if X . shape [ 1 ] != sum ( self . n_descriptors_in_ ): raise ValueError ( msg . format ( len ( self . descriptors ), X . shape [ 1 ])) X_tr = np . empty (( n_samples , self . n_features_in_ ), dtype = object ) for c , categories in enumerate ( self . categories_ ): indices = np . cumsum ([ 0 ] + self . n_descriptors_in_ ) var_descriptor_conditions = X [:, indices [ c ] : indices [ c + 1 ]] var_descriptor_orig_data = np . column_stack ( self . values_ [ c ]) var_categorical_transformed = [] # Find the closest points by euclidean distance for i in range ( n_samples ): # Euclidean distance calculation eucl_distance_squ = np . sum ( np . square ( np . subtract ( var_descriptor_orig_data , var_descriptor_conditions [ i , :], ) ), axis = 1 , ) # Choose closest point category_index = np . where ( eucl_distance_squ == np . min ( eucl_distance_squ ) )[ 0 ][ 0 ] # Find the matching name of the categorical variable category = categories [ category_index ] # Add the original categorical variables name to the dataset var_categorical_transformed . append ( category ) X_tr [:, c ] = np . asarray ( var_categorical_transformed ) return X_tr def get_feature_names_out ( self , input_features = None ): \"\"\"Get output feature names for transformation. Args: input_features (array-like of str or None): default=None. Input features. - If `input_features` is `None`, then `feature_names_in_` is used as feature names in. If `feature_names_in_` is not defined, then names are generated: `[x0, x1, ..., x(n_features_in_)]`. - If `input_features` is an array-like, then `input_features` must match `feature_names_in_` if `feature_names_in_` is defined. Returns: feature_names_out (ndarray(str)): Transformed feature names. \"\"\" check_is_fitted ( self ) desc = self . descriptors_ input_features = _check_feature_names_in ( self , input_features ) feature_names = [] for i in range ( len ( desc )): names = [ input_features [ i ] + \"_\" + str ( t ) for t in desc [ i ]] feature_names . extend ( names ) return np . asarray ( feature_names , dtype = object )","title":"CategoricalDescriptorEncoder"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder.CategoricalDescriptorEncoder.__init__","text":"Encoder to translate categorical parameters into continuous descriptor values. Parameters: Name Type Description Default values List[List[List[float]]] Nested list of descriptor values. Must be of shape (n_features, n_categories_per_feature, n_descriptors). required categories List[List[str]] List of strings referring to the categories (not the feature names!) occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) 'auto' descriptors List[List[str]] List of strings referring to the descriptor names occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) 'auto' sparse bool Sparse matrix output is currently not supported. Defaults to False. False dtype dtype [description]. Defaults to np.float64. <class 'numpy.float64'> handle_unknown str Allows to distinguish between \"error\" and \"ignore\". When \"ignore\", onknown categories are encoded as zeros. Defaults to \"error\". 'error' Source code in bofire/utils/categoricalDescriptorEncoder.py def __init__ ( self , * , categories : Union [ str , List [ List [ str ]]] = \"auto\" , descriptors : Union [ str , List [ List [ str ]]] = \"auto\" , values : List [ List [ List [ float ]]], sparse : bool = False , dtype = np . float64 , handle_unknown = \"error\" , ): \"\"\"Encoder to translate categorical parameters into continuous descriptor values. Args: values (List[List[List[float]]]): Nested list of descriptor values. Must be of shape (n_features, n_categories_per_feature, n_descriptors). categories (List[List[str]], optional): List of strings referring to the categories (not the feature names!) occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) descriptors (List[List[str]], optional): List of strings referring to the descriptor names occuring in the dataset. Defaults to \"auto\". (When no list is passed, the descriptor names are generated automatically.) sparse (bool, optional): Sparse matrix output is currently not supported. Defaults to False. dtype (dtype, optional): [description]. Defaults to np.float64. handle_unknown (str, optional): Allows to distinguish between \"error\" and \"ignore\". When \"ignore\", onknown categories are encoded as zeros. Defaults to \"error\". \"\"\" self . categories = categories self . descriptors = descriptors self . values = values # check, whether we have only 1D data or multiple descriptors provided at once if not isinstance ( self . values [ 0 ][ 0 ], list ): self . values = [ values ] if not isinstance ( self . categories [ 0 ], list ) and self . categories != \"auto\" : self . categories = [ categories ] if not isinstance ( self . descriptors [ 0 ], list ) and self . descriptors != \"auto\" : self . descriptors = [ descriptors ] self . sparse = sparse self . dtype = dtype self . handle_unknown = handle_unknown","title":"__init__()"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder.CategoricalDescriptorEncoder.fit","text":"Fit Encoder to X. Parameters: Name Type Description Default X array-like of shape (n_samples, n_features The data to determine the categories of each feature. required y None. Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . None Returns: Type Description self Fitted encoder. Source code in bofire/utils/categoricalDescriptorEncoder.py def fit ( self , X , y = None ): \"\"\" Fit Encoder to X. Args: X (array-like of shape (n_samples, n_features)): The data to determine the categories of each feature. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: self: Fitted encoder. \"\"\" self . _validate_keywords () self . _fit ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" ) return self","title":"fit()"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder.CategoricalDescriptorEncoder.fit_transform","text":"Fit categoricalDescriptorEncoder to X, then transform X. Equivalent to fit(X).transform(X) but more convenient. Parameters: Name Type Description Default X array-like of shape (n_samples, n_features The data to encode. required y None. Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . None Returns: Type Description X_out ({ndarray, matrix} of shape (n_samples, n_encoded_features)) Transformed input. Source code in bofire/utils/categoricalDescriptorEncoder.py def fit_transform ( self , X , y = None ): \"\"\" Fit categoricalDescriptorEncoder to X, then transform X. Equivalent to fit(X).transform(X) but more convenient. Args: X (array-like of shape (n_samples, n_features)): The data to encode. y: None. Ignored. This parameter exists only for compatibility with :class:`~sklearn.pipeline.Pipeline`. Returns: X_out ({ndarray, matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" self . _validate_keywords () return super () . fit_transform ( X , y )","title":"fit_transform()"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder.CategoricalDescriptorEncoder.get_feature_names_out","text":"Get output feature names for transformation. Parameters: Name Type Description Default input_features array-like of str or None default=None. Input features. - If input_features is None , then feature_names_in_ is used as feature names in. If feature_names_in_ is not defined, then names are generated: [x0, x1, ..., x(n_features_in_)] . - If input_features is an array-like, then input_features must match feature_names_in_ if feature_names_in_ is defined. None Returns: Type Description feature_names_out (ndarray(str)) Transformed feature names. Source code in bofire/utils/categoricalDescriptorEncoder.py def get_feature_names_out ( self , input_features = None ): \"\"\"Get output feature names for transformation. Args: input_features (array-like of str or None): default=None. Input features. - If `input_features` is `None`, then `feature_names_in_` is used as feature names in. If `feature_names_in_` is not defined, then names are generated: `[x0, x1, ..., x(n_features_in_)]`. - If `input_features` is an array-like, then `input_features` must match `feature_names_in_` if `feature_names_in_` is defined. Returns: feature_names_out (ndarray(str)): Transformed feature names. \"\"\" check_is_fitted ( self ) desc = self . descriptors_ input_features = _check_feature_names_in ( self , input_features ) feature_names = [] for i in range ( len ( desc )): names = [ input_features [ i ] + \"_\" + str ( t ) for t in desc [ i ]] feature_names . extend ( names ) return np . asarray ( feature_names , dtype = object )","title":"get_feature_names_out()"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder.CategoricalDescriptorEncoder.inverse_transform","text":"Convert the data back to the original representation. Parameters: Name Type Description Default X {array-like, sparse matrix} of shape (n_samples, n_encoded_features*n_descriptors_per_feature The transformed data. required Returns: Type Description X_tr (ndarray of shape (n_samples, n_features)) Inverse transformed array. Source code in bofire/utils/categoricalDescriptorEncoder.py def inverse_transform ( self , X ): \"\"\" Convert the data back to the original representation. Args: X ({array-like, sparse matrix} of shape (n_samples, n_encoded_features*n_descriptors_per_feature)): The transformed data. Returns: X_tr (ndarray of shape (n_samples, n_features)): Inverse transformed array. \"\"\" check_is_fitted ( self ) X = check_array ( X , accept_sparse = \"csr\" ) n_samples = X . shape [ 0 ] # validate shape of passed X msg = ( \"Shape of the passed X data is not correct. Expected {0} columns, got {1} .\" ) if X . shape [ 1 ] != sum ( self . n_descriptors_in_ ): raise ValueError ( msg . format ( len ( self . descriptors ), X . shape [ 1 ])) X_tr = np . empty (( n_samples , self . n_features_in_ ), dtype = object ) for c , categories in enumerate ( self . categories_ ): indices = np . cumsum ([ 0 ] + self . n_descriptors_in_ ) var_descriptor_conditions = X [:, indices [ c ] : indices [ c + 1 ]] var_descriptor_orig_data = np . column_stack ( self . values_ [ c ]) var_categorical_transformed = [] # Find the closest points by euclidean distance for i in range ( n_samples ): # Euclidean distance calculation eucl_distance_squ = np . sum ( np . square ( np . subtract ( var_descriptor_orig_data , var_descriptor_conditions [ i , :], ) ), axis = 1 , ) # Choose closest point category_index = np . where ( eucl_distance_squ == np . min ( eucl_distance_squ ) )[ 0 ][ 0 ] # Find the matching name of the categorical variable category = categories [ category_index ] # Add the original categorical variables name to the dataset var_categorical_transformed . append ( category ) X_tr [:, c ] = np . asarray ( var_categorical_transformed ) return X_tr","title":"inverse_transform()"},{"location":"ref-utils/#bofire.utils.categoricalDescriptorEncoder.CategoricalDescriptorEncoder.transform","text":"Transform X using descriptors. Parameters: Name Type Description Default X array-like of shape (n_samples, n_features The data to encode. required Returns: Type Description X_out ({ndarray, sparse matrix} of shape (n_samples, n_encoded_features)) Transformed input. Source code in bofire/utils/categoricalDescriptorEncoder.py def transform ( self , X ): \"\"\" Transform X using descriptors. Args: X (array-like of shape (n_samples, n_features)): The data to encode. Returns: X_out ({ndarray, sparse matrix} of shape (n_samples, n_encoded_features)): Transformed input. \"\"\" check_is_fitted ( self ) # validation of X happens in _check_X called by _transform warn_on_unknown = self . handle_unknown == \"ignore\" X_int , X_mask = self . _transform ( X , handle_unknown = self . handle_unknown , force_all_finite = \"allow-nan\" , # type: ignore warn_on_unknown = warn_on_unknown , ) n_samples = X . shape [ 0 ] n_values = [ len ( cats ) for cats in self . categories_ ] X_tr = np . zeros (( n_samples , sum ( self . n_descriptors_in_ )), dtype = float ) for c , categories in enumerate ( self . categories_ ): for i , category in enumerate ( categories ): for j in range ( self . n_descriptors_in_ [ c ]): col = sum ( self . n_descriptors_in_ [: c ]) # insert values to categorical data row = np . where ( X == category )[ 0 ] X_tr [ row , col + j ] = self . values_ [ c ][ j ][ i ] ### check whats going on here ### #TODO: sparse matrix is currently not supported! mask = X_mask . ravel () feature_indices = np . cumsum ([ 0 ] + n_values ) indices = ( X_int + feature_indices [: - 1 ]) . ravel ()[ mask ] indptr = np . empty ( n_samples + 1 , dtype = int ) indptr [ 0 ] = 0 np . sum ( X_mask , axis = 1 , out = indptr [ 1 :]) np . cumsum ( indptr [ 1 :], out = indptr [ 1 :]) data = np . ones ( indptr [ - 1 ]) _ = sparse . csr_matrix ( ( data , indices , indptr ), shape = ( n_samples , feature_indices [ - 1 ]), dtype = self . dtype , ) #################################### if not self . sparse : return X_tr else : raise NotImplementedError ( \"Sparse matrices as output are not implemented yet\" ) # return X_tr_sparse","title":"transform()"},{"location":"ref-utils/#bofire.utils.enum","text":"","title":"enum"},{"location":"ref-utils/#bofire.utils.enum.AcquisitionFunctionEnum","text":"An enumeration. Source code in bofire/utils/enum.py class AcquisitionFunctionEnum ( Enum ): QNEI = \"qNEI\" QUCB = \"qUCB\" QEI = \"qEI\" QPI = \"qPI\" QSR = \"qSR\" # QEHVI = \"qEHVI\" # QNEHVI = \"qNEHVI\"","title":"AcquisitionFunctionEnum"},{"location":"ref-utils/#bofire.utils.enum.CategoricalEncodingEnum","text":"Enumeration class of implemented categorical encodings Currently, one-hot and ordinal encoding are implemented. Source code in bofire/utils/enum.py class CategoricalEncodingEnum ( Enum ): \"\"\"Enumeration class of implemented categorical encodings Currently, one-hot and ordinal encoding are implemented. \"\"\" ONE_HOT = \"ONE_HOT\" ORDINAL = \"ORDINAL\"","title":"CategoricalEncodingEnum"},{"location":"ref-utils/#bofire.utils.enum.CategoricalMethodEnum","text":"Enumeration class of supported methods how to handle categorical features Currently, exhaustive search and free relaxation are implemented. Source code in bofire/utils/enum.py class CategoricalMethodEnum ( Enum ): \"\"\"Enumeration class of supported methods how to handle categorical features Currently, exhaustive search and free relaxation are implemented. \"\"\" EXHAUSTIVE = \"EXHAUSTIVE\" FREE = \"FREE\"","title":"CategoricalMethodEnum"},{"location":"ref-utils/#bofire.utils.enum.DescriptorEncodingEnum","text":"Enumeration class how categorical features with descriptors should be encoded Categoricals with descriptors can be handled similar to categoricals, or the descriptors can be used. Source code in bofire/utils/enum.py class DescriptorEncodingEnum ( Enum ): \"\"\"Enumeration class how categorical features with descriptors should be encoded Categoricals with descriptors can be handled similar to categoricals, or the descriptors can be used. \"\"\" DESCRIPTOR = \"DESCRIPTOR\" CATEGORICAL = \"CATEGORICAL\"","title":"DescriptorEncodingEnum"},{"location":"ref-utils/#bofire.utils.enum.DescriptorMethodEnum","text":"Enumeration class of implemented methods how to handle discrete descriptors Currently, exhaustive search and free relaxation are implemented. Source code in bofire/utils/enum.py class DescriptorMethodEnum ( Enum ): \"\"\"Enumeration class of implemented methods how to handle discrete descriptors Currently, exhaustive search and free relaxation are implemented. \"\"\" EXHAUSTIVE = \"EXHAUSTIVE\" FREE = \"FREE\"","title":"DescriptorMethodEnum"},{"location":"ref-utils/#bofire.utils.enum.KernelEnum","text":"Enumeration class of all supported kernels Currently, RBF and matern kernel (1/2, 3/2 and 5/2) are implemented. Source code in bofire/utils/enum.py class KernelEnum ( Enum ): \"\"\"Enumeration class of all supported kernels Currently, RBF and matern kernel (1/2, 3/2 and 5/2) are implemented. \"\"\" RBF = \"RBF\" MATERN_25 = \"MATERN_25\" MATERN_15 = \"MATERN_15\" MATERN_05 = \"MATERN_05\"","title":"KernelEnum"},{"location":"ref-utils/#bofire.utils.enum.SamplingMethodEnum","text":"An enumeration. Source code in bofire/utils/enum.py class SamplingMethodEnum ( Enum ): UNIFORM = \"UNIFORM\" SOBOL = \"SOBOL\" LHS = \"LHS\"","title":"SamplingMethodEnum"},{"location":"ref-utils/#bofire.utils.enum.ScalerEnum","text":"Enumeration class of supported scalers Currently, normalization and standardization are implemented. Source code in bofire/utils/enum.py class ScalerEnum ( Enum ): \"\"\"Enumeration class of supported scalers Currently, normalization and standardization are implemented. \"\"\" NORMALIZE = \"NORMALIZE\" STANDARDIZE = \"STANDARDIZE\"","title":"ScalerEnum"},{"location":"ref-utils/#bofire.utils.reduce","text":"","title":"reduce"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform","text":"Class to switch back and forth from the reduced to the original domain. Source code in bofire/utils/reduce.py class AffineTransform : \"\"\"Class to switch back and forth from the reduced to the original domain.\"\"\" def __init__ ( self , equalities : List [ Tuple [ str , List [ str ], List [ float ]]]): \"\"\"Initializes a `AffineTransformation` object. Args: equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients. \"\"\" self . equalities = equalities def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore the eliminated features in a dataframe Args: data (pd.DataFrame): Dataframe that should be restored. Returns: pd.DataFrame: Restored dataframe \"\"\" if len ( self . equalities ) == 0 : return data data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated features from a dataframe. Args: data (pd.DataFrame): Dataframe with features to be dropped. Returns: pd.DataFrame: Reduced dataframe. \"\"\" if len ( self . equalities ) == 0 : return data drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop )","title":"AffineTransform"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.__init__","text":"Initializes a AffineTransformation object. Parameters: Name Type Description Default equalities List[Tuple[str,List[str],List[float]]] List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients. required Source code in bofire/utils/reduce.py def __init__ ( self , equalities : List [ Tuple [ str , List [ str ], List [ float ]]]): \"\"\"Initializes a `AffineTransformation` object. Args: equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients. \"\"\" self . equalities = equalities","title":"__init__()"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.augment_data","text":"Restore the eliminated features in a dataframe Parameters: Name Type Description Default data pd.DataFrame Dataframe that should be restored. required Returns: Type Description pd.DataFrame Restored dataframe Source code in bofire/utils/reduce.py def augment_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Restore the eliminated features in a dataframe Args: data (pd.DataFrame): Dataframe that should be restored. Returns: pd.DataFrame: Restored dataframe \"\"\" if len ( self . equalities ) == 0 : return data data = data . copy () for name_lhs , names_rhs , coeffs in self . equalities : data [ name_lhs ] = coeffs [ - 1 ] for i , name in enumerate ( names_rhs ): data [ name_lhs ] += coeffs [ i ] * data [ name ] return data","title":"augment_data()"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.drop_data","text":"Drop eliminated features from a dataframe. Parameters: Name Type Description Default data pd.DataFrame Dataframe with features to be dropped. required Returns: Type Description pd.DataFrame Reduced dataframe. Source code in bofire/utils/reduce.py def drop_data ( self , data : pd . DataFrame ) -> pd . DataFrame : \"\"\"Drop eliminated features from a dataframe. Args: data (pd.DataFrame): Dataframe with features to be dropped. Returns: pd.DataFrame: Reduced dataframe. \"\"\" if len ( self . equalities ) == 0 : return data drop = [] for name_lhs , _ , _ in self . equalities : if name_lhs in data . columns : drop . append ( name_lhs ) return data . drop ( columns = drop )","title":"drop_data()"},{"location":"ref-utils/#bofire.utils.reduce.adjust_boundary","text":"Adjusts the boundaries of a feature. Parameters: Name Type Description Default feature ContinuousInputFeature Feature to be adjusted. required coef float Coefficient. required rhs float Right-hand-side of the constraint. required Source code in bofire/utils/reduce.py def adjust_boundary ( feature : ContinuousInput , coef : float , rhs : float ): \"\"\"Adjusts the boundaries of a feature. Args: feature (ContinuousInputFeature): Feature to be adjusted. coef (float): Coefficient. rhs (float): Right-hand-side of the constraint. \"\"\" boundary = rhs / coef if coef > 0 : if boundary > feature . lower_bound : feature . lower_bound = boundary else : if boundary < feature . upper_bound : feature . upper_bound = boundary","title":"adjust_boundary()"},{"location":"ref-utils/#bofire.utils.reduce.check_domain_for_reduction","text":"Check if the reduction can be applied or if a trivial case is present. Parameters: Name Type Description Default domain Domain Domain to be checked. required Returns: Type Description bool True if reducable, else False. Source code in bofire/utils/reduce.py def check_domain_for_reduction ( domain : Domain ) -> bool : \"\"\"Check if the reduction can be applied or if a trivial case is present. Args: domain (Domain): Domain to be checked. Returns: bool: True if reducable, else False. \"\"\" # are there any constraints? if len ( domain . constraints ) == 0 : return False # are there any linear equality constraints? linear_equalities = domain . cnstrs . get ( LinearEqualityConstraint ) if len ( linear_equalities ) == 0 : return False # are there no NChooseKConstraint constraints? if len ( domain . cnstrs . get ([ NChooseKConstraint ])) > 0 : return False # are there continuous inputs continuous_inputs = domain . get_features ( ContinuousInput ) if len ( continuous_inputs ) == 0 : return False # check that equality constraints only contain continuous inputs for c in linear_equalities : assert isinstance ( c , LinearConstraint ) for feat in c . features : if feat not in domain . get_feature_keys ( ContinuousInput ): return False return True","title":"check_domain_for_reduction()"},{"location":"ref-utils/#bofire.utils.reduce.check_existence_of_solution","text":"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem. Source code in bofire/utils/reduce.py def check_existence_of_solution ( A_aug ): \"\"\"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.\"\"\" A = A_aug [:, : - 1 ] b = A_aug [:, - 1 ] len_inputs = np . shape ( A )[ 1 ] # catch special cases rk_A_aug = np . linalg . matrix_rank ( A_aug ) rk_A = np . linalg . matrix_rank ( A ) if rk_A == rk_A_aug : if rk_A < len_inputs : return # all good else : x = np . linalg . solve ( A , b ) raise Exception ( f \"There is a unique solution x for the linear equality constraints: x= { x } \" ) elif rk_A < rk_A_aug : raise Exception ( \"There is no solution fulfilling the linear equality constraints.\" )","title":"check_existence_of_solution()"},{"location":"ref-utils/#bofire.utils.reduce.reduce_domain","text":"Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated. Parameters: Name Type Description Default domain Domain Domain to be reduced. required Returns: Type Description Tuple[Domain, AffineTransform] reduced domain and the according transformation to switch between the reduced and orginal domain. Source code in bofire/utils/reduce.py def reduce_domain ( domain : Domain ) -> Tuple [ Domain , AffineTransform ]: \"\"\"Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated. Args: domain (Domain): Domain to be reduced. Returns: Tuple[Domain, AffineTransform]: reduced domain and the according transformation to switch between the reduced and orginal domain. \"\"\" # check if the domain can be reduced if not check_domain_for_reduction ( domain ): return domain , AffineTransform ([]) # find linear equality constraints linear_equalities = domain . cnstrs . get ( LinearEqualityConstraint ) other_constraints = domain . cnstrs . get ( Constraint , excludes = [ LinearEqualityConstraint ] ) # only consider continuous inputs continuous_inputs = [ cast ( ContinuousInput , f ) for f in domain . get_features ( ContinuousInput ) ] other_inputs = domain . inputs . get ( InputFeature , excludes = [ ContinuousInput ]) # assemble Matrix A from equality constraints N = len ( linear_equalities ) M = len ( continuous_inputs ) + 1 names = np . concatenate (([ feat . key for feat in continuous_inputs ], [ \"rhs\" ])) A_aug = pd . DataFrame ( data = np . zeros ( shape = ( N , M )), columns = names ) for i in range ( len ( linear_equalities )): c = linear_equalities [ i ] assert isinstance ( c , LinearEqualityConstraint ) A_aug . loc [ i , c . features ] = c . coefficients # type: ignore A_aug . loc [ i , \"rhs\" ] = c . rhs A_aug = A_aug . values # catch special cases check_existence_of_solution ( A_aug ) # bring A_aug to reduced row-echelon form A_aug_rref , pivots = rref ( A_aug ) pivots = np . array ( pivots ) A_aug_rref = np . array ( A_aug_rref ) . astype ( np . float64 ) # formulate box bounds as linear inequality constraints in matrix form B = np . zeros ( shape = ( 2 * ( M - 1 ), M )) B [: M - 1 , : M - 1 ] = np . eye ( M - 1 ) B [ M - 1 :, : M - 1 ] = - np . eye ( M - 1 ) B [: M - 1 , - 1 ] = np . array ([ feat . upper_bound for feat in continuous_inputs ]) B [ M - 1 :, - 1 ] = - 1.0 * np . array ([ feat . lower_bound for feat in continuous_inputs ]) # eliminate columns with pivot element for i in range ( len ( pivots )): p = pivots [ i ] B [ p , :] -= A_aug_rref [ i , :] B [ p + M - 1 , :] += A_aug_rref [ i , :] # build up reduced domain _domain = Domain . construct ( # _fields_set = {\"input_features\", \"output_features\", \"constraints\"} input_features = deepcopy ( other_inputs ), output_features = deepcopy ( domain . output_features ), constraints = deepcopy ( other_constraints ), ) new_inputs = [ deepcopy ( feat ) for i , feat in enumerate ( continuous_inputs ) if i not in pivots ] all_inputs = _domain . inputs + new_inputs assert isinstance ( all_inputs , InputFeatures ) _domain . input_features = all_inputs constraints : List [ Constraint ] = [] for i in pivots : # reduce equation system of upper bounds ind = np . where ( B [ i , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i , - 1 ] < np . inf : if len ( list ( names [ ind ])) > 1 : c = LinearInequalityConstraint . from_greater_equal ( features = list ( names [ ind ]), coefficients = ( - 1.0 * B [ i , ind ]) . tolist (), rhs = B [ i , - 1 ] * - 1.0 , ) constraints . append ( c ) else : key = names [ ind ][ 0 ] feat = cast ( ContinuousInput , _domain . get_feature ( key )) adjust_boundary ( feat , ( - 1.0 * B [ i , ind ])[ 0 ], B [ i , - 1 ] * - 1.0 ) else : if B [ i , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) # reduce equation system of lower bounds ind = np . where ( B [ i + M - 1 , : - 1 ] != 0 )[ 0 ] if len ( ind ) > 0 and B [ i + M - 1 , - 1 ] < np . inf : if len ( list ( names [ ind ])) > 1 : c = LinearInequalityConstraint . from_greater_equal ( features = list ( names [ ind ]), coefficients = ( - 1.0 * B [ i + M - 1 , ind ]) . tolist (), rhs = B [ i + M - 1 , - 1 ] * - 1.0 , ) constraints . append ( c ) else : key = names [ ind ][ 0 ] feat = cast ( ContinuousInput , _domain . get_feature ( key )) adjust_boundary ( feat , ( - 1.0 * B [ i + M - 1 , ind ])[ 0 ], B [ i + M - 1 , - 1 ] * - 1.0 , ) else : if B [ i + M - 1 , - 1 ] < - 1e-16 : raise Exception ( \"There is no solution that fulfills the constraints.\" ) if len ( constraints ) > 0 : _domain . _set_constraints_unvalidated ( _domain . cnstrs + constraints ) # assemble equalities _equalities = [] for i in range ( len ( pivots )): name_lhs = names [ pivots [ i ]] names_rhs = [] coeffs = [] for j in range ( len ( names ) - 1 ): if A_aug_rref [ i , j ] != 0 and j != pivots [ i ]: coeffs . append ( - A_aug_rref [ i , j ]) names_rhs . append ( names [ j ]) coeffs . append ( A_aug_rref [ i , - 1 ]) _equalities . append (( name_lhs , names_rhs , coeffs )) trafo = AffineTransform ( _equalities ) # remove remaining dependencies of eliminated inputs from the problem _domain = remove_eliminated_inputs ( _domain , trafo ) return _domain , trafo","title":"reduce_domain()"},{"location":"ref-utils/#bofire.utils.reduce.remove_eliminated_inputs","text":"Eliminates remaining occurences of eliminated inputs in linear constraints. Parameters: Name Type Description Default domain Domain Domain in which the linear constraints should be purged. required transform AffineTransform Affine transformation object that defines the obsolete features. required Exceptions: Type Description ValueError If feature occurs in a constraint different from a linear one. Returns: Type Description Domain Purged domain. Source code in bofire/utils/reduce.py def remove_eliminated_inputs ( domain : Domain , transform : AffineTransform ) -> Domain : \"\"\"Eliminates remaining occurences of eliminated inputs in linear constraints. Args: domain (Domain): Domain in which the linear constraints should be purged. transform (AffineTransform): Affine transformation object that defines the obsolete features. Raises: ValueError: If feature occurs in a constraint different from a linear one. Returns: Domain: Purged domain. \"\"\" inputs_names = domain . get_feature_keys () M = len ( inputs_names ) # write the equalities for the backtransformation into one matrix inputs_dict = { inputs_names [ i ]: i for i in range ( M )} # build up dict from domain.equalities e.g. {\"xi1\": [coeff(xj1), ..., coeff(xjn)], ... \"xik\":...} coeffs_dict = {} for i , e in enumerate ( transform . equalities ): coeffs = np . zeros ( M + 1 ) for j , name in enumerate ( e [ 1 ]): coeffs [ inputs_dict [ name ]] = e [ 2 ][ j ] coeffs [ - 1 ] = e [ 2 ][ - 1 ] coeffs_dict [ e [ 0 ]] = coeffs constraints = [] for c in domain . cnstrs . get (): # Nonlinear constraints not supported if not isinstance ( c , LinearConstraint ): raise ValueError ( \"Elimination of variables is only supported for LinearEquality and LinearInequality constraints.\" ) # no changes, if the constraint does not contain eliminated inputs elif all ( name in inputs_names for name in c . features ): constraints . append ( c ) # remove inputs from the constraint that were eliminated from the inputs before else : totally_removed = False _features = np . array ( inputs_names ) _rhs = c . rhs # create new lhs and rhs from the old one and knowledge from problem._equalities _coefficients = np . zeros ( M ) for j , name in enumerate ( c . features ): if name in inputs_names : _coefficients [ inputs_dict [ name ]] += c . coefficients [ j ] else : _coefficients += c . coefficients [ j ] * coeffs_dict [ name ][: - 1 ] _rhs -= c . coefficients [ j ] * coeffs_dict [ name ][ - 1 ] _features = _features [ np . abs ( _coefficients ) > 1e-16 ] _coefficients = _coefficients [ np . abs ( _coefficients ) > 1e-16 ] _c = None if isinstance ( c , LinearEqualityConstraint ): if len ( _features ) > 1 : _c = LinearEqualityConstraint ( features = _features . tolist (), coefficients = _coefficients . tolist (), rhs = _rhs , ) elif len ( _features ) == 0 : totally_removed = True else : feat = domain . get_feature ( _features [ 0 ]) feat . lower_bound = _coefficients [ 0 ] feat . upper_bound = _coefficients [ 0 ] totally_removed = True else : if len ( _features ) > 1 : _c = LinearInequalityConstraint ( features = _features . tolist (), coefficients = _coefficients . tolist (), rhs = _rhs , ) elif len ( _features ) == 0 : totally_removed = True else : feat = cast ( ContinuousInput , domain . get_feature ( _features [ 0 ])) adjust_boundary ( feat , _coefficients [ 0 ], _rhs ) totally_removed = True # check if constraint is always fulfilled/not fulfilled if not totally_removed : assert _c is not None if len ( _c . features ) == 0 and _c . rhs >= 0 : pass elif len ( _c . features ) == 0 and _c . rhs < 0 : raise Exception ( \"Linear constraints cannot be fulfilled.\" ) elif np . isinf ( _c . rhs ): pass else : constraints . append ( _c ) domain . constraints = Constraints ( constraints = constraints ) return domain","title":"remove_eliminated_inputs()"},{"location":"ref-utils/#bofire.utils.reduce.rref","text":"Computes the reduced row echelon form of a Matrix Parameters: Name Type Description Default A ndarray 2d array representing a matrix. required tol float tolerance for rounding to 0. Defaults to 1e-8. 1e-08 Returns: Type Description Tuple[numpy.ndarray, List[int]] (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref Source code in bofire/utils/reduce.py def rref ( A : np . ndarray , tol : float = 1e-8 ) -> Tuple [ np . ndarray , List [ int ]]: \"\"\"Computes the reduced row echelon form of a Matrix Args: A (ndarray): 2d array representing a matrix. tol (float, optional): tolerance for rounding to 0. Defaults to 1e-8. Returns: (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots is a numpy array containing the pivot columns of A_rref \"\"\" A = np . array ( A , dtype = np . float64 ) n , m = np . shape ( A ) col = 0 row = 0 pivots = [] for col in range ( m ): # does a pivot element exist? if all ( np . abs ( A [ row :, col ]) < tol ): pass # if yes: start elimination else : pivots . append ( col ) max_row = np . argmax ( np . abs ( A [ row :, col ])) + row # switch to most stable row A [[ row , max_row ], :] = A [[ max_row , row ], :] # type: ignore # normalize row A [ row , :] /= A [ row , col ] # eliminate other elements from column for r in range ( n ): if r != row : A [ r , :] -= A [ r , col ] / A [ row , col ] * A [ row , :] row += 1 prec = int ( - np . log10 ( tol )) return np . round ( A , prec ), pivots","title":"rref()"},{"location":"ref-utils/#bofire.utils.study","text":"","title":"study"},{"location":"ref-utils/#bofire.utils.study.MetricsEnum","text":"An enumeration. Source code in bofire/utils/study.py class MetricsEnum ( Enum ): HYPERVOLUME = \"HYPERVOLUME\" STRATEGY = \"STRATEGY\"","title":"MetricsEnum"},{"location":"ref-utils/#bofire.utils.study.PoolStudy","text":"Source code in bofire/utils/study.py class PoolStudy ( BaseModel ): class Config : arbitrary_types_allowed = True domain : Domain num_starting_experiments : int experiments : pd . DataFrame meta : Optional [ pd . DataFrame ] strategy : Optional [ Strategy ] starting_point_generator : Optional [ Callable ] metrics : Optional [ MetricsEnum ] = None ref_point : Optional [ dict ] @validator ( \"domain\" ) def validate_feature_count ( cls , domain : Domain ): if len ( domain . input_features ) == 0 : raise ValueError ( \"no input feature specified\" ) if len ( domain . output_features ) == 0 : raise ValueError ( \"no output feature specified\" ) return domain @validator ( \"experiments\" ) def validate_experiments ( cls , experiments , values ): experiments = values [ \"domain\" ] . validate_experiments ( experiments ) # # we pick only those experiments where at least one output is valid cleaned = ( values [ \"domain\" ] . preprocess_experiments_any_valid_output ( experiments ) . copy () . reset_index ( drop = True ) ) if len ( experiments ) < 2 : raise ValueError ( \"too less experiments available for PoolStudy.\" ) return cleaned @validator ( \"metrics\" ) def validate_metrics ( cls , metrics , values ): if ( metrics is MetricsEnum . HYPERVOLUME and len ( values [ \"domain\" ] . output_features . get_by_objective ( excludes = None )) < 2 ): raise ValueError ( \"For metrics HYPERVOLUME at least two output features has to be defined.\" ) return metrics @validator ( \"ref_point\" ) def validate_ref_point ( cls , ref_point , values ): if ref_point is None : return None if len ( ref_point ) != len ( values [ \"domain\" ] . output_features . get_by_objective ( excludes = None ) ): raise ValueError ( \"Length of refpoint does not match number of output features.\" ) for feat in values [ \"domain\" ] . output_features . get_keys_by_objective ( excludes = None ): assert feat in ref_point . keys () return ref_point @staticmethod def generate_uniform ( experiments : pd . DataFrame , num_starting_experiments ): assert num_starting_experiments > 0 return np . random . choice ( np . arange ( experiments . shape [ 0 ]), size = num_starting_experiments , replace = False , ) def __init__ ( self , ** data ): super () . __init__ ( ** data ) self . meta = pd . DataFrame ( index = range ( self . experiments . shape [ 0 ]), columns = [ \"iteration\" ], data = np . nan ) if self . starting_point_generator is None : start_idx = self . generate_uniform ( self . experiments , self . num_starting_experiments ) else : start_idx = self . starting_point_generator ( self . experiments , self . num_starting_experiments ) # if ( self . metrics == MetricsEnum . HYPERVOLUME ) and ( self . ref_point is None ): self . ref_point = infer_ref_point ( self . domain , self . experiments , return_masked = False ) self . meta . loc [ start_idx , \"iteration\" ] = 0 @property def picked_experiments ( self ): return self . experiments . loc [ self . meta . iteration . notna ()] # type: ignore @property def num_picked_experiments ( self ): return self . picked_experiments . shape [ 0 ] @property def open_experiments ( self ): return self . experiments . loc [ self . meta . iteration . isna ()] # type: ignore @property def num_open_experiments ( self ): return self . open_experiments . shape [ 0 ] @property def num_iterations ( self ): return int ( self . meta . iteration . max ()) # type: ignore def get_fbest ( self , experiments : pd . DataFrame ): if self . metrics is None : return np . nan elif self . metrics == MetricsEnum . HYPERVOLUME : opt_exps = get_pareto_front ( self . domain , experiments ) return compute_hypervolume ( domain = self . domain , optimal_experiments = opt_exps , ref_point = self . ref_point , # type: ignore ) elif self . metrics == MetricsEnum . STRATEGY : return self . strategy . get_fbest ( experiments ) # type: ignore @property def expected_random ( self ): \"\"\"Expected value for number of random picks until finding the best one. According to https://arxiv.org/abs/1404.1161 it is defined as E(X) = (N+1)/(K+1) where N is the number of possible solutions and K the number of good solutions. \"\"\" K = ( get_pareto_front ( domain = self . domain , experiments = self . experiments ) . shape [ 0 ] if self . metrics == MetricsEnum . HYPERVOLUME else 1 ) return ( self . experiments . shape [ 0 ] + 1 ) / ( K + 1 ) def optimize ( self , strategy : Strategy , num_iterations : int = 100 , candidate_count : int = 1 , progress_bar : bool = True , early_stopping = False , ): self . strategy = strategy ( self . domain ) # type: ignore if candidate_count > 1 : raise ValueError ( \"batch_size > 1 not yet implemented.\" ) if num_iterations < 1 : raise ValueError ( \"At least one iteration has to be performed!\" ) if num_iterations > self . num_open_experiments : num_iterations = self . num_open_experiments fbest = self . get_fbest ( self . experiments ) with tqdm ( range ( num_iterations ), disable = True if progress_bar is False else False , postfix = { \"dist2best\" : \"?\" }, ) as pbar : for i in pbar : strategy . tell ( self . picked_experiments ) acqf_values = strategy . _choose_from_pool ( self . open_experiments ) picked_idx = self . open_experiments . iloc [ acqf_values . argmax ()] . name # type: ignore self . meta . loc [ picked_idx , \"iteration\" ] = i + 1 # type: ignore cfbest = self . get_fbest ( self . picked_experiments ) pbar . set_postfix ({ \"dist2best\" : fbest - cfbest }) # type: ignore if np . allclose ( fbest , cfbest ) and early_stopping : # type: ignore break","title":"PoolStudy"},{"location":"ref-utils/#bofire.utils.study.PoolStudy.expected_random","text":"Expected value for number of random picks until finding the best one. According to https://arxiv.org/abs/1404.1161 it is defined as E(X) = (N+1)/(K+1) where N is the number of possible solutions and K the number of good solutions.","title":"expected_random"},{"location":"ref-utils/#bofire.utils.torch_tools","text":"","title":"torch_tools"},{"location":"ref-utils/#bofire.utils.torch_tools.get_linear_constraints","text":"Converts linear constraints to the form required by BoTorch. Parameters: Name Type Description Default domain Domain Optimization problem definition. required constraint Union[LinearEqualityConstraint, LinearInequalityConstraint] Type of constraint that should be converted. required unit_scaled bool If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False. False Returns: Type Description List[Tuple[Tensor, Tensor, float]] List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs. Source code in bofire/utils/torch_tools.py def get_linear_constraints ( domain : Domain , constraint : Union [ LinearEqualityConstraint , LinearInequalityConstraint ], unit_scaled : bool = False , ) -> List [ Tuple [ Tensor , Tensor , float ]]: \"\"\"Converts linear constraints to the form required by BoTorch. Args: domain (Domain): Optimization problem definition. constraint (Union[LinearEqualityConstraint, LinearInequalityConstraint]): Type of constraint that should be converted. unit_scaled (bool, optional): If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False. Returns: List[Tuple[Tensor, Tensor, float]]: List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs. \"\"\" constraints = [] for c in domain . cnstrs . get ( constraint ): indices = [] coefficients = [] lower = [] upper = [] rhs = 0.0 for i , featkey in enumerate ( c . features ): # type: ignore idx = domain . get_feature_keys ( InputFeature ) . index ( featkey ) feat = domain . get_feature ( featkey ) if feat . is_fixed (): # type: ignore rhs -= feat . fixed_value () * c . coefficients [ i ] # type: ignore else : lower . append ( feat . lower_bound ) # type: ignore upper . append ( feat . upper_bound ) # type: ignore indices . append ( idx ) coefficients . append ( c . coefficients [ i ] # type: ignore ) # if unit_scaled == False else c_scaled.coefficients[i]) if unit_scaled : lower = np . array ( lower ) upper = np . array ( upper ) s = upper - lower scaled_coefficients = s * np . array ( coefficients ) constraints . append ( ( torch . tensor ( indices ), - torch . tensor ( scaled_coefficients ) . to ( ** tkwargs ), - ( rhs + c . rhs - np . sum ( np . array ( coefficients ) * lower )), # type: ignore ) ) else : constraints . append ( ( torch . tensor ( indices ), - torch . tensor ( coefficients ) . to ( ** tkwargs ), - ( rhs + c . rhs ), # type: ignore ) ) return constraints","title":"get_linear_constraints()"},{"location":"ref-utils/#bofire.utils.transformer","text":"","title":"transformer"},{"location":"ref-utils/#bofire.utils.transformer.Transformer","text":"Source code in bofire/utils/transformer.py class Transformer ( BaseModel ): domain : Domain descriptor_encoding : Optional [ Union [ DescriptorEncodingEnum , None ]] categorical_encoding : Optional [ Union [ CategoricalEncodingEnum , None ]] scale_inputs : Optional [ ScalerEnum ] = None scale_outputs : Optional [ ScalerEnum ] = None is_fitted : bool = False features2transformedFeatures : Dict = Field ( default_factory = lambda : {}) encoders : Dict = Field ( default_factory = lambda : {}) \"\"\"Pre/post-processing of data for strategies Parameters: is_fitted: bool features2transformedFeatures: Dict encoders: Dict \"\"\" def __init__ ( self , domain , descriptor_encoding = None , # TODO: default tbd! categorical_encoding = None , # TODO: default tbd! scale_inputs = None , scale_outputs = None , ) -> None : \"\"\"Pre/post-processing of data for strategies Args: domain (Domain): A domain for that is being used in the strategy descriptor_encoding (DescriptorEncodingEnum, optional): Transform the descriptors into continuous features (\"DESCRIPTOR\")/ numerical representation of categoricals (\"CATEGORICAL\"). Defaults to None. categorical_encoding (CategoricalEncodingEnum, optional): Distinction between one hot and ordinal encoding for categoricals. Defaults to None. scale_inputs/ scale_outputs (ScalerEnum, optional): In-/Outputs can be standardized, normalized or not scaled. Defaults to None. \"\"\" super () . __init__ ( domain = domain , descriptor_encoding = descriptor_encoding , # ( # descriptor_encoding.name # if isinstance(descriptor_encoding, Enum) # else None # ), categorical_encoding = categorical_encoding , # ( # categorical_encoding.name # if isinstance(categorical_encoding, Enum) # else None # ), scale_inputs = scale_inputs , scale_outputs = scale_outputs , ) for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . descriptors ] self . features2transformedFeatures [ feature . key ] = keys elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . categories ] self . features2transformedFeatures [ feature . key ] = keys return def fit ( self , input : pd . DataFrame ): \"\"\"Fit the encoders to provided input data Args: input (pd.DataFrame): The data, the encoders are fitted to. Raises: NotImplementedError: User-provided descriptor values are currently ignored. The descriptor values are set in the CategoricalDescriptorInputFeature. DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformer object: The fitted transformer \"\"\" experiment = input . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): if all ( np . isin ( feature . descriptors , experiment . columns )): # how should we deal with descriptors entered by the user, which are not in line with the values assigned in the features`? # TODO: Check if user-provided decriptors are valid # TODO: implement hierarchy which descriptors are preferred raise NotImplementedError ( \"User-provided descriptor values are currently ignored\" ) enc = CategoricalDescriptorEncoder ( categories = [ feature . categories ], descriptors = [ feature . descriptors ], values = [ feature . values ], ) values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) column_names = enc . get_feature_names_out () self . encoders [ feature . key ] = enc values = self . encoders [ feature . key ] . transform ( values ) for loc , column_name in enumerate ( column_names ): experiment [ column_name ] = values [:, loc ] var_min = min ([ val [ loc ] for val in feature . values ]) var_max = max ([ val [ loc ] for val in feature . values ]) # Scale inputs self . fit_scaling ( column_name , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) experiment = experiment . drop ( column_name , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): enc = OrdinalEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Create one-hot encoding columns & insert to DataSet # TODO: drop in oneHot encoder testen enc = OneHotEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif isinstance ( feature , ContinuousInput ): var_min , var_max = feature . lower_bound , feature . upper_bound self . fit_scaling ( feature . key , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) self . fit_scaling ( feature . key , experiment , scaler_type = self . scale_outputs ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) self . is_fitted = True return self def transform ( self , experiment : pd . DataFrame ): \"\"\"Transform data inputs and outputs for a strategy given already fitted encoders Args: experiment (pd.DataFrame): Input data to be transformed Raises: DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformed_experiment (pd.DataFrame): the transformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" transformed_experiment = experiment . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) values = self . encoders [ feature . key ] . transform ( values ) column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , values [:, i ]) # Scale inputs if self . encoders [ column_name ] is not None : values_unscaled = np . atleast_2d ( transformed_experiment [ column_name ] . to_numpy () ) . T transformed_experiment [ column_name ] = self . encoders [ column_name ] . transform ( values_unscaled ) # Ensure descriptor features are floats transformed_experiment [ column_names ] = transformed_experiment [ column_names ] . astype ( float ) # drop categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) transformed_experiment [ feature . key ] = enc_values . astype ( \"int32\" ) # categorical kernel needs int as input to avoid numerical trouble. Thus, these columns are also not scaled elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) . toarray () column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , enc_values [:, i ] ) # Drop old categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif isinstance ( feature , ContinuousInput ): if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) return transformed_experiment . copy () def fit_transform ( self , experiment : pd . DataFrame ): \"\"\"A combination of self.fit and self.transform Args: experiment (pd.DataFrame): [description] Returns: transformed_experiment (pd.DataFrame): the transfered input data \"\"\" self . fit ( experiment ) transformed_experiment = self . transform ( experiment ) return transformed_experiment def inverse_transform ( self , transformed_candidate : pd . DataFrame ): \"\"\"backtransformation of data using the fitted encoders Args: transformed_candidate (pd.DataFrame): input data to be backtransformed Raises: DomainError: Feature is defined in the domain, but no feature data is provided in the dataset to be backtransformed. NotImplementedError: Acctually, categorical outputs are not supported. Returns: candidate (pd.DataFrame): backtransformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" # Determine input and output columns in dataset candidate = transformed_candidate . copy () for feature in self . get_features_to_be_transformed (): # Categorical variables with descriptors if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): enc = self . encoders [ feature . key ] column_names = enc . get_feature_names_out () for column_name in column_names : candidate = self . un_scale ( column_name , candidate ) values = candidate [ column_names ] . to_numpy () enc_values = enc . inverse_transform ( values ) index = candidate . columns . get_loc ( column_names [ 0 ]) candidate . insert ( index , feature . key , enc_values ) # Delete the descriptor columns candidate = candidate . drop ( columns = column_names , axis = 1 ) # Categorical features using one-hot encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed column_names = enc . get_feature_names_out () values = candidate [ column_names ] . to_numpy () # Do inverse transform index = candidate . columns . get_loc ( column_names [ 0 ]) enc_values = enc . inverse_transform ( values ) candidate . insert ( index , feature . key , enc_values ) # Add to dataset and drop one-hot encoding candidate = candidate . drop ( column_names , axis = 1 ) # Categorical features using ordinal encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed values = np . atleast_2d ( candidate [ feature . key ] . to_numpy ()) . T # Do inverse transform candidate [ feature . key ] = enc . inverse_transform ( values ) # Continuous features elif isinstance ( feature , ContinuousInput ): candidate = self . un_scale ( feature . key , candidate ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): pass else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) for feature in self . domain . get_features ( OutputFeature ): if feature . key in transformed_candidate . columns : if isinstance ( feature , ContinuousOutput ): candidate = self . un_scale ( feature . key , candidate ) else : raise NotImplementedError ( \"Acctually, only continuous outputs are implemented\" ) return candidate def get_features_to_be_transformed ( self ): excludes = [] if self . categorical_encoding is None : excludes . append ( CategoricalInput ) features_cat_desc = self . domain . get_features ( CategoricalDescriptorInput ) else : features_cat_desc = [] if self . descriptor_encoding is None : excludes . append ( CategoricalDescriptorInput ) features_cat_desc = [] return ( self . domain . get_features ( InputFeature , excludes = excludes ) + features_cat_desc ) def fit_scaling ( self , key : str , experiment : pd . DataFrame , var_min : float = np . nan , var_max : float = np . nan , scaler_type : Optional [ ScalerEnum ] = None , ) -> \"Transformer\" : # TODO: switch to Self some time in the future which is available in Python 3.11 \"\"\"fitting the chosen scaler type to provided input data Args: key (str): column name in input data of the feature to be scaled experiment (pd.DataFrame): input data to be fitted to var_min (float, optional): Lower bound to be used for min max scaling. When not defined or nan, the minimum of the input data is used. Defaults to np.nan. var_max (float, optional): Upper bound to be used for min max scaling. When not defined or nan, the maximum of the input data is used. Defaults to np.nan. scaler_type (ScalerEnum, optional): Defines the type of scaling (Normalize/ standardize/ None). Defaults to None. Returns: transformer object: fitted encoders are added to self.encoders \"\"\" values = np . atleast_2d ( experiment [ key ] . to_numpy ()) . T if scaler_type == ScalerEnum . STANDARDIZE : enc = StandardScaler () enc . fit ( values ) elif scaler_type == ScalerEnum . NORMALIZE : enc = MinMaxScaler () if np . isnan ( var_min ): var_min = min ( values ) if np . isnan ( var_max ): var_max = max ( values ) enc . fit ( np . array ([ var_min , var_max ]) . reshape ( - 1 , 1 )) else : enc = None self . encoders [ key ] = enc return self def un_scale ( self , key : str , candidate : pd . DataFrame ) -> pd . DataFrame : \"\"\"uses the fitted encoders to back-scale the input data to original scale Args: key (str): column name in input data of the feature to be back-scaled candidate (pd.DataFrame): input data to be un-scaled Returns: candidate (pd.DataFrame): The un-scaled input data \"\"\" if ( key in self . encoders . keys ()) and ( self . encoders [ key ] is not None ): values = np . atleast_2d ( candidate [ key ] . to_numpy ()) . T candidate [ key ] = self . encoders [ key ] . inverse_transform ( values ) candidate [ key ] = candidate [ key ] . astype ( float ) return candidate","title":"Transformer"},{"location":"ref-utils/#bofire.utils.transformer.Transformer.__init__","text":"Pre/post-processing of data for strategies Parameters: Name Type Description Default domain Domain A domain for that is being used in the strategy required descriptor_encoding DescriptorEncodingEnum Transform the descriptors into continuous features (\"DESCRIPTOR\")/ numerical representation of categoricals (\"CATEGORICAL\"). Defaults to None. None categorical_encoding CategoricalEncodingEnum Distinction between one hot and ordinal encoding for categoricals. Defaults to None. None scale_inputs/ scale_outputs (ScalerEnum In-/Outputs can be standardized, normalized or not scaled. Defaults to None. required Source code in bofire/utils/transformer.py def __init__ ( self , domain , descriptor_encoding = None , # TODO: default tbd! categorical_encoding = None , # TODO: default tbd! scale_inputs = None , scale_outputs = None , ) -> None : \"\"\"Pre/post-processing of data for strategies Args: domain (Domain): A domain for that is being used in the strategy descriptor_encoding (DescriptorEncodingEnum, optional): Transform the descriptors into continuous features (\"DESCRIPTOR\")/ numerical representation of categoricals (\"CATEGORICAL\"). Defaults to None. categorical_encoding (CategoricalEncodingEnum, optional): Distinction between one hot and ordinal encoding for categoricals. Defaults to None. scale_inputs/ scale_outputs (ScalerEnum, optional): In-/Outputs can be standardized, normalized or not scaled. Defaults to None. \"\"\" super () . __init__ ( domain = domain , descriptor_encoding = descriptor_encoding , # ( # descriptor_encoding.name # if isinstance(descriptor_encoding, Enum) # else None # ), categorical_encoding = categorical_encoding , # ( # categorical_encoding.name # if isinstance(categorical_encoding, Enum) # else None # ), scale_inputs = scale_inputs , scale_outputs = scale_outputs , ) for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . descriptors ] self . features2transformedFeatures [ feature . key ] = keys elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): keys = [ feature . key + \"_\" + str ( t ) for t in feature . categories ] self . features2transformedFeatures [ feature . key ] = keys return","title":"__init__()"},{"location":"ref-utils/#bofire.utils.transformer.Transformer.fit","text":"Fit the encoders to provided input data Parameters: Name Type Description Default input pd.DataFrame The data, the encoders are fitted to. required Exceptions: Type Description DomainError Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError Output features cannot be categorical features currently. DomainError Unknown output feature type Returns: Type Description transformer object The fitted transformer Source code in bofire/utils/transformer.py def fit ( self , input : pd . DataFrame ): \"\"\"Fit the encoders to provided input data Args: input (pd.DataFrame): The data, the encoders are fitted to. Raises: NotImplementedError: User-provided descriptor values are currently ignored. The descriptor values are set in the CategoricalDescriptorInputFeature. DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformer object: The fitted transformer \"\"\" experiment = input . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): if all ( np . isin ( feature . descriptors , experiment . columns )): # how should we deal with descriptors entered by the user, which are not in line with the values assigned in the features`? # TODO: Check if user-provided decriptors are valid # TODO: implement hierarchy which descriptors are preferred raise NotImplementedError ( \"User-provided descriptor values are currently ignored\" ) enc = CategoricalDescriptorEncoder ( categories = [ feature . categories ], descriptors = [ feature . descriptors ], values = [ feature . values ], ) values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) column_names = enc . get_feature_names_out () self . encoders [ feature . key ] = enc values = self . encoders [ feature . key ] . transform ( values ) for loc , column_name in enumerate ( column_names ): experiment [ column_name ] = values [:, loc ] var_min = min ([ val [ loc ] for val in feature . values ]) var_max = max ([ val [ loc ] for val in feature . values ]) # Scale inputs self . fit_scaling ( column_name , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) experiment = experiment . drop ( column_name , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): enc = OrdinalEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Create one-hot encoding columns & insert to DataSet # TODO: drop in oneHot encoder testen enc = OneHotEncoder ( categories = [ feature . categories ]) # type: ignore values = pd . DataFrame ( experiment [ feature . key ], columns = [ feature . key ]) enc . fit ( values ) self . encoders [ feature . key ] = enc elif isinstance ( feature , ContinuousInput ): var_min , var_max = feature . lower_bound , feature . upper_bound self . fit_scaling ( feature . key , experiment , var_min , var_max , scaler_type = self . scale_inputs , ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) self . fit_scaling ( feature . key , experiment , scaler_type = self . scale_outputs ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) self . is_fitted = True return self","title":"fit()"},{"location":"ref-utils/#bofire.utils.transformer.Transformer.fit_scaling","text":"fitting the chosen scaler type to provided input data Parameters: Name Type Description Default key str column name in input data of the feature to be scaled required experiment pd.DataFrame input data to be fitted to required var_min float Lower bound to be used for min max scaling. When not defined or nan, the minimum of the input data is used. Defaults to np.nan. nan var_max float Upper bound to be used for min max scaling. When not defined or nan, the maximum of the input data is used. Defaults to np.nan. nan scaler_type ScalerEnum Defines the type of scaling (Normalize/ standardize/ None). Defaults to None. None Returns: Type Description transformer object fitted encoders are added to self.encoders Source code in bofire/utils/transformer.py def fit_scaling ( self , key : str , experiment : pd . DataFrame , var_min : float = np . nan , var_max : float = np . nan , scaler_type : Optional [ ScalerEnum ] = None , ) -> \"Transformer\" : # TODO: switch to Self some time in the future which is available in Python 3.11 \"\"\"fitting the chosen scaler type to provided input data Args: key (str): column name in input data of the feature to be scaled experiment (pd.DataFrame): input data to be fitted to var_min (float, optional): Lower bound to be used for min max scaling. When not defined or nan, the minimum of the input data is used. Defaults to np.nan. var_max (float, optional): Upper bound to be used for min max scaling. When not defined or nan, the maximum of the input data is used. Defaults to np.nan. scaler_type (ScalerEnum, optional): Defines the type of scaling (Normalize/ standardize/ None). Defaults to None. Returns: transformer object: fitted encoders are added to self.encoders \"\"\" values = np . atleast_2d ( experiment [ key ] . to_numpy ()) . T if scaler_type == ScalerEnum . STANDARDIZE : enc = StandardScaler () enc . fit ( values ) elif scaler_type == ScalerEnum . NORMALIZE : enc = MinMaxScaler () if np . isnan ( var_min ): var_min = min ( values ) if np . isnan ( var_max ): var_max = max ( values ) enc . fit ( np . array ([ var_min , var_max ]) . reshape ( - 1 , 1 )) else : enc = None self . encoders [ key ] = enc return self","title":"fit_scaling()"},{"location":"ref-utils/#bofire.utils.transformer.Transformer.fit_transform","text":"A combination of self.fit and self.transform Parameters: Name Type Description Default experiment pd.DataFrame [description] required Returns: Type Description transformed_experiment (pd.DataFrame) the transfered input data Source code in bofire/utils/transformer.py def fit_transform ( self , experiment : pd . DataFrame ): \"\"\"A combination of self.fit and self.transform Args: experiment (pd.DataFrame): [description] Returns: transformed_experiment (pd.DataFrame): the transfered input data \"\"\" self . fit ( experiment ) transformed_experiment = self . transform ( experiment ) return transformed_experiment","title":"fit_transform()"},{"location":"ref-utils/#bofire.utils.transformer.Transformer.inverse_transform","text":"backtransformation of data using the fitted encoders Parameters: Name Type Description Default transformed_candidate pd.DataFrame input data to be backtransformed required Returns: Type Description candidate (pd.DataFrame) backtransformed input data Source code in bofire/utils/transformer.py def inverse_transform ( self , transformed_candidate : pd . DataFrame ): \"\"\"backtransformation of data using the fitted encoders Args: transformed_candidate (pd.DataFrame): input data to be backtransformed Raises: DomainError: Feature is defined in the domain, but no feature data is provided in the dataset to be backtransformed. NotImplementedError: Acctually, categorical outputs are not supported. Returns: candidate (pd.DataFrame): backtransformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" # Determine input and output columns in dataset candidate = transformed_candidate . copy () for feature in self . get_features_to_be_transformed (): # Categorical variables with descriptors if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): enc = self . encoders [ feature . key ] column_names = enc . get_feature_names_out () for column_name in column_names : candidate = self . un_scale ( column_name , candidate ) values = candidate [ column_names ] . to_numpy () enc_values = enc . inverse_transform ( values ) index = candidate . columns . get_loc ( column_names [ 0 ]) candidate . insert ( index , feature . key , enc_values ) # Delete the descriptor columns candidate = candidate . drop ( columns = column_names , axis = 1 ) # Categorical features using one-hot encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed column_names = enc . get_feature_names_out () values = candidate [ column_names ] . to_numpy () # Do inverse transform index = candidate . columns . get_loc ( column_names [ 0 ]) enc_values = enc . inverse_transform ( values ) candidate . insert ( index , feature . key , enc_values ) # Add to dataset and drop one-hot encoding candidate = candidate . drop ( column_names , axis = 1 ) # Categorical features using ordinal encoding elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): # Get encoder enc = self . encoders [ feature . key ] # Get array to be transformed values = np . atleast_2d ( candidate [ feature . key ] . to_numpy ()) . T # Do inverse transform candidate [ feature . key ] = enc . inverse_transform ( values ) # Continuous features elif isinstance ( feature , ContinuousInput ): candidate = self . un_scale ( feature . key , candidate ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): pass else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) for feature in self . domain . get_features ( OutputFeature ): if feature . key in transformed_candidate . columns : if isinstance ( feature , ContinuousOutput ): candidate = self . un_scale ( feature . key , candidate ) else : raise NotImplementedError ( \"Acctually, only continuous outputs are implemented\" ) return candidate","title":"inverse_transform()"},{"location":"ref-utils/#bofire.utils.transformer.Transformer.transform","text":"Transform data inputs and outputs for a strategy given already fitted encoders Parameters: Name Type Description Default experiment pd.DataFrame Input data to be transformed required Returns: Type Description transformed_experiment (pd.DataFrame) the transformed input data Source code in bofire/utils/transformer.py def transform ( self , experiment : pd . DataFrame ): \"\"\"Transform data inputs and outputs for a strategy given already fitted encoders Args: experiment (pd.DataFrame): Input data to be transformed Raises: DomainError: Unknown input feature type. Features have to be of type continuous or categorical feature. DomainError: Output features cannot be categorical features currently. DomainError: Unknown output feature type Returns: transformed_experiment (pd.DataFrame): the transformed input data \"\"\" assert self . is_fitted is True , \"Encoders are not initialized\" transformed_experiment = experiment . copy () for feature in self . get_features_to_be_transformed (): if ( isinstance ( feature , CategoricalDescriptorInput ) and self . descriptor_encoding == DescriptorEncodingEnum . DESCRIPTOR ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) values = self . encoders [ feature . key ] . transform ( values ) column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , values [:, i ]) # Scale inputs if self . encoders [ column_name ] is not None : values_unscaled = np . atleast_2d ( transformed_experiment [ column_name ] . to_numpy () ) . T transformed_experiment [ column_name ] = self . encoders [ column_name ] . transform ( values_unscaled ) # Ensure descriptor features are floats transformed_experiment [ column_names ] = transformed_experiment [ column_names ] . astype ( float ) # drop categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ORDINAL ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) transformed_experiment [ feature . key ] = enc_values . astype ( \"int32\" ) # categorical kernel needs int as input to avoid numerical trouble. Thus, these columns are also not scaled elif ( isinstance ( feature , CategoricalInput ) and self . categorical_encoding == CategoricalEncodingEnum . ONE_HOT ): values = pd . DataFrame ( transformed_experiment [ feature . key ], columns = [ feature . key ] ) enc_values = self . encoders [ feature . key ] . transform ( values ) . toarray () column_names = self . encoders [ feature . key ] . get_feature_names_out () index = transformed_experiment . columns . get_loc ( feature . key ) for i , column_name in enumerate ( column_names ): transformed_experiment . insert ( index + i , column_name , enc_values [:, i ] ) # Drop old categorical column transformed_experiment = transformed_experiment . drop ( feature . key , axis = 1 ) elif isinstance ( feature , ContinuousInput ): if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) elif ( self . categorical_encoding is None and self . descriptor_encoding is not None ): logging . warning ( \"Descriptors should be encoded as categoricals. However, categoricals are selected to be not transformed. Thus, I will skip categoricals with descriptors as well.\" ) pass else : raise DomainError ( f \"Feature { feature . key } is not a continuous or categorical feature.\" ) for feature in self . domain . get_features ( OutputFeature ): if isinstance ( feature , OutputFeature ): if not is_continuous ( feature ): raise DomainError ( \"Output features cannot be categorical features currently.\" ) if self . encoders [ feature . key ] is not None : values = np . atleast_2d ( transformed_experiment [ feature . key ] . to_numpy () ) . T transformed_experiment [ feature . key ] = self . encoders [ feature . key ] . transform ( values ) else : raise DomainError ( f \"Feature { feature . key } is not in the dataset.\" ) return transformed_experiment . copy ()","title":"transform()"},{"location":"ref-utils/#bofire.utils.transformer.Transformer.un_scale","text":"uses the fitted encoders to back-scale the input data to original scale Parameters: Name Type Description Default key str column name in input data of the feature to be back-scaled required candidate pd.DataFrame input data to be un-scaled required Returns: Type Description candidate (pd.DataFrame) The un-scaled input data Source code in bofire/utils/transformer.py def un_scale ( self , key : str , candidate : pd . DataFrame ) -> pd . DataFrame : \"\"\"uses the fitted encoders to back-scale the input data to original scale Args: key (str): column name in input data of the feature to be back-scaled candidate (pd.DataFrame): input data to be un-scaled Returns: candidate (pd.DataFrame): The un-scaled input data \"\"\" if ( key in self . encoders . keys ()) and ( self . encoders [ key ] is not None ): values = np . atleast_2d ( candidate [ key ] . to_numpy ()) . T candidate [ key ] = self . encoders [ key ] . inverse_transform ( values ) candidate [ key ] = candidate [ key ] . astype ( float ) return candidate","title":"un_scale()"}]}